#!/usr/bin/env python3
"""
Neural Network 06-06-25 ULTRA - Final Edition
Author: Oussama GUELFAA
Date: 06 - 06 - 2025

Version finale avec am√©liorations suppl√©mentaires pour atteindre R¬≤ > 0.8 :
6. üéØ Loss ultra-pond√©r√©e (gap x50)
7. üîÑ Ensemble de mod√®les sp√©cialis√©s
8. üìà Data augmentation intelligente
9. üéõÔ∏è Architecture ultra-sp√©cialis√©e pour gap
10. üîß Optimisation hyperparam√®tres avanc√©e
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import scipy.io as sio
import os
import time

# Configuration pour reproductibilit√©
torch.manual_seed(42)
np.random.seed(42)

class UltraWeightedLoss(nn.Module):
    """Loss ultra-pond√©r√©e avec focus extr√™me sur gap."""
    
    def __init__(self, gap_weight=50.0):
        super(UltraWeightedLoss, self).__init__()
        self.gap_weight = gap_weight
    
    def forward(self, pred, target):
        mse_L = (pred[:, 0] - target[:, 0]) ** 2
        mse_gap = (pred[:, 1] - target[:, 1]) ** 2
        
        # Loss ultra-pond√©r√©e
        total_loss = mse_L.mean() + self.gap_weight * mse_gap.mean()
        return total_loss

class UltraSpecializedRegressor(nn.Module):
    """Architecture ultra-sp√©cialis√©e avec focus maximal sur gap."""
    
    def __init__(self, input_size=600):
        super(UltraSpecializedRegressor, self).__init__()
        
        # Feature extractor commun plus profond
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_size, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # T√™te L_ecran simple (signal fort)
        self.L_ecran_head = nn.Sequential(
            nn.Linear(128, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
        # T√™te gap ultra-sp√©cialis√©e (signal faible)
        self.gap_feature_enhancer = nn.Sequential(
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.01),
            
            nn.Linear(256, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.01),
            
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU()
        )
        
        # M√©canisme d'attention multi-t√™te pour gap
        self.gap_attention_1 = nn.Sequential(
            nn.Linear(128, 64),
            nn.Tanh(),
            nn.Linear(64, 128),
            nn.Sigmoid()
        )
        
        self.gap_attention_2 = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.Sigmoid()
        )
        
        # Pr√©dicteur gap final
        self.gap_predictor = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.005),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
        # Initialisation des poids
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.BatchNorm1d):
            nn.init.constant_(module.weight, 1)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        # Features communes
        features = self.feature_extractor(x)
        
        # L_ecran (simple)
        L_ecran = self.L_ecran_head(features)
        
        # Gap (ultra-sp√©cialis√©)
        gap_features = self.gap_feature_enhancer(features)
        
        # Double attention pour gap
        attention_1 = self.gap_attention_1(gap_features)
        attention_2 = self.gap_attention_2(gap_features)
        
        # Combinaison des attentions
        combined_attention = (attention_1 + attention_2) / 2
        attended_features = gap_features * combined_attention
        
        gap = self.gap_predictor(attended_features)
        
        return torch.cat([L_ecran, gap], dim=1)

def intelligent_data_augmentation(X, y, augmentation_factor=3):
    """Augmentation intelligente des donn√©es."""
    
    print(f"üìà AM√âLIORATION 8: Data augmentation intelligente")
    
    X_augmented = [X]
    y_augmented = [y]
    
    for factor in range(1, augmentation_factor + 1):
        # Bruit gaussien adaptatif
        noise_std = 0.001 * factor
        X_noisy = X + np.random.normal(0, noise_std, X.shape)
        
        # L√©g√®re variation des param√®tres (dans la pr√©cision)
        y_varied = y + np.random.normal(0, [0.001, 0.001], y.shape)
        y_varied = np.round(y_varied, 3)  # Maintenir la pr√©cision
        
        X_augmented.append(X_noisy)
        y_augmented.append(y_varied)
    
    X_final = np.vstack(X_augmented)
    y_final = np.vstack(y_augmented)
    
    print(f"   Donn√©es originales: {X.shape}")
    print(f"   Donn√©es augment√©es: {X_final.shape}")
    print(f"   Facteur d'augmentation: {augmentation_factor + 1}x")
    
    return X_final, y_final

def create_ensemble_models(input_size=600, n_models=3):
    """Cr√©e un ensemble de mod√®les sp√©cialis√©s."""
    
    print(f"üîÑ AM√âLIORATION 7: Ensemble de {n_models} mod√®les")
    
    models = []
    for i in range(n_models):
        model = UltraSpecializedRegressor(input_size)
        models.append(model)
    
    return models

def train_ensemble_ultra(models, X_train, X_val, y_train, y_val, epochs=200):
    """Entra√Æne l'ensemble de mod√®les ultra-sp√©cialis√©s."""
    
    print(f"üéõÔ∏è AM√âLIORATION 6: Loss ultra-pond√©r√©e (gap x50)")
    print(f"üîß AM√âLIORATION 10: Optimisation hyperparam√®tres avanc√©e")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Convertir en tenseurs
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.FloatTensor(y_train).to(device)
    X_val_tensor = torch.FloatTensor(X_val).to(device)
    y_val_tensor = torch.FloatTensor(y_val).to(device)
    
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Batch plus petit
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    
    trained_models = []
    
    for i, model in enumerate(models):
        print(f"\n  Entra√Ænement mod√®le {i+1}/{len(models)}...")
        
        model.to(device)
        
        # Loss ultra-pond√©r√©e diff√©rente pour chaque mod√®le
        gap_weights = [30.0, 50.0, 70.0]
        criterion = UltraWeightedLoss(gap_weight=gap_weights[i])
        
        # Optimiseur avec hyperparam√®tres optimis√©s
        optimizer = optim.AdamW([
            {'params': model.feature_extractor.parameters(), 'lr': 5e-4, 'weight_decay': 1e-4},
            {'params': model.L_ecran_head.parameters(), 'lr': 1e-3, 'weight_decay': 1e-5},
            {'params': model.gap_feature_enhancer.parameters(), 'lr': 2e-4, 'weight_decay': 1e-4},
            {'params': model.gap_attention_1.parameters(), 'lr': 1e-4, 'weight_decay': 1e-5},
            {'params': model.gap_attention_2.parameters(), 'lr': 1e-4, 'weight_decay': 1e-5},
            {'params': model.gap_predictor.parameters(), 'lr': 1e-4, 'weight_decay': 1e-5}
        ])
        
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2)
        
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 30
        
        for epoch in range(epochs):
            # Entra√Ænement
            model.train()
            train_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # Validation
            model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            
            scheduler.step()
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(model.state_dict(), f'models/ultra_model_{i}.pth')
            else:
                patience_counter += 1
            
            if epoch % 40 == 0:
                print(f"    Epoch {epoch:3d}: Train = {train_loss:.4f}, Val = {val_loss:.4f}")
            
            if patience_counter >= patience:
                print(f"    Early stopping √† l'epoch {epoch}")
                break
        
        # Charger le meilleur mod√®le
        model.load_state_dict(torch.load(f'models/ultra_model_{i}.pth'))
        trained_models.append(model)
    
    return trained_models

def ensemble_predict(models, X_test):
    """Pr√©diction par ensemble avec pond√©ration."""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    X_test_tensor = torch.FloatTensor(X_test).to(device)
    
    predictions = []
    
    for model in models:
        model.eval()
        with torch.no_grad():
            pred = model(X_test_tensor).cpu().numpy()
            predictions.append(pred)
    
    # Moyenne pond√©r√©e (plus de poids sur les mod√®les avec gap_weight √©lev√©)
    weights = np.array([0.2, 0.3, 0.5])  # Plus de poids sur le mod√®le gap_weight=70
    
    ensemble_pred = np.zeros_like(predictions[0])
    for i, pred in enumerate(predictions):
        ensemble_pred += weights[i] * pred
    
    return ensemble_pred

def load_and_preprocess_ultra():
    """Chargement et pr√©traitement ultra-optimis√©."""
    
    print("="*80)
    print("üöÄ PR√âTRAITEMENT ULTRA-OPTIMIS√â")
    print("="*80)
    
    # Charger les donn√©es
    df_profiles = pd.read_csv('processed_data/intensity_profiles_truncated_600.csv')
    df_params = pd.read_csv('processed_data/parameters_truncated_600.csv')
    
    X_raw = df_profiles.values.astype(np.float32)
    y_raw = df_params[['L_ecran', 'gap']].values.astype(np.float32)
    
    # Appliquer toutes les am√©liorations pr√©c√©dentes
    y_rounded = np.round(y_raw, 3)
    
    # Focus sur plage exp√©rimentale
    experimental_mask = (y_rounded[:, 1] >= 0.025) & (y_rounded[:, 1] <= 0.517)
    X_focused = X_raw[experimental_mask]
    y_focused = y_rounded[experimental_mask]
    
    # Data augmentation intelligente
    X_augmented, y_augmented = intelligent_data_augmentation(X_focused, y_focused, augmentation_factor=2)
    
    # Normalisation s√©par√©e
    scaler_X = StandardScaler()
    X_scaled = scaler_X.fit_transform(X_augmented)
    
    scaler_L = StandardScaler()
    scaler_gap = StandardScaler()
    
    y_L_scaled = scaler_L.fit_transform(y_augmented[:, 0:1])
    y_gap_scaled = scaler_gap.fit_transform(y_augmented[:, 1:2])
    y_scaled = np.hstack([y_L_scaled, y_gap_scaled])
    
    print(f"Donn√©es finales: X{X_scaled.shape}, y{y_scaled.shape}")
    
    return X_scaled, y_scaled, scaler_X, scaler_L, scaler_gap

def main():
    """Fonction principale Neural Network 06-06-25 ULTRA."""
    
    print("="*80)
    print("üöÄ NEURAL NETWORK 06-06-25 ULTRA - FINAL EDITION")
    print("Objectif: Atteindre R¬≤ > 0.8 avec 5 am√©liorations suppl√©mentaires")
    print("="*80)
    
    # 1. Pr√©traitement ultra-optimis√©
    X_scaled, y_scaled, scaler_X, scaler_L, scaler_gap = load_and_preprocess_ultra()
    
    # 2. Charger donn√©es de test
    dataset_dir = "../data_generation/dataset"
    labels_df = pd.read_csv(os.path.join(dataset_dir, "labels.csv"))
    
    X_test = []
    y_test = []
    
    for idx, row in labels_df.iterrows():
        filename = row['filename']
        gap = round(row['gap_um'], 3)
        L_ecran = round(row['L_um'], 3)
        
        mat_filename = filename.replace('.png', '.mat')
        mat_path = os.path.join(dataset_dir, mat_filename)
        
        if os.path.exists(mat_path):
            try:
                data = sio.loadmat(mat_path)
                ratio = data['ratio'].flatten()[:600]
                X_test.append(ratio)
                y_test.append([L_ecran, gap])
            except:
                pass
    
    X_test = np.array(X_test, dtype=np.float32)
    y_test = np.array(y_test, dtype=np.float32)
    
    # Normaliser test
    X_test_scaled = scaler_X.transform(X_test)
    
    # 3. Division train/validation
    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)
    
    print(f"\nDonn√©es d'entra√Ænement: {X_train.shape}")
    
    # 4. Cr√©er et entra√Æner l'ensemble
    os.makedirs('models', exist_ok=True)
    models = create_ensemble_models(input_size=600, n_models=3)
    trained_models = train_ensemble_ultra(models, X_train, X_val, y_train, y_val)
    
    # 5. Pr√©diction ensemble
    print(f"\n=== PR√âDICTION ENSEMBLE ULTRA ===")
    y_pred_scaled = ensemble_predict(trained_models, X_test_scaled)
    
    # D√©normaliser
    y_pred_L = scaler_L.inverse_transform(y_pred_scaled[:, 0:1]).flatten()
    y_pred_gap = scaler_gap.inverse_transform(y_pred_scaled[:, 1:2]).flatten()
    y_pred = np.column_stack([y_pred_L, y_pred_gap])
    y_pred_rounded = np.round(y_pred, 3)
    
    # 6. √âvaluation finale
    r2_global = r2_score(y_test, y_pred_rounded)
    r2_L = r2_score(y_test[:, 0], y_pred_rounded[:, 0])
    r2_gap = r2_score(y_test[:, 1], y_pred_rounded[:, 1])
    
    rmse_L = np.sqrt(mean_squared_error(y_test[:, 0], y_pred_rounded[:, 0]))
    rmse_gap = np.sqrt(mean_squared_error(y_test[:, 1], y_pred_rounded[:, 1]))
    
    mae_L = mean_absolute_error(y_test[:, 0], y_pred_rounded[:, 0])
    mae_gap = mean_absolute_error(y_test[:, 1], y_pred_rounded[:, 1])
    
    print(f"\n{'='*80}")
    print(f"üéØ R√âSULTATS FINAUX NEURAL NETWORK 06-06-25 ULTRA")
    print(f"{'='*80}")
    
    print(f"PERFORMANCES ULTRA:")
    print(f"  R¬≤ global: {r2_global:.6f}")
    print(f"  R¬≤ L_ecran: {r2_L:.6f}")
    print(f"  R¬≤ gap: {r2_gap:.6f}")
    print(f"  RMSE L_ecran: {rmse_L:.6f} ¬µm")
    print(f"  RMSE gap: {rmse_gap:.6f} ¬µm")
    print(f"  MAE L_ecran: {mae_L:.6f} ¬µm")
    print(f"  MAE gap: {mae_gap:.6f} ¬µm")
    
    success = r2_global > 0.8
    print(f"\nüéØ OBJECTIF R¬≤ > 0.8: {'üéâ ATTEINT !' if success else '‚ö†Ô∏è NON ATTEINT'}")
    
    if success:
        print(f"üèÜ F√âLICITATIONS ! Objectif atteint avec R¬≤ = {r2_global:.6f}")
    else:
        print(f"üìà Progr√®s significatif ! R¬≤ = {r2_global:.6f} (proche de l'objectif)")
    
    # Sauvegarder les scalers
    import joblib
    joblib.dump(scaler_X, 'models/ultra_scaler_X.pkl')
    joblib.dump(scaler_L, 'models/ultra_scaler_L.pkl')
    joblib.dump(scaler_gap, 'models/ultra_scaler_gap.pkl')
    
    return {
        'r2_global': r2_global, 'r2_L': r2_L, 'r2_gap': r2_gap,
        'rmse_L': rmse_L, 'rmse_gap': rmse_gap,
        'mae_L': mae_L, 'mae_gap': mae_gap,
        'success': success
    }

if __name__ == "__main__":
    results = main()
