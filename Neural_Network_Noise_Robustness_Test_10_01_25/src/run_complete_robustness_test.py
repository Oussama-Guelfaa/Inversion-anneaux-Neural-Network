#!/usr/bin/env python3
"""
Script principal pour le test complet de robustesse au bruit

Ce script orchestre tous les tests de robustesse au bruit:
1. Test de base sans augmentation
2. Test avec augmentation de donn√©es
3. Comparaison et analyse compl√®te

Auteur: Oussama GUELFAA
Date: 10 - 01 - 2025
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path

def run_basic_robustness_test():
    """Ex√©cute le test de robustesse de base."""
    
    print("="*60)
    print("√âTAPE 1: TEST DE ROBUSTESSE DE BASE")
    print("="*60)
    
    try:
        from noise_robustness_test import main as run_noise_test
        
        # Simuler l'ex√©cution du test principal
        print("Lancement du test de robustesse au bruit...")
        
        # Importer et ex√©cuter le code principal
        exec(open('noise_robustness_test.py').read())
        
        print("‚úÖ Test de robustesse de base termin√©")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur dans le test de base: {e}")
        return False

def run_augmentation_test():
    """Ex√©cute le test avec augmentation de donn√©es."""
    
    print("\n" + "="*60)
    print("√âTAPE 2: TEST AVEC AUGMENTATION DE DONN√âES")
    print("="*60)
    
    try:
        from test_augmentation_robustness import main as run_aug_test
        
        print("Lancement du test avec augmentation...")
        
        # Importer et ex√©cuter le test d'augmentation
        exec(open('test_augmentation_robustness.py').read())
        
        print("‚úÖ Test avec augmentation termin√©")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur dans le test d'augmentation: {e}")
        return False

def generate_comprehensive_report():
    """G√©n√®re un rapport complet de tous les tests."""
    
    print("\n" + "="*60)
    print("√âTAPE 3: G√âN√âRATION DU RAPPORT COMPLET")
    print("="*60)
    
    try:
        # Charger les r√©sultats des diff√©rents tests
        results = {}
        
        # R√©sultats du test de base
        if os.path.exists('../results/noise_robustness_summary.json'):
            with open('../results/noise_robustness_summary.json', 'r') as f:
                results['basic_robustness'] = json.load(f)
        
        # R√©sultats du test d'augmentation
        if os.path.exists('../results/augmentation_comparison.json'):
            with open('../results/augmentation_comparison.json', 'r') as f:
                results['augmentation_comparison'] = json.load(f)
        
        # G√©n√©rer le rapport consolid√©
        report = generate_consolidated_report(results)
        
        # Sauvegarder le rapport
        with open('../results/comprehensive_robustness_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # G√©n√©rer le rapport markdown
        generate_markdown_report(report)
        
        print("‚úÖ Rapport complet g√©n√©r√©")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur dans la g√©n√©ration du rapport: {e}")
        return False

def generate_consolidated_report(results):
    """G√©n√®re un rapport consolid√© de tous les tests."""
    
    report = {
        'test_summary': {
            'test_date': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_type': 'comprehensive_noise_robustness',
            'tests_completed': list(results.keys())
        },
        'key_findings': {},
        'recommendations': {},
        'detailed_results': results
    }
    
    # Analyser les r√©sultats du test de base
    if 'basic_robustness' in results:
        basic_results = results['basic_robustness']
        
        # Trouver le seuil de tol√©rance
        threshold_80 = None
        for noise_level, data in basic_results.get('results_by_noise', {}).items():
            if data['r2'] >= 0.8:
                threshold_80 = int(noise_level)
            else:
                break
        
        report['key_findings']['noise_tolerance_threshold'] = threshold_80
        report['key_findings']['max_tested_noise'] = max([int(k) for k in basic_results.get('results_by_noise', {}).keys()])
        
        # Performance de r√©f√©rence (0% bruit)
        if '0' in basic_results.get('results_by_noise', {}):
            ref_performance = basic_results['results_by_noise']['0']
            report['key_findings']['reference_performance'] = {
                'r2': ref_performance['r2'],
                'rmse': ref_performance['rmse']
            }
    
    # Analyser les b√©n√©fices de l'augmentation
    if 'augmentation_comparison' in results:
        aug_results = results['augmentation_comparison']
        
        if 'comparison' in aug_results:
            improvements = []
            for noise_level, data in aug_results['comparison'].items():
                improvements.append(data['r2_improvement_percent'])
            
            avg_improvement = sum(improvements) / len(improvements) if improvements else 0
            report['key_findings']['augmentation_benefit'] = {
                'average_r2_improvement_percent': avg_improvement,
                'beneficial': avg_improvement > 1.0  # Seuil de 1% d'am√©lioration
            }
    
    # G√©n√©rer les recommandations
    report['recommendations'] = generate_recommendations(report['key_findings'])
    
    return report

def generate_recommendations(findings):
    """G√©n√®re des recommandations bas√©es sur les r√©sultats."""
    
    recommendations = {
        'acquisition_specifications': {},
        'model_deployment': {},
        'future_improvements': {}
    }
    
    # Recommandations d'acquisition
    threshold = findings.get('noise_tolerance_threshold')
    if threshold:
        if threshold >= 5:
            recommendations['acquisition_specifications'] = {
                'snr_requirement': f"SNR > {100/threshold:.1f}",
                'noise_level_max': f"{threshold}%",
                'acquisition_quality': "Standard - Mod√®le robuste"
            }
        elif threshold >= 2:
            recommendations['acquisition_specifications'] = {
                'snr_requirement': f"SNR > {100/threshold:.1f}",
                'noise_level_max': f"{threshold}%",
                'acquisition_quality': "√âlev√©e - Attention aux conditions"
            }
        else:
            recommendations['acquisition_specifications'] = {
                'snr_requirement': "SNR > 50",
                'noise_level_max': "< 2%",
                'acquisition_quality': "Tr√®s √©lev√©e - Conditions strictes"
            }
    
    # Recommandations de d√©ploiement
    ref_perf = findings.get('reference_performance', {})
    if ref_perf.get('r2', 0) > 0.95:
        recommendations['model_deployment'] = {
            'readiness': "Pr√™t pour d√©ploiement",
            'confidence_level': "√âlev√©e",
            'monitoring': "Standard"
        }
    else:
        recommendations['model_deployment'] = {
            'readiness': "N√©cessite optimisation",
            'confidence_level': "Mod√©r√©e",
            'monitoring': "Renforc√©"
        }
    
    # Recommandations d'am√©lioration
    aug_benefit = findings.get('augmentation_benefit', {})
    if aug_benefit.get('beneficial', False):
        recommendations['future_improvements'] = {
            'data_augmentation': "Recommand√©e - B√©n√©fice d√©montr√©",
            'priority': "Haute",
            'next_steps': ["Optimiser param√®tres d'augmentation", "Tester autres techniques"]
        }
    else:
        recommendations['future_improvements'] = {
            'data_augmentation': "Peu b√©n√©fique",
            'priority': "Basse",
            'next_steps': ["Explorer autres architectures", "Am√©liorer qualit√© des donn√©es"]
        }
    
    return recommendations

def generate_markdown_report(report):
    """G√©n√®re un rapport au format Markdown."""
    
    markdown_content = f"""# Rapport Complet - Test de Robustesse au Bruit

**Date:** {report['test_summary']['test_date']}  
**Auteur:** Oussama GUELFAA  
**Type de test:** {report['test_summary']['test_type']}

## üéØ R√©sum√© Ex√©cutif

### Tests R√©alis√©s
{', '.join(report['test_summary']['tests_completed'])}

### R√©sultats Cl√©s

#### Seuil de Tol√©rance au Bruit
- **Niveau maximum tol√©r√© (R¬≤ > 0.8):** {report['key_findings'].get('noise_tolerance_threshold', 'Non d√©termin√©')}%
- **Niveau maximum test√©:** {report['key_findings'].get('max_tested_noise', 'N/A')}%

#### Performance de R√©f√©rence (0% bruit)
- **R¬≤ Score:** {report['key_findings'].get('reference_performance', {}).get('r2', 'N/A'):.4f}
- **RMSE:** {report['key_findings'].get('reference_performance', {}).get('rmse', 'N/A'):.4f} ¬µm

#### B√©n√©fice de l'Augmentation de Donn√©es
- **Am√©lioration moyenne R¬≤:** {report['key_findings'].get('augmentation_benefit', {}).get('average_r2_improvement_percent', 'N/A'):.1f}%
- **Recommand√©e:** {'Oui' if report['key_findings'].get('augmentation_benefit', {}).get('beneficial', False) else 'Non'}

## üìã Recommandations

### Sp√©cifications d'Acquisition
- **SNR requis:** {report['recommendations']['acquisition_specifications'].get('snr_requirement', 'N/A')}
- **Niveau de bruit max:** {report['recommendations']['acquisition_specifications'].get('noise_level_max', 'N/A')}
- **Qualit√© d'acquisition:** {report['recommendations']['acquisition_specifications'].get('acquisition_quality', 'N/A')}

### D√©ploiement du Mod√®le
- **√âtat de pr√©paration:** {report['recommendations']['model_deployment'].get('readiness', 'N/A')}
- **Niveau de confiance:** {report['recommendations']['model_deployment'].get('confidence_level', 'N/A')}
- **Monitoring:** {report['recommendations']['model_deployment'].get('monitoring', 'N/A')}

### Am√©liorations Futures
- **Augmentation de donn√©es:** {report['recommendations']['future_improvements'].get('data_augmentation', 'N/A')}
- **Priorit√©:** {report['recommendations']['future_improvements'].get('priority', 'N/A')}

## üìä Fichiers G√©n√©r√©s

### R√©sultats Num√©riques
- `noise_robustness_summary.json` - R√©sultats du test de base
- `augmentation_comparison.json` - Comparaison avec/sans augmentation
- `comprehensive_robustness_report.json` - Rapport consolid√©

### Visualisations
- `noise_robustness_analysis.png` - Analyse de robustesse
- `predictions_by_noise.png` - Pr√©dictions par niveau de bruit
- `augmentation_comparison.png` - Comparaison augmentation

### Documentation
- `README.md` - Guide du projet
- `comprehensive_robustness_report.md` - Ce rapport

---

**Note:** Ce rapport synth√©tise tous les tests de robustesse au bruit effectu√©s sur le mod√®le de pr√©diction du gap.
"""
    
    with open('../results/comprehensive_robustness_report.md', 'w') as f:
        f.write(markdown_content)

def main():
    """Fonction principale pour ex√©cuter tous les tests."""
    
    print("üöÄ LANCEMENT DU TEST COMPLET DE ROBUSTESSE AU BRUIT")
    print("="*60)
    
    start_time = time.time()
    
    # Cr√©er les dossiers n√©cessaires
    os.makedirs("../models", exist_ok=True)
    os.makedirs("../plots", exist_ok=True)
    os.makedirs("../results", exist_ok=True)
    
    success_count = 0
    total_tests = 3
    
    # √âtape 1: Test de robustesse de base
    if run_basic_robustness_test():
        success_count += 1
    
    # √âtape 2: Test avec augmentation
    if run_augmentation_test():
        success_count += 1
    
    # √âtape 3: Rapport complet
    if generate_comprehensive_report():
        success_count += 1
    
    # R√©sum√© final
    total_time = time.time() - start_time
    
    print("\n" + "="*60)
    print("üèÅ R√âSUM√â FINAL DU TEST COMPLET")
    print("="*60)
    
    print(f"‚úÖ Tests r√©ussis: {success_count}/{total_tests}")
    print(f"‚è±Ô∏è  Temps total: {total_time/60:.1f} minutes")
    
    if success_count == total_tests:
        print("üéâ TOUS LES TESTS TERMIN√âS AVEC SUCC√àS!")
        print("\nüìÅ Fichiers g√©n√©r√©s:")
        print("   üìä ../results/ - R√©sultats num√©riques")
        print("   üìà ../plots/ - Graphiques et visualisations")
        print("   ü§ñ ../models/ - Mod√®les entra√Æn√©s")
        print("   üìã ../results/comprehensive_robustness_report.md - Rapport final")
    else:
        print("‚ö†Ô∏è  Certains tests ont √©chou√©. V√©rifiez les logs ci-dessus.")
    
    return success_count == total_tests

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Test complet de robustesse au bruit')
    parser.add_argument('--quick', action='store_true', 
                       help='Mode rapide avec moins de niveaux de bruit')
    parser.add_argument('--no-augmentation', action='store_true',
                       help='Ignorer les tests d\'augmentation')
    
    args = parser.parse_args()
    
    if args.quick:
        print("üèÉ Mode rapide activ√©")
    
    if args.no_augmentation:
        print("‚è≠Ô∏è  Tests d'augmentation ignor√©s")
    
    success = main()
    sys.exit(0 if success else 1)
