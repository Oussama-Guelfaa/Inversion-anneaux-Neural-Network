#!/usr/bin/env python3
"""
R√©entra√Ænement du r√©seau de neurones avec le nouveau dataset fusionn√©
Auteur: Oussama GUELFAA
Date: 12 - 06 - 2025

Script pour r√©entra√Æner le mod√®le existant avec les nouvelles donn√©es du dataset fusionn√©,
incluant l'augmentation par interpolation et le bruit synth√©tique de 5%.
"""

import os
import sys
import numpy as np
import pandas as pd
import scipy.io as sio
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.interpolate import interp1d

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau

import json
import time
import warnings
warnings.filterwarnings('ignore')

# Configuration pour reproductibilit√©
torch.manual_seed(42)
np.random.seed(42)

class EarlyStopping:
    """Early stopping pour √©viter l'overfitting."""
    
    def __init__(self, patience=20, min_delta=1e-6, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights and self.best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False

class IntensityDataset(Dataset):
    """Dataset PyTorch pour les profils d'intensit√©."""
    
    def __init__(self, intensity_profiles, gap_values):
        self.intensity_profiles = torch.FloatTensor(intensity_profiles)
        self.gap_values = torch.FloatTensor(gap_values)
    
    def __len__(self):
        return len(self.intensity_profiles)
    
    def __getitem__(self, idx):
        return self.intensity_profiles[idx], self.gap_values[idx]

class RobustGapPredictor(nn.Module):
    """Mod√®le robuste pour pr√©diction du gap - Architecture identique au mod√®le existant."""
    
    def __init__(self, input_size=600, dropout_rate=0.2):
        super(RobustGapPredictor, self).__init__()
        
        self.fc1 = nn.Linear(input_size, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.dropout1 = nn.Dropout(dropout_rate)
        
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout2 = nn.Dropout(dropout_rate)
        
        self.fc3 = nn.Linear(256, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.dropout3 = nn.Dropout(dropout_rate)
        
        self.fc4 = nn.Linear(128, 1)
        
    def forward(self, x):
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)
        
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)
        
        x = F.relu(self.bn3(self.fc3(x)))
        x = self.dropout3(x)
        
        x = self.fc4(x)
        return x

def load_dataset_from_folder(dataset_path="../data_generation/dataset"):
    """
    Charge les donn√©es depuis le dossier dataset fusionn√©.

    Cette fonction lit le fichier labels.csv pour obtenir les m√©tadonn√©es,
    puis charge chaque fichier .mat correspondant pour extraire les profils
    d'intensit√© (variable 'ratio'). Les profils sont normalis√©s √† 600 points.

    Args:
        dataset_path (str): Chemin vers le dossier dataset contenant labels.csv
                           et les fichiers .mat individuels

    Returns:
        tuple: (intensity_profiles, gap_values) o√π:
               - intensity_profiles: array (n_samples, 600) des profils
               - gap_values: array (n_samples,) des valeurs de gap en ¬µm
    """
    print("üîÑ Chargement des donn√©es depuis le dataset fusionn√©...")
    
    dataset_path = Path(dataset_path)
    labels_path = dataset_path / "labels.csv"
    
    if not labels_path.exists():
        print(f"‚ùå Erreur: Fichier labels.csv non trouv√© dans {dataset_path}")
        print(f"   V√©rifiez que le chemin est correct et que le fichier existe")
        return None, None
    
    # Charger les m√©tadonn√©es
    labels_df = pd.read_csv(labels_path)
    print(f"üìä {len(labels_df)} √©chantillons trouv√©s dans labels.csv")
    
    intensity_profiles = []
    gap_values = []
    
    for _, row in labels_df.iterrows():
        gap_value = row['gap_um']
        
        # Construire le nom du fichier .mat correspondant
        mat_filename = f"gap_{gap_value:.4f}um_L_10.000um.mat"
        mat_path = dataset_path / mat_filename
        
        if mat_path.exists():
            try:
                # Charger le fichier .mat
                mat_data = sio.loadmat(mat_path)
                
                # Extraire le profil d'intensit√© (variable 'ratio')
                if 'ratio' in mat_data:
                    profile = mat_data['ratio'].flatten()
                    
                    # Tronquer √† 600 points si n√©cessaire
                    if len(profile) > 600:
                        profile = profile[:600]
                    elif len(profile) < 600:
                        # Interpoler si moins de 600 points
                        x_old = np.linspace(0, 1, len(profile))
                        x_new = np.linspace(0, 1, 600)
                        f = interp1d(x_old, profile, kind='linear')
                        profile = f(x_new)
                    
                    intensity_profiles.append(profile)
                    gap_values.append(gap_value)
                else:
                    print(f"‚ö†Ô∏è Variable 'ratio' non trouv√©e dans {mat_filename}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du chargement de {mat_filename}: {e}")
        else:
            print(f"‚ö†Ô∏è Fichier {mat_filename} non trouv√©")
    
    intensity_profiles = np.array(intensity_profiles)
    gap_values = np.array(gap_values)
    
    print(f"‚úÖ {len(intensity_profiles)} √©chantillons charg√©s avec succ√®s")
    print(f"   Forme des profils: {intensity_profiles.shape}")
    print(f"   Plage de gap: {gap_values.min():.3f} √† {gap_values.max():.3f} ¬µm")
    print(f"   Valeurs manquantes: {np.isnan(intensity_profiles).sum()} points")
    print(f"   Profils valides: {len(intensity_profiles)} / {len(labels_df)} fichiers")
    
    return intensity_profiles, gap_values

def augment_data_by_interpolation(X, y, factor=2):
    """
    Augmente les donn√©es par interpolation entre √©chantillons adjacents.
    
    Args:
        X (np.array): Profils d'intensit√©
        y (np.array): Valeurs de gap
        factor (int): Facteur d'augmentation (2 = doubler le dataset)
        
    Returns:
        tuple: (X_augmented, y_augmented)
    """
    print(f"üîÑ Augmentation des donn√©es par interpolation (facteur {factor})...")
    
    # Trier par valeur de gap pour interpolation coh√©rente
    sort_indices = np.argsort(y)
    X_sorted = X[sort_indices]
    y_sorted = y[sort_indices]
    
    X_augmented = [X_sorted]
    y_augmented = [y_sorted]
    
    # G√©n√©rer des √©chantillons interpol√©s
    for i in range(factor - 1):
        X_interp = []
        y_interp = []
        
        for j in range(len(X_sorted) - 1):
            # Interpolation lin√©aire entre √©chantillons adjacents
            alpha = (i + 1) / factor
            
            profile_interp = (1 - alpha) * X_sorted[j] + alpha * X_sorted[j + 1]
            gap_interp = (1 - alpha) * y_sorted[j] + alpha * y_sorted[j + 1]
            
            X_interp.append(profile_interp)
            y_interp.append(gap_interp)
        
        X_augmented.append(np.array(X_interp))
        y_augmented.append(np.array(y_interp))
    
    # Concat√©ner tous les √©chantillons
    X_final = np.concatenate(X_augmented, axis=0)
    y_final = np.concatenate(y_augmented, axis=0)
    
    print(f"‚úÖ Augmentation termin√©e: {len(X)} ‚Üí {len(X_final)} √©chantillons")
    print(f"   Facteur d'augmentation r√©alis√©: {len(X_final)/len(X):.2f}x")
    print(f"   √âchantillons interpol√©s g√©n√©r√©s: {len(X_final) - len(X)}")
    
    return X_final, y_final

def add_gaussian_noise(X, noise_level_percent=5):
    """
    Ajoute du bruit gaussien proportionnel au signal.
    
    Args:
        X (np.array): Donn√©es d'entr√©e
        noise_level_percent (float): Niveau de bruit en pourcentage
        
    Returns:
        np.array: Donn√©es avec bruit ajout√©
    """
    if noise_level_percent == 0:
        return X.copy()
    
    # Calculer l'√©cart-type du signal pour chaque √©chantillon
    signal_std = np.std(X, axis=1, keepdims=True)
    
    # G√©n√©rer le bruit proportionnel
    noise_std = (noise_level_percent / 100.0) * signal_std
    noise = np.random.normal(0, noise_std, X.shape)
    
    X_noisy = X + noise
    
    print(f"üîä Bruit {noise_level_percent}% ajout√© - SNR moyen: {1/(noise_level_percent/100):.1f}")
    print(f"   √âcart-type du bruit: {np.mean(noise_std):.6f}")
    
    return X_noisy

def prepare_stratified_splits(X, y, test_size=0.2, random_state=42):
    """
    Divise les donn√©es en train/test avec stratification par plage de gap.
    
    Args:
        X (np.array): Profils d'intensit√©
        y (np.array): Valeurs de gap
        test_size (float): Proportion du jeu de test
        random_state (int): Graine al√©atoire
        
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    print(f"üìä Division stratifi√©e des donn√©es (test: {test_size*100:.0f}%)...")
    
    # Cr√©er des bins pour stratification
    n_bins = 10
    gap_bins = pd.cut(y, bins=n_bins, labels=False)
    
    # Division stratifi√©e
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, 
        stratify=gap_bins
    )
    
    print(f"   Train: {len(X_train)} √©chantillons")
    print(f"   Test: {len(X_test)} √©chantillons")
    
    # V√©rifier la distribution
    print(f"   Distribution train - Gap: {y_train.min():.3f} √† {y_train.max():.3f} ¬µm")
    print(f"   Distribution test - Gap: {y_test.min():.3f} √† {y_test.max():.3f} ¬µm")
    
    return X_train, X_test, y_train, y_test

def train_model(X_train, y_train, X_val, y_val, noise_level=5):
    """
    Entra√Æne le mod√®le avec les nouvelles donn√©es.

    Args:
        X_train, y_train: Donn√©es d'entra√Ænement
        X_val, y_val: Donn√©es de validation
        noise_level: Niveau de bruit en pourcentage

    Returns:
        tuple: (model, scaler, history, training_time)
    """
    print(f"\nüöÄ ENTRA√éNEMENT AVEC {noise_level}% DE BRUIT")

    start_time = time.time()

    # Ajouter du bruit aux donn√©es d'entra√Ænement UNIQUEMENT
    X_train_noisy = add_gaussian_noise(X_train, noise_level)

    # Normalisation
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_noisy)
    X_val_scaled = scaler.transform(X_val)  # Validation SANS bruit

    # Datasets et DataLoaders
    train_dataset = IntensityDataset(X_train_scaled, y_train)
    val_dataset = IntensityDataset(X_val_scaled, y_val)

    batch_size = 16
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # Mod√®le et optimisation
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = RobustGapPredictor(input_size=X_train.shape[1]).to(device)

    learning_rate = 0.0001
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    criterion = nn.MSELoss()

    early_stopping = EarlyStopping(patience=25)

    # Historique d'entra√Ænement
    history = {'train_loss': [], 'val_loss': [], 'train_r2': [], 'val_r2': []}

    max_epochs = 150
    print(f"üìà Entra√Ænement sur {device}")

    for epoch in range(max_epochs):
        # Phase d'entra√Ænement
        model.train()
        train_loss = 0.0
        train_predictions = []
        train_targets = []

        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)

            optimizer.zero_grad()
            outputs = model(batch_X).squeeze()
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_predictions.extend(outputs.detach().cpu().numpy())
            train_targets.extend(batch_y.detach().cpu().numpy())

        train_loss /= len(train_loader)
        train_r2 = r2_score(train_targets, train_predictions)

        # Phase de validation
        model.eval()
        val_loss = 0.0
        val_predictions = []
        val_targets = []

        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X).squeeze()
                loss = criterion(outputs, batch_y)

                val_loss += loss.item()
                val_predictions.extend(outputs.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())

        val_loss /= len(val_loader)
        val_r2 = r2_score(val_targets, val_predictions)

        # Mise √† jour historique
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_r2'].append(train_r2)
        history['val_r2'].append(val_r2)

        # Scheduler et early stopping
        scheduler.step(val_loss)

        if (epoch + 1) % 20 == 0:
            print(f"   Epoch {epoch+1:3d}: Train R¬≤={train_r2:.4f}, Val R¬≤={val_r2:.4f}, LR={optimizer.param_groups[0]['lr']:.2e}")

        if early_stopping(val_loss, model):
            print(f"   ‚èπÔ∏è Early stopping √† l'√©poque {epoch+1}")
            break

    training_time = time.time() - start_time

    # Sauvegarder le mod√®le r√©entra√Æn√© avec facteur 3
    model_path = "models/model_retrained_5percent_factor3.pth"
    torch.save(model.state_dict(), model_path)

    print(f"‚úÖ Entra√Ænement termin√© en {training_time:.1f}s")
    print(f"   Performance finale: R¬≤ = {val_r2:.4f}")

    return model, scaler, history, training_time

def evaluate_model(model, scaler, X_test, y_test):
    """
    √âvalue le mod√®le sur l'ensemble de test.

    Args:
        model: Mod√®le entra√Æn√©
        scaler: Normalisateur
        X_test, y_test: Donn√©es de test

    Returns:
        dict: M√©triques d'√©valuation
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    # Normalisation et pr√©diction
    X_test_scaled = scaler.transform(X_test)

    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_test_scaled).to(device)
        y_pred = model(X_tensor).squeeze().cpu().numpy()

    # Calcul des m√©triques
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    # Analyse par plage de gap
    gap_ranges = [(0.0, 1.0), (1.0, 2.0), (2.0, 3.0)]
    range_metrics = {}

    for gap_min, gap_max in gap_ranges:
        mask = (y_test >= gap_min) & (y_test < gap_max)
        if np.sum(mask) > 0:
            r2_range = r2_score(y_test[mask], y_pred[mask])
            rmse_range = np.sqrt(mean_squared_error(y_test[mask], y_pred[mask]))
            range_metrics[f"{gap_min}-{gap_max}¬µm"] = {
                'r2': r2_range,
                'rmse': rmse_range,
                'n_samples': np.sum(mask)
            }

    metrics = {
        'r2': r2,
        'mse': mse,
        'mae': mae,
        'rmse': rmse,
        'predictions': y_pred,
        'range_metrics': range_metrics
    }

    return metrics

def create_analysis_plots(y_test, y_pred, history, range_metrics):
    """
    Cr√©e les graphiques d'analyse des r√©sultats.

    Args:
        y_test: Valeurs r√©elles
        y_pred: Pr√©dictions
        history: Historique d'entra√Ænement
        range_metrics: M√©triques par plage
    """
    print("üìä G√©n√©ration des graphiques d'analyse...")

    plt.figure(figsize=(20, 12))

    # 1. Courbes d'entra√Ænement
    plt.subplot(2, 4, 1)
    plt.plot(history['train_loss'], label='Train Loss', color='blue')
    plt.plot(history['val_loss'], label='Val Loss', color='red')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Courbes de Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(2, 4, 2)
    plt.plot(history['train_r2'], label='Train R¬≤', color='blue')
    plt.plot(history['val_r2'], label='Val R¬≤', color='red')
    plt.xlabel('Epoch')
    plt.ylabel('R¬≤ Score')
    plt.title('Courbes de R¬≤')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 2. Scatter plot pr√©dictions vs r√©alit√©
    plt.subplot(2, 4, 3)
    plt.scatter(y_test, y_pred, alpha=0.6, s=20)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Gap R√©el (¬µm)')
    plt.ylabel('Gap Pr√©dit (¬µm)')
    plt.title('Pr√©dictions vs R√©alit√©')
    plt.grid(True, alpha=0.3)

    # 3. Erreurs absolues
    plt.subplot(2, 4, 4)
    errors = np.abs(y_pred - y_test)
    plt.scatter(y_test, errors, alpha=0.6, s=20)
    plt.xlabel('Gap R√©el (¬µm)')
    plt.ylabel('Erreur Absolue (¬µm)')
    plt.title('Erreurs Absolues')
    plt.grid(True, alpha=0.3)

    # 4. Distribution des erreurs
    plt.subplot(2, 4, 5)
    plt.hist(y_pred - y_test, bins=50, alpha=0.7, edgecolor='black')
    plt.xlabel('Erreur (¬µm)')
    plt.ylabel('Fr√©quence')
    plt.title('Distribution des Erreurs')
    plt.grid(True, alpha=0.3)

    # 5. Performance par plage
    plt.subplot(2, 4, 6)
    ranges = list(range_metrics.keys())
    r2_values = [range_metrics[r]['r2'] for r in ranges]
    plt.bar(ranges, r2_values, alpha=0.7)
    plt.ylabel('R¬≤ Score')
    plt.title('Performance par Plage de Gap')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    # 6. RMSE par plage
    plt.subplot(2, 4, 7)
    rmse_values = [range_metrics[r]['rmse'] for r in ranges]
    plt.bar(ranges, rmse_values, alpha=0.7, color='orange')
    plt.ylabel('RMSE (¬µm)')
    plt.title('RMSE par Plage de Gap')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    # 7. Zone critique [1.75-2.00 ¬µm]
    plt.subplot(2, 4, 8)
    critical_mask = (y_test >= 1.75) & (y_test <= 2.00)
    if np.sum(critical_mask) > 0:
        plt.scatter(y_test[critical_mask], y_pred[critical_mask], alpha=0.8, s=30, color='red')
        plt.plot([1.75, 2.00], [1.75, 2.00], 'k--', lw=2)
        plt.xlabel('Gap R√©el (¬µm)')
        plt.ylabel('Gap Pr√©dit (¬µm)')
        plt.title('Zone Critique [1.75-2.00 ¬µm]')
        plt.grid(True, alpha=0.3)

        # Calculer R¬≤ pour la zone critique
        if np.sum(critical_mask) > 1:
            r2_critical = r2_score(y_test[critical_mask], y_pred[critical_mask])
            plt.text(0.05, 0.95, f'R¬≤ = {r2_critical:.3f}', transform=plt.gca().transAxes,
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.tight_layout()
    plt.savefig('plots/retrained_model_analysis_factor3.png', dpi=300, bbox_inches='tight')
    plt.close()

    print("‚úÖ Graphiques sauvegard√©s dans plots/retrained_model_analysis_factor3.png")

def save_results(metrics, y_test, y_pred, training_time):
    """
    Sauvegarde les r√©sultats du r√©entra√Ænement.

    Args:
        metrics: M√©triques d'√©valuation
        y_test: Valeurs r√©elles
        y_pred: Pr√©dictions
        training_time: Temps d'entra√Ænement
    """
    print("üíæ Sauvegarde des r√©sultats...")

    # R√©sum√© des performances (conversion en types Python natifs pour JSON)
    range_performance = {}
    for range_name, range_data in metrics['range_metrics'].items():
        range_performance[range_name] = {
            'r2': float(range_data['r2']),
            'rmse': float(range_data['rmse']),
            'n_samples': int(range_data['n_samples'])
        }

    summary = {
        'model_type': 'RobustGapPredictor_Retrained',
        'dataset': 'dataset_merged (0.005-3.000¬µm)',
        'augmentation': 'interpolation_factor_3',
        'noise_level': '5%',
        'training_time_s': float(training_time),
        'performance': {
            'r2_score': float(metrics['r2']),
            'rmse_um': float(metrics['rmse']),
            'mae_um': float(metrics['mae'])
        },
        'range_performance': range_performance
    }

    # Sauvegarder le r√©sum√© avec facteur 3
    with open('results/retrained_model_summary_factor3.json', 'w') as f:
        json.dump(summary, f, indent=2)

    # Sauvegarder les pr√©dictions d√©taill√©es avec facteur 3
    predictions_df = pd.DataFrame({
        'gap_true': y_test,
        'gap_predicted': y_pred,
        'error': y_pred - y_test,
        'absolute_error': np.abs(y_pred - y_test)
    })
    predictions_df.to_csv('results/retrained_predictions_factor3.csv', index=False)

    print("‚úÖ R√©sultats sauvegard√©s!")

def main():
    """
    Fonction principale de r√©entra√Ænement.
    """
    print("üî¨ R√âENTRA√éNEMENT AVEC NOUVEAU DATASET FUSIONN√â")
    print("=" * 60)

    # 1. Charger les donn√©es du dataset fusionn√©
    intensity_profiles, gap_values = load_dataset_from_folder("../data_generation/dataset")

    if intensity_profiles is None:
        print("‚ùå √âchec du chargement des donn√©es. Arr√™t du programme.")
        return

    # 2. Augmentation par interpolation avec facteur 3
    print("\nüìà AUGMENTATION DES DONN√âES")
    X_augmented, y_augmented = augment_data_by_interpolation(
        intensity_profiles, gap_values, factor=3
    )

    # 3. Division stratifi√©e des donn√©es (80% train, 20% test)
    print("\nüìä DIVISION DES DONN√âES")
    X_train_full, X_test, y_train_full, y_test = prepare_stratified_splits(
        X_augmented, y_augmented, test_size=0.2, random_state=42
    )

    # 4. Division train/validation (80% train, 20% validation du train_full)
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full, test_size=0.2, random_state=42
    )

    print(f"   Division finale:")
    print(f"   Train: {len(X_train)} √©chantillons")
    print(f"   Validation: {len(X_val)} √©chantillons")
    print(f"   Test: {len(X_test)} √©chantillons")

    # 5. Entra√Ænement avec bruit de 5%
    print("\nüöÄ ENTRA√éNEMENT DU MOD√àLE")
    model, scaler, history, training_time = train_model(
        X_train, y_train, X_val, y_val, noise_level=5
    )

    # 6. √âvaluation sur le jeu de test
    print("\nüìä √âVALUATION SUR LE JEU DE TEST")
    metrics = evaluate_model(model, scaler, X_test, y_test)

    print(f"\nüéØ R√âSULTATS FINAUX:")
    print(f"   R¬≤ Score: {metrics['r2']:.4f}")
    print(f"   RMSE: {metrics['rmse']:.4f} ¬µm")
    print(f"   MAE: {metrics['mae']:.4f} ¬µm")
    print(f"   Temps d'entra√Ænement: {training_time:.1f}s")
    print(f"   √âchantillons de test: {len(y_test)}")

    print(f"\nüìà PERFORMANCE PAR PLAGE:")
    for range_name, range_metrics in metrics['range_metrics'].items():
        print(f"   {range_name}: R¬≤={range_metrics['r2']:.4f}, "
              f"RMSE={range_metrics['rmse']:.4f} ¬µm "
              f"({range_metrics['n_samples']} √©chantillons)")

    # 7. Analyse de la zone critique [1.75-2.00 ¬µm]
    critical_mask = (y_test >= 1.75) & (y_test <= 2.00)
    if np.sum(critical_mask) > 1:
        y_pred = metrics['predictions']
        r2_critical = r2_score(y_test[critical_mask], y_pred[critical_mask])
        rmse_critical = np.sqrt(mean_squared_error(y_test[critical_mask], y_pred[critical_mask]))
        print(f"\nüéØ ZONE CRITIQUE [1.75-2.00 ¬µm]:")
        print(f"   R¬≤ Score: {r2_critical:.4f}")
        print(f"   RMSE: {rmse_critical:.4f} ¬µm")
        print(f"   √âchantillons: {np.sum(critical_mask)}")

    # 8. G√©n√©ration des graphiques
    print("\nüìä G√âN√âRATION DES ANALYSES VISUELLES")
    create_analysis_plots(y_test, metrics['predictions'], history, metrics['range_metrics'])

    # 9. Sauvegarde des r√©sultats
    save_results(metrics, y_test, metrics['predictions'], training_time)

    # 10. Comparaison avec le mod√®le pr√©c√©dent
    print(f"\nüìã COMPARAISON AVEC LE MOD√àLE PR√âC√âDENT:")
    print(f"   Nouveau dataset: {len(intensity_profiles)} √©chantillons originaux")
    print(f"   Apr√®s augmentation (facteur 3): {len(X_augmented)} √©chantillons")
    print(f"   Am√©lioration vs facteur 2: +{len(X_augmented) - (len(intensity_profiles)*2)} √©chantillons")
    print(f"   Plage √©tendue: 0.005 - 3.000 ¬µm")
    print(f"   Focus zone critique: [1.75-2.00 ¬µm]")

    if metrics['r2'] > 0.8:
        print(f"‚úÖ OBJECTIF ATTEINT: R¬≤ = {metrics['r2']:.4f} > 0.8")
    else:
        print(f"‚ö†Ô∏è OBJECTIF NON ATTEINT: R¬≤ = {metrics['r2']:.4f} < 0.8")

    print("\nüèÅ R√©entra√Ænement termin√© avec succ√®s!")

    return model, scaler, metrics

if __name__ == "__main__":
    # Cr√©er les dossiers n√©cessaires
    os.makedirs('models', exist_ok=True)
    os.makedirs('plots', exist_ok=True)
    os.makedirs('results', exist_ok=True)

    # Lancer le r√©entra√Ænement
    model, scaler, metrics = main()
