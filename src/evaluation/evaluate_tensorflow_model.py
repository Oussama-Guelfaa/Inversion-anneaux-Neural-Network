#!/usr/bin/env python3
"""
TensorFlow Model Evaluation
Author: Oussama GUELFAA
Date: 05 - 06 - 2025

√âvalue le mod√®le TensorFlow entra√Æn√© sur les donn√©es exp√©rimentales.
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import scipy.io as sio
import joblib
import os

def load_test_data():
    """Charge les donn√©es de test exp√©rimentales."""
    
    print("=== CHARGEMENT DES DONN√âES DE TEST ===")
    
    dataset_dir = "../data_generation/dataset"
    labels_df = pd.read_csv(os.path.join(dataset_dir, "labels.csv"))
    
    X_test = []
    y_test = []
    filenames = []
    
    for idx, row in labels_df.iterrows():
        filename = row['filename']
        gap = row['gap_um']
        L_ecran = row['L_um']
        
        mat_filename = filename.replace('.png', '.mat')
        mat_path = os.path.join(dataset_dir, mat_filename)
        
        if os.path.exists(mat_path):
            try:
                data = sio.loadmat(mat_path)
                ratio = data['ratio'].flatten()
                
                X_test.append(ratio)
                y_test.append([L_ecran, gap])  # Ordre: L_ecran, gap
                filenames.append(filename)
                
            except Exception as e:
                print(f"Erreur {mat_filename}: {e}")
    
    X_test = np.array(X_test, dtype='float32')
    y_test = np.array(y_test, dtype='float32')
    
    print(f"Donn√©es de test charg√©es: X{X_test.shape}, y{y_test.shape}")
    return X_test, y_test, filenames

def evaluate_tensorflow_model():
    """√âvalue le mod√®le TensorFlow sur les donn√©es exp√©rimentales."""
    
    print("\n=== √âVALUATION DU MOD√àLE TENSORFLOW ===")
    
    # Charger les donn√©es de test
    X_test, y_test, filenames = load_test_data()
    
    # Charger le mod√®le entra√Æn√© (utiliser le mod√®le existant)
    try:
        # Essayer de charger le mod√®le depuis l'entra√Ænement pr√©c√©dent
        model_path = 'models/tensorflow_best_model.keras'
        if not os.path.exists(model_path):
            # Fallback vers le mod√®le .h5 s'il existe
            model_path = 'models/tensorflow_best_model.h5'
        
        print(f"Chargement du mod√®le: {model_path}")
        
        # Charger avec compile=False pour √©viter les probl√®mes de s√©rialisation
        model = tf.keras.models.load_model(model_path, compile=False)
        
        # Recompiler le mod√®le
        model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )
        
        print("Mod√®le charg√© et recompil√© avec succ√®s")
        
    except Exception as e:
        print(f"Erreur lors du chargement du mod√®le: {e}")
        print("Utilisation du mod√®le depuis l'entra√Ænement en cours...")
        return None
    
    # Charger le scaler
    try:
        scaler_X = joblib.load('models/tensorflow_scaler_X.pkl')
        print("Scaler charg√© avec succ√®s")
    except:
        print("Erreur: Impossible de charger le scaler")
        print("Cr√©er un scaler temporaire...")
        # Cr√©er un scaler temporaire bas√© sur les donn√©es de test
        scaler_X = StandardScaler()
        scaler_X.fit(X_test)
    
    # Normaliser les donn√©es de test
    X_test_scaled = scaler_X.transform(X_test)
    
    # Pr√©dictions
    print("G√©n√©ration des pr√©dictions...")
    y_pred = model.predict(X_test_scaled, verbose=0)
    
    # Calculer les m√©triques
    r2_global = r2_score(y_test, y_pred)
    r2_L = r2_score(y_test[:, 0], y_pred[:, 0])
    r2_gap = r2_score(y_test[:, 1], y_pred[:, 1])
    
    rmse_L = np.sqrt(mean_squared_error(y_test[:, 0], y_pred[:, 0]))
    rmse_gap = np.sqrt(mean_squared_error(y_test[:, 1], y_pred[:, 1]))
    
    mae_L = mean_absolute_error(y_test[:, 0], y_pred[:, 0])
    mae_gap = mean_absolute_error(y_test[:, 1], y_pred[:, 1])
    
    # Erreurs relatives
    mape_L = np.mean(np.abs((y_test[:, 0] - y_pred[:, 0]) / y_test[:, 0])) * 100
    mape_gap = np.mean(np.abs((y_test[:, 1] - y_pred[:, 1]) / y_test[:, 1])) * 100
    
    print(f"\n{'='*60}")
    print(f"R√âSULTATS TENSORFLOW/KERAS")
    print(f"{'='*60}")
    print(f"M√©triques de performance:")
    print(f"  R¬≤ global: {r2_global:.6f}")
    print(f"  R¬≤ L_ecran: {r2_L:.6f}")
    print(f"  R¬≤ gap: {r2_gap:.6f}")
    print(f"  RMSE L_ecran: {rmse_L:.6f} ¬µm")
    print(f"  RMSE gap: {rmse_gap:.6f} ¬µm")
    print(f"  MAE L_ecran: {mae_L:.6f} ¬µm")
    print(f"  MAE gap: {mae_gap:.6f} ¬µm")
    print(f"  MAPE L_ecran: {mape_L:.2f}%")
    print(f"  MAPE gap: {mape_gap:.2f}%")
    
    success = r2_global > 0.8
    print(f"  Objectif R¬≤ > 0.8: {'‚úì ATTEINT' if success else '‚úó NON ATTEINT'}")
    
    # Cr√©er des visualisations
    create_tensorflow_visualizations(y_test, y_pred, {
        'r2_global': r2_global, 'r2_L': r2_L, 'r2_gap': r2_gap,
        'rmse_L': rmse_L, 'rmse_gap': rmse_gap,
        'mae_L': mae_L, 'mae_gap': mae_gap,
        'mape_L': mape_L, 'mape_gap': mape_gap,
        'success': success
    })
    
    return y_pred, {
        'r2_global': r2_global, 'r2_L': r2_L, 'r2_gap': r2_gap,
        'rmse_L': rmse_L, 'rmse_gap': rmse_gap,
        'mae_L': mae_L, 'mae_gap': mae_gap,
        'mape_L': mape_L, 'mape_gap': mape_gap,
        'success': success
    }

def create_tensorflow_visualizations(y_test, y_pred, metrics):
    """Cr√©e des visualisations pour l'√©valuation TensorFlow."""
    
    print("\n=== G√âN√âRATION DES VISUALISATIONS TENSORFLOW ===")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. Pr√©dictions vs Vraies valeurs L_ecran
    ax1.scatter(y_test[:, 0], y_pred[:, 0], alpha=0.7, s=50, color='blue')
    ax1.plot([y_test[:, 0].min(), y_test[:, 0].max()], 
             [y_test[:, 0].min(), y_test[:, 0].max()], 'r--', lw=2)
    ax1.set_xlabel('L_ecran vraie (¬µm)')
    ax1.set_ylabel('L_ecran pr√©dite (¬µm)')
    ax1.set_title(f'TensorFlow - L_ecran: R¬≤ = {metrics["r2_L"]:.4f}')
    ax1.grid(True, alpha=0.3)
    
    # 2. Pr√©dictions vs Vraies valeurs gap
    ax2.scatter(y_test[:, 1], y_pred[:, 1], alpha=0.7, s=50, color='green')
    ax2.plot([y_test[:, 1].min(), y_test[:, 1].max()], 
             [y_test[:, 1].min(), y_test[:, 1].max()], 'r--', lw=2)
    ax2.set_xlabel('gap vraie (¬µm)')
    ax2.set_ylabel('gap pr√©dite (¬µm)')
    ax2.set_title(f'TensorFlow - gap: R¬≤ = {metrics["r2_gap"]:.4f}')
    ax2.grid(True, alpha=0.3)
    
    # 3. Erreurs absolues L_ecran
    errors_L = np.abs(y_test[:, 0] - y_pred[:, 0])
    ax3.hist(errors_L, bins=15, alpha=0.7, edgecolor='black', color='blue')
    ax3.set_xlabel('Erreur absolue L_ecran (¬µm)')
    ax3.set_ylabel('Fr√©quence')
    ax3.set_title(f'Distribution erreurs L_ecran\nMAE = {metrics["mae_L"]:.4f}')
    ax3.grid(True, alpha=0.3)
    
    # 4. Erreurs absolues gap
    errors_gap = np.abs(y_test[:, 1] - y_pred[:, 1])
    ax4.hist(errors_gap, bins=15, alpha=0.7, edgecolor='black', color='green')
    ax4.set_xlabel('Erreur absolue gap (¬µm)')
    ax4.set_ylabel('Fr√©quence')
    ax4.set_title(f'Distribution erreurs gap\nMAE = {metrics["mae_gap"]:.4f}')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Sauvegarder
    os.makedirs('plots', exist_ok=True)
    plt.savefig('plots/tensorflow_evaluation.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("Visualisations sauvegard√©es: plots/tensorflow_evaluation.png")
    
    # Cr√©er un graphique de comparaison des m√©triques
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    params = ['L_ecran', 'gap']
    r2_scores = [metrics['r2_L'], metrics['r2_gap']]
    colors = ['green' if r2 > 0.8 else 'orange' if r2 > 0.5 else 'red' for r2 in r2_scores]
    
    bars = ax.bar(params, r2_scores, color=colors, alpha=0.7, edgecolor='black')
    ax.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Objectif R¬≤ = 0.8')
    ax.set_ylabel('R¬≤ Score')
    ax.set_title(f'Performance TensorFlow par Param√®tre\nR¬≤ Global = {metrics["r2_global"]:.4f}')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Ajouter les valeurs sur les barres
    for bar, value in zip(bars, r2_scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('plots/tensorflow_r2_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("Comparaison R¬≤ sauvegard√©e: plots/tensorflow_r2_comparison.png")

def main():
    """Fonction principale d'√©valuation."""
    
    print("="*80)
    print("√âVALUATION MOD√àLE TENSORFLOW/KERAS")
    print("="*80)
    
    # √âvaluer le mod√®le
    result = evaluate_tensorflow_model()
    
    if result is not None:
        y_pred, metrics = result
        
        print(f"\n{'='*80}")
        print(f"R√âSUM√â FINAL TENSORFLOW")
        print(f"{'='*80}")
        print(f"Le mod√®le TensorFlow/Keras a √©t√© √©valu√© avec succ√®s!")
        print(f"Performance globale: R¬≤ = {metrics['r2_global']:.6f}")
        
        if metrics['success']:
            print("üéâ OBJECTIF ATTEINT: R¬≤ > 0.8")
        else:
            print("‚ùå Objectif non atteint, mais des am√©liorations sont possibles")
            
        print(f"\nFichiers g√©n√©r√©s:")
        print(f"  ‚Ä¢ plots/tensorflow_evaluation.png")
        print(f"  ‚Ä¢ plots/tensorflow_r2_comparison.png")
    else:
        print("‚ùå Impossible d'√©valuer le mod√®le TensorFlow")
        print("Assurez-vous que le mod√®le a √©t√© entra√Æn√© avec train_tensorflow_model.py")

if __name__ == "__main__":
    main()
