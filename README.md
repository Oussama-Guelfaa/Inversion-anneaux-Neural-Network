# ðŸ”¬ Inversion d'Anneaux - Neural Network Project

**Author:** Oussama GUELFAA
**Date:** 10 - 01 - 2025

## ðŸ“– Project Overview

This project implements **7 modular neural network solutions** for holographic ring analysis and parameter prediction. Each neural network is organized as an independent, self-contained unit with standardized structure for easy deployment, comparison, and archiving.

## ðŸŽ¯ Objectives

- **Primary Goal**: Predict gap and L_ecran parameters from 1D intensity profiles
- **Data Source**: Holographic intensity ratios (I_subs/I_subs_inc) from MATLAB files
- **Target Accuracy**: RÂ² > 0.8 for regression tasks
- **Architecture**: 1D profile-based neural networks (preferred over 2D CNN approaches)
- **Modularity**: Each network as independent, deployable unit

## ðŸ—ï¸ Project Structure

The project follows a modular architecture where each neural network is self-contained:

```
Inversion_anneaux/
â”œâ”€â”€ ðŸ”¬ Reseau_Gap_Prediction_CNN/          # CNN for gap parameter prediction
â”œâ”€â”€ ðŸ”Š Reseau_Noise_Robustness/            # Noise robustness testing
â”œâ”€â”€ ðŸ§ª Reseau_Overfitting_Test/            # Overfitting validation
â”œâ”€â”€ ðŸ§  Reseau_Advanced_Regressor/          # Advanced regressor with attention
â”œâ”€â”€ ðŸ”¥ Reseau_Ultra_Specialized/           # Ultra-specialized architecture
â”œâ”€â”€ âš¡ Reseau_PyTorch_Optimized/           # Optimized PyTorch implementation
â”œâ”€â”€ ðŸ”§ Reseau_TensorFlow_Alternative/      # TensorFlow/Keras alternative
â”œâ”€â”€ ðŸ“Š data_generation/                    # Original MATLAB data and scripts
â”œâ”€â”€ ðŸ“‹ project_map.md                      # Complete project overview
â””â”€â”€ ðŸ“– README.md                           # This file
```

### Standardized Network Structure

Each neural network follows the same organization:

```
Reseau_XYZ/
â”œâ”€â”€ run.py              # Autonomous main script
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml     # Complete configuration
â”œâ”€â”€ models/             # Trained models (.pth, .h5, .pkl)
â”œâ”€â”€ plots/              # Visualizations and analysis
â”œâ”€â”€ results/            # Metrics and reports (JSON, CSV)
â”œâ”€â”€ docs/               # Specialized documentation
â””â”€â”€ README.md           # Usage guide
```

## ðŸš€ Quick Start

### 1ï¸âƒ£ Setup Environment
```bash
# Install common dependencies
pip install torch pandas numpy matplotlib seaborn scikit-learn pyyaml scipy joblib

# For TensorFlow (optional)
pip install tensorflow
```

### 2ï¸âƒ£ Choose Your Neural Network
```bash
# For production use (recommended)
cd Reseau_Advanced_Regressor
python run.py

# For maximum performance
cd Reseau_Ultra_Specialized
python run.py

# For gap-only prediction
cd Reseau_Gap_Prediction_CNN
python run.py --mode train

# For robustness testing
cd Reseau_Noise_Robustness
python run.py
```

### 3ï¸âƒ£ View Results
Each network generates:
- **Models**: Trained neural networks
- **Plots**: Performance visualizations
- **Results**: Detailed metrics and reports

## ðŸŽ¯ Neural Networks Available

### 1. ðŸ”¬ Reseau_Gap_Prediction_CNN
**Specialized gap parameter prediction**
- **Architecture:** CNN 1D with residual blocks
- **Performance:** RÂ² > 0.99 on gap
- **Use case:** Gap-only prediction with high accuracy

### 2. ðŸ”Š Reseau_Noise_Robustness
**Noise robustness testing**
- **Architecture:** Simplified network for testing
- **Performance:** RÂ² > 0.8 even with 5% noise
- **Use case:** Evaluate model robustness under noise

### 3. ðŸ§ª Reseau_Overfitting_Test
**Overfitting validation**
- **Architecture:** Simple without regularization
- **Performance:** RÂ² > 0.99 and Loss < 0.001
- **Use case:** Validate model learning capacity

### 4. ðŸ§  Reseau_Advanced_Regressor
**Advanced regressor with attention** â­ **Recommended**
- **Architecture:** Multi-head with attention mechanism
- **Performance:** RÂ² > 0.8 gap, RÂ² > 0.95 L_ecran
- **Use case:** Production deployment

### 5. ðŸ”¥ Reseau_Ultra_Specialized
**Maximum performance ensemble**
- **Architecture:** Ensemble of 3 ultra-deep models
- **Performance:** RÂ² > 0.85 gap, RÂ² > 0.98 L_ecran
- **Use case:** Research and maximum accuracy

### 6. âš¡ Reseau_PyTorch_Optimized
**Optimized PyTorch implementation**
- **Architecture:** ResNet 1D with advanced optimizations
- **Performance:** RÂ² > 0.95 global
- **Use case:** PyTorch development and research

### 7. ðŸ”§ Reseau_TensorFlow_Alternative
**TensorFlow/Keras alternative**
- **Architecture:** Dense 512â†’256â†’128â†’64â†’2
- **Performance:** RÂ² > 0.85 global
- **Use case:** TensorFlow/Keras development

## ðŸ“Š Dataset Information

### Common Data Source
- **Dataset:** `data_generation/all_banque_new_24_01_25_NEW_full.mat`
- **Variables:**
  - `L_ecran_subs_vect`: Screen distances (6.0 to 14.0 Âµm)
  - `gap_sphere_vect`: Gap values (0.025 to 1.5 Âµm)
  - `I_subs`: Scattered intensities [33Ã—30Ã—1000]
  - `I_subs_inc`: Incident intensities [33Ã—30Ã—1000]

### Training Data
- **990 samples** (33 L_ecran Ã— 30 gap combinations)
- **600-1000 radial points** per profile (network-dependent)
- **Input:** Intensity ratios `I_subs/I_subs_inc`
- **Output:** Physical parameters [L_ecran, gap]

## ðŸ“ˆ Performance Comparison

| Network | Gap RÂ² | L_ecran RÂ² | Specialty | Training Time |
|---------|--------|------------|-----------|---------------|
| Gap Prediction CNN | >0.99 | - | Gap only | ~5 min |
| Noise Robustness | >0.8* | >0.95* | Noise testing | ~15 min |
| Overfitting Test | >0.99 | >0.99 | Validation | ~3 min |
| **Advanced Regressor** â­ | >0.8 | >0.95 | **Production** | ~8 min |
| Ultra Specialized | >0.85 | >0.98 | Max performance | ~20 min |
| PyTorch Optimized | >0.8 | >0.95 | PyTorch dev | ~10 min |
| TensorFlow Alternative | >0.8 | >0.95 | TensorFlow dev | ~15 min |

*\* Performance under 5% noise*

## ðŸ”¬ Physical Background

### Intensity Calculation
The neural networks train on the ratio `I_subs/I_subs_inc`, which represents the normalized scattered intensity:

```
Ratio = |E_total|Â² / |E_incident|Â²
      = |E_incident + E_scattered|Â² / |E_incident|Â²
      = |1 + E_scattered/E_incident|Â²
```

### Advantages of 1D Profile Approach
1. **Better Performance:** More efficient than 2D CNN approaches
2. **Physical Relevance:** Directly related to ring structure
3. **Interpretability:** Clear relationship between input and output
4. **Computational Efficiency:** Faster training and inference

## ðŸ“š Documentation

- **[Project Map](project_map.md):** Complete overview of all networks
- **Individual READMEs:** Each network has detailed documentation
- **Configuration Files:** YAML configs for each network
- **Results:** Automated metrics and visualizations

## ðŸŽ¯ Selection Guide

### For Production Use
- **Recommended:** `Reseau_Advanced_Regressor` or `Reseau_Ultra_Specialized`
- **Reason:** Systematic problem solving, high performance

### For Research
- **Gap only:** `Reseau_Gap_Prediction_CNN`
- **Robustness:** `Reseau_Noise_Robustness`
- **Diagnostics:** `Reseau_Overfitting_Test`

### For Development
- **PyTorch:** `Reseau_PyTorch_Optimized`
- **TensorFlow:** `Reseau_TensorFlow_Alternative`

## ðŸ”§ Modular Benefits

### Independent Units
- âœ… Each network is self-contained
- âœ… Can be zipped and deployed separately
- âœ… Easy to compare different approaches
- âœ… Simplified maintenance and updates

### Standardized Structure
- âœ… Consistent organization across networks
- âœ… Autonomous `run.py` scripts
- âœ… Complete configuration files
- âœ… Automated result generation

## ðŸŽ‰ Project Achievements

This modular neural network suite successfully provides:
- âœ… **7 specialized networks** for different use cases
- âœ… **Standardized structure** for easy deployment
- âœ… **High performance** (RÂ² > 0.8 consistently achieved)
- âœ… **Complete documentation** and configuration
- âœ… **Production-ready** solutions for holographic analysis
- âœ… **Modular architecture** for easy extension and maintenance

**Each network is ready for independent deployment in holographic parameter inversion!** ðŸš€
