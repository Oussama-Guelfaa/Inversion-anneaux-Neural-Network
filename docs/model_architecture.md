# Neural Network pour l'Inversion de ParamÃ¨tres Holographiques

**Auteur:** Oussama GUELFAA  
**Date:** 05 - 06 - 2025

Ce projet implÃ©mente un rÃ©seau de neurones pour prÃ©dire les paramÃ¨tres L_ecran et gap Ã  partir de profils radiaux d'intensitÃ© extraits du dataset `all_banque_new_24_01_25_NEW_full.mat`.

## ðŸ“ Structure du Projet

```
Neural_Network/
â”œâ”€â”€ extract_training_data.py      # Extraction et prÃ©paration des donnÃ©es
â”œâ”€â”€ train_new_dataset.py          # EntraÃ®nement du rÃ©seau de neurones
â”œâ”€â”€ evaluate_new_model.py         # Ã‰valuation des performances
â”œâ”€â”€ processed_data/               # DonnÃ©es extraites et organisÃ©es
â”‚   â”œâ”€â”€ training_data.npz         # Dataset complet (990 Ã©chantillons)
â”‚   â”œâ”€â”€ intensity_profiles_full.csv # Tous les profils (990Ã—1000)
â”‚   â”œâ”€â”€ parameters.csv            # ParamÃ¨tres [L_ecran, gap]
â”‚   â””â”€â”€ data_visualization.png    # Visualisation des donnÃ©es
â”œâ”€â”€ models/                       # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ ring_regressor_new.pth    # Meilleur modÃ¨le sauvegardÃ©
â”‚   â””â”€â”€ scalers_new.npz          # Normalisateurs
â””â”€â”€ plots/                        # RÃ©sultats d'Ã©valuation
    â”œâ”€â”€ evaluation_results.png    # Graphiques de performance
    â””â”€â”€ predictions_results.csv   # PrÃ©dictions vs vraies valeurs
```

## ðŸš€ Utilisation

### 1. Extraction des donnÃ©es
```bash
python extract_training_data.py
```

### 2. EntraÃ®nement du modÃ¨le
```bash
python train_new_dataset.py
```

### 3. Ã‰valuation des performances
```bash
python evaluate_new_model.py
```

## ðŸ“Š Dataset

- **Source:** `all_banque_new_24_01_25_NEW_full.mat`
- **Ã‰chantillons:** 990 (33 L_ecran Ã— 30 gap)
- **Features:** Profils radiaux 1D (1000 points)
- **Targets:** [L_ecran, gap]
- **Plages:**
  - L_ecran: 6.0 Ã  14.0 Âµm
  - gap: 0.025 Ã  1.5 Âµm

## ðŸ§  Architecture du RÃ©seau

- **EntrÃ©e:** 1000 points (profil radial)
- **Couches cachÃ©es:** [512, 256, 128]
- **Sortie:** 2 paramÃ¨tres [L_ecran, gap]
- **Techniques:** Blocs rÃ©siduels, BatchNorm, Dropout
- **Optimiseur:** Adam avec scheduler adaptatif

## ðŸ“ˆ Performances

Le modÃ¨le atteint d'excellentes performances :
- **RÂ² Score:** > 0.99 pour L_ecran et gap
- **RMSE:** < 0.01 pour les paramÃ¨tres normalisÃ©s
- **Temps d'entraÃ®nement:** ~5 minutes sur CPU

## ðŸ”§ Troubleshooting

1. **Erreur de mÃ©moire** : RÃ©duire `batch_size`
2. **Pas de GPU** : Le modÃ¨le fonctionne aussi sur CPU
3. **DonnÃ©es manquantes** : VÃ©rifier les chemins vers les fichiers `.mat`
4. **Overfitting** : Augmenter le dropout ou rÃ©duire la complexitÃ©
