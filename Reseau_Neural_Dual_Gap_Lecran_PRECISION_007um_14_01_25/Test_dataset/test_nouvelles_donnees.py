#!/usr/bin/env python3
"""
Test sur Nouvelles Donn√©es - Mod√®le Dual Gap + L_ecran

Auteur: Oussama GUELFAA
Date: 18 - 06 - 2025

Script exemple pour tester le mod√®le sur de nouvelles donn√©es.
Montre le workflow complet √©tape par √©tape.
"""

import sys
import os
sys.path.append('../src')

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import pandas as pd

from dual_parameter_model import DualParameterPredictor
from data_loader import DualDataLoader

def predict_sample(model, profile, data_loader):
    """
    Pr√©dit les param√®tres pour un √©chantillon.
    
    Args:
        model: Mod√®le entra√Æn√©
        profile (np.array): Profil d'intensit√© [600]
        data_loader: DataLoader pour normalisation
    
    Returns:
        tuple: (gap_pred, L_ecran_pred)
    """
    print(f"üîß Traitement d'un √©chantillon:")
    print(f"   Input shape: {profile.shape}")
    print(f"   Input range: [{profile.min():.3f}, {profile.max():.3f}]")
    
    # √âTAPE 1: Normaliser l'entr√©e
    profile_scaled = data_loader.input_scaler.transform(profile.reshape(1, -1))
    print(f"   Apr√®s normalisation: [{profile_scaled.min():.3f}, {profile_scaled.max():.3f}]")
    
    # √âTAPE 2: Pr√©diction
    with torch.no_grad():
        input_tensor = torch.FloatTensor(profile_scaled)
        prediction_scaled = model(input_tensor).numpy()
    print(f"   Pr√©diction normalis√©e: {prediction_scaled[0]}")
    
    # √âTAPE 3: D√©normaliser la sortie
    prediction_original = data_loader.inverse_transform_predictions(prediction_scaled)
    print(f"   Pr√©diction finale: {prediction_original[0]}")
    
    gap_pred = prediction_original[0, 0]
    L_ecran_pred = prediction_original[0, 1]
    
    return gap_pred, L_ecran_pred

def load_model_for_new_data():
    """
    Charge le mod√®le et le data_loader pour nouvelles donn√©es.
    
    Returns:
        tuple: (model, data_loader)
    """
    print("üîÑ Chargement du mod√®le pour nouvelles donn√©es...")
    
    # Charger le mod√®le
    model_path = "../models/dual_parameter_model.pth"
    if not Path(model_path).exists():
        raise FileNotFoundError(f"Mod√®le non trouv√©: {model_path}")
    
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    
    model = DualParameterPredictor(input_size=600)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"‚úÖ Mod√®le charg√©: {sum(p.numel() for p in model.parameters()):,} param√®tres")
    
    # Charger le data_loader avec les scalers d'entra√Ænement
    data_loader = DualDataLoader()
    
    # Configuration minimale pour charger les scalers
    config = {
        'data_processing': {
            'augmentation': {'enable': False},
            'data_splits': {'train': 0.8, 'validation': 0.1, 'test': 0.1},
            'normalization': {'target_scaling': {'separate_scaling': True}}
        },
        'training': {'batch_size': 32}
    }
    
    # Charger le pipeline pour initialiser les scalers
    _ = data_loader.get_complete_pipeline(config)
    
    print(f"‚úÖ DataLoader configur√© avec scalers d'entra√Ænement")
    
    return model, data_loader

def create_sample_data():
    """
    Cr√©e des donn√©es d'exemple pour d√©monstration.
    
    Returns:
        tuple: (X_sample, y_sample)
    """
    print("üé≤ Cr√©ation de donn√©es d'exemple...")
    
    # Utiliser quelques √©chantillons du dataset existant comme exemple
    data_loader = DualDataLoader()
    config = {
        'data_processing': {
            'augmentation': {'enable': False},
            'data_splits': {'train': 0.8, 'validation': 0.1, 'test': 0.1},
            'normalization': {'target_scaling': {'separate_scaling': True}}
        },
        'training': {'batch_size': 32}
    }
    
    pipeline_result = data_loader.get_complete_pipeline(config)
    X_test = pipeline_result['raw_data'][2]
    y_test = pipeline_result['raw_data'][5]
    
    # Prendre 5 √©chantillons al√©atoirement
    indices = np.random.choice(len(X_test), 5, replace=False)
    X_sample = X_test[indices]
    y_sample = y_test[indices]
    
    print(f"‚úÖ Donn√©es d'exemple cr√©√©es: {X_sample.shape}")
    print(f"   √âchantillons s√©lectionn√©s: {indices}")
    
    return X_sample, y_sample

def test_nouvelles_donnees(X_new, y_new=None, model=None, data_loader=None):
    """
    Teste le mod√®le sur de nouvelles donn√©es.
    
    Args:
        X_new: Nouvelles donn√©es [n_samples, 600]
        y_new: Labels optionnels [n_samples, 2]
        model: Mod√®le entra√Æn√© (optionnel)
        data_loader: DataLoader configur√© (optionnel)
    
    Returns:
        dict: R√©sultats des pr√©dictions
    """
    print(f"\nüéØ TEST SUR NOUVELLES DONN√âES")
    print("="*40)
    print(f"üìä Donn√©es d'entr√©e:")
    print(f"   Shape: {X_new.shape}")
    print(f"   Type: {X_new.dtype}")
    print(f"   Range: [{X_new.min():.3f}, {X_new.max():.3f}]")
    
    # Charger le mod√®le si non fourni
    if model is None or data_loader is None:
        model, data_loader = load_model_for_new_data()
    
    # Pr√©dictions
    predictions_gap = []
    predictions_L_ecran = []
    
    print(f"\nüîÑ Calcul des pr√©dictions...")
    for i in range(len(X_new)):
        print(f"\n--- √âchantillon {i+1}/{len(X_new)} ---")
        
        profile = X_new[i]
        gap_pred, L_ecran_pred = predict_sample(model, profile, data_loader)
        
        predictions_gap.append(gap_pred)
        predictions_L_ecran.append(L_ecran_pred)
        
        print(f"   ‚Üí Gap pr√©dit: {gap_pred:.4f} ¬µm")
        print(f"   ‚Üí L_ecran pr√©dit: {L_ecran_pred:.2f} ¬µm")
        
        if y_new is not None:
            gap_true = y_new[i, 0]
            L_ecran_true = y_new[i, 1]
            gap_error = abs(gap_pred - gap_true)
            L_ecran_error = abs(L_ecran_pred - L_ecran_true)
            
            print(f"   ‚úì Gap vrai: {gap_true:.4f} ¬µm (erreur: {gap_error:.4f} ¬µm)")
            print(f"   ‚úì L_ecran vrai: {L_ecran_true:.2f} ¬µm (erreur: {L_ecran_error:.4f} ¬µm)")
    
    # Conversion en arrays
    predictions_gap = np.array(predictions_gap)
    predictions_L_ecran = np.array(predictions_L_ecran)
    
    # R√©sultats
    results = {
        'n_samples': len(X_new),
        'predictions': {
            'gap': predictions_gap.tolist(),
            'L_ecran': predictions_L_ecran.tolist()
        }
    }
    
    # √âvaluation si labels disponibles
    if y_new is not None:
        true_gap = y_new[:, 0]
        true_L_ecran = y_new[:, 1]
        
        # M√©triques
        gap_r2 = r2_score(true_gap, predictions_gap)
        L_ecran_r2 = r2_score(true_L_ecran, predictions_L_ecran)
        combined_r2 = (gap_r2 + L_ecran_r2) / 2
        
        gap_mae = mean_absolute_error(true_gap, predictions_gap)
        L_ecran_mae = mean_absolute_error(true_L_ecran, predictions_L_ecran)
        
        # Tol√©rance
        gap_within_tolerance = np.sum(np.abs(predictions_gap - true_gap) <= 0.01)
        L_ecran_within_tolerance = np.sum(np.abs(predictions_L_ecran - true_L_ecran) <= 0.1)
        
        gap_accuracy = gap_within_tolerance / len(X_new)
        L_ecran_accuracy = L_ecran_within_tolerance / len(X_new)
        
        # Ajouter aux r√©sultats
        results['evaluation'] = {
            'gap_r2': gap_r2,
            'L_ecran_r2': L_ecran_r2,
            'combined_r2': combined_r2,
            'gap_mae': gap_mae,
            'L_ecran_mae': L_ecran_mae,
            'gap_accuracy': gap_accuracy,
            'L_ecran_accuracy': L_ecran_accuracy
        }
        
        print(f"\nüìä √âVALUATION:")
        print("="*30)
        print(f"üéØ M√©triques R¬≤:")
        print(f"   Gap R¬≤: {gap_r2:.4f}")
        print(f"   L_ecran R¬≤: {L_ecran_r2:.4f}")
        print(f"   Combined R¬≤: {combined_r2:.4f}")
        
        print(f"\nüìè Erreurs moyennes:")
        print(f"   Gap MAE: {gap_mae:.4f} ¬µm")
        print(f"   L_ecran MAE: {L_ecran_mae:.4f} ¬µm")
        
        print(f"\nüéØ Pr√©cision dans tol√©rance:")
        print(f"   Gap (¬±0.01¬µm): {gap_within_tolerance}/{len(X_new)} ({gap_accuracy:.1%})")
        print(f"   L_ecran (¬±0.1¬µm): {L_ecran_within_tolerance}/{len(X_new)} ({L_ecran_accuracy:.1%})")
    
    return results

def save_results(results, filename="nouvelles_predictions.json"):
    """
    Sauvegarde les r√©sultats.
    
    Args:
        results: R√©sultats des pr√©dictions
        filename: Nom du fichier de sauvegarde
    """
    Path("../results").mkdir(exist_ok=True)
    filepath = f"../results/{filename}"
    
    with open(filepath, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"üíæ R√©sultats sauvegard√©s: {filepath}")

def create_visualization(results, save_path="../plots/nouvelles_predictions.png"):
    """
    Cr√©e une visualisation des r√©sultats.
    
    Args:
        results: R√©sultats des pr√©dictions
        save_path: Chemin de sauvegarde
    """
    if 'evaluation' not in results:
        print("‚ö†Ô∏è Pas d'√©valuation disponible, visualisation limit√©e")
        return
    
    Path("../plots").mkdir(exist_ok=True)
    
    # Donn√©es
    gap_pred = np.array(results['predictions']['gap'])
    L_ecran_pred = np.array(results['predictions']['L_ecran'])
    
    # Figure simple
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Gap
    ax1.bar(range(len(gap_pred)), gap_pred, alpha=0.7)
    ax1.set_xlabel('√âchantillon')
    ax1.set_ylabel('Gap Pr√©dit (¬µm)')
    ax1.set_title(f'Pr√©dictions Gap - {results["n_samples"]} √©chantillons')
    ax1.grid(True, alpha=0.3)
    
    # L_ecran
    ax2.bar(range(len(L_ecran_pred)), L_ecran_pred, alpha=0.7, color='orange')
    ax2.set_xlabel('√âchantillon')
    ax2.set_ylabel('L_ecran Pr√©dit (¬µm)')
    ax2.set_title(f'Pr√©dictions L_ecran - {results["n_samples"]} √©chantillons')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìà Visualisation sauvegard√©e: {save_path}")

def main():
    """
    Fonction principale de d√©monstration.
    """
    print("üéØ TEST SUR NOUVELLES DONN√âES - MOD√àLE DUAL GAP + L_ECRAN")
    print("="*60)
    print(f"Auteur: Oussama GUELFAA")
    print(f"Date: 18-06-2025")
    print("="*60)
    
    try:
        # 1. Charger le mod√®le
        model, data_loader = load_model_for_new_data()
        
        # 2. Cr√©er des donn√©es d'exemple (remplacez par vos vraies donn√©es)
        print(f"\nüìù √âTAPE 1: Pr√©paration des donn√©es")
        X_sample, y_sample = create_sample_data()
        
        print(f"\nüìù √âTAPE 2: Test sur les nouvelles donn√©es")
        # 3. Tester sur les nouvelles donn√©es
        results = test_nouvelles_donnees(X_sample, y_sample, model, data_loader)
        
        print(f"\nüìù √âTAPE 3: Sauvegarde et visualisation")
        # 4. Sauvegarder les r√©sultats
        save_results(results, "exemple_nouvelles_donnees.json")
        
        # 5. Cr√©er une visualisation
        create_visualization(results, "../plots/exemple_nouvelles_donnees.png")
        
        print(f"\nüéâ Test termin√© avec succ√®s !")
        print(f"\nüìã POUR VOS PROPRES DONN√âES:")
        print(f"   1. Remplacez create_sample_data() par le chargement de vos donn√©es")
        print(f"   2. Format requis: X_new [n_samples, 600], y_new [n_samples, 2] (optionnel)")
        print(f"   3. Appelez: test_nouvelles_donnees(X_new, y_new)")
        print(f"   4. Les r√©sultats seront automatiquement sauvegard√©s")
        
    except Exception as e:
        print(f"‚ùå Erreur dans le test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
