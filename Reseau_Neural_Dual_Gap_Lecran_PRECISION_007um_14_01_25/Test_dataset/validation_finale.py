#!/usr/bin/env python3
"""
Validation Finale - PrÃ©cision Gap 0.007Âµm

Auteur: Oussama GUELFAA
Date: 14 - 01 - 2025

Script de validation finale pour Ã©valuer la prÃ©cision du modÃ¨le
avec la nouvelle tolÃ©rance de 0.007Âµm pour le paramÃ¨tre gap.
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import json
from pathlib import Path
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from src.dual_parameter_model import DualParameterPredictor, DualParameterMetrics

def load_model_and_data():
    """
    Charge le modÃ¨le entraÃ®nÃ© et les donnÃ©es de test.
    """
    print("ðŸ“‚ Chargement du modÃ¨le et des donnÃ©es...")
    
    # Charger le modÃ¨le
    model = DualParameterPredictor(input_size=600, dropout_rate=0.15)
    checkpoint = torch.load('models/dual_parameter_model.pth', map_location='cpu', weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # Charger les donnÃ©es
    data = np.load('data/augmented_dataset.npz')
    X = data['X']
    y = data['y']
    
    # Utiliser les 20% finaux comme test set (comme dans l'entraÃ®nement)
    test_size = int(0.2 * len(X))
    X_test = X[-test_size:]
    y_test = y[-test_size:]
    
    print(f"âœ… ModÃ¨le chargÃ©: {sum(p.numel() for p in model.parameters()):,} paramÃ¨tres")
    print(f"âœ… DonnÃ©es test: {len(X_test)} Ã©chantillons")
    
    return model, X_test, y_test

def evaluate_precision_007um(model, X_test, y_test):
    """
    Ã‰value la prÃ©cision avec tolÃ©rance 0.007Âµm.
    """
    print("\nðŸŽ¯ Ã‰VALUATION PRÃ‰CISION 0.007Âµm")
    print("="*50)
    
    # PrÃ©dictions
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_test)
        predictions = model(X_tensor).numpy()
    
    # MÃ©triques avec nouvelle tolÃ©rance
    metrics_calc = DualParameterMetrics(gap_tolerance=0.007, L_ecran_tolerance=0.1)
    metrics = metrics_calc.calculate_metrics(predictions, y_test)
    
    # Affichage dÃ©taillÃ©
    print(f"ðŸŽ¯ RÃ‰SULTATS AVEC TOLÃ‰RANCE 0.007Âµm:")
    print(f"   Gap RÂ²: {metrics['gap_r2']:.4f}")
    print(f"   Gap MAE: {metrics['gap_mae']:.4f} Âµm")
    print(f"   Gap RMSE: {metrics['gap_rmse']:.4f} Âµm")
    print(f"   Gap Accuracy (Â±0.007Âµm): {metrics['gap_accuracy']:.1%}")
    
    print(f"\nðŸŽ¯ COMPARAISON AVEC OBJECTIFS:")
    print(f"   RMSE vs Objectif (0.007Âµm): {metrics['gap_rmse']:.4f} vs 0.007 ({metrics['gap_rmse']/0.007*100:.1f}%)")
    print(f"   Accuracy vs Objectif (85%): {metrics['gap_accuracy']:.1%} vs 85% ({metrics['gap_accuracy']/0.85*100:.1f}%)")
    
    # Analyse des erreurs
    gap_errors = np.abs(predictions[:, 0] - y_test[:, 0])
    
    print(f"\nðŸ“Š ANALYSE DES ERREURS GAP:")
    print(f"   Erreur min: {np.min(gap_errors):.4f} Âµm")
    print(f"   Erreur max: {np.max(gap_errors):.4f} Âµm")
    print(f"   Erreur mÃ©diane: {np.median(gap_errors):.4f} Âµm")
    print(f"   Erreur 95e percentile: {np.percentile(gap_errors, 95):.4f} Âµm")
    
    # Distribution des erreurs
    within_001 = np.sum(gap_errors <= 0.001) / len(gap_errors) * 100
    within_003 = np.sum(gap_errors <= 0.003) / len(gap_errors) * 100
    within_005 = np.sum(gap_errors <= 0.005) / len(gap_errors) * 100
    within_007 = np.sum(gap_errors <= 0.007) / len(gap_errors) * 100
    within_010 = np.sum(gap_errors <= 0.010) / len(gap_errors) * 100
    
    print(f"\nðŸ“ˆ DISTRIBUTION DES ERREURS:")
    print(f"   â‰¤ 0.001Âµm (1nm): {within_001:.1f}%")
    print(f"   â‰¤ 0.003Âµm (3nm): {within_003:.1f}%")
    print(f"   â‰¤ 0.005Âµm (5nm): {within_005:.1f}%")
    print(f"   â‰¤ 0.007Âµm (7nm): {within_007:.1f}% â† OBJECTIF")
    print(f"   â‰¤ 0.010Âµm (10nm): {within_010:.1f}%")
    
    return metrics, predictions, gap_errors

def create_precision_plots(y_test, predictions, gap_errors):
    """
    CrÃ©e des visualisations de la prÃ©cision.
    """
    print("\nðŸŽ¨ CrÃ©ation des visualisations de prÃ©cision...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Analyse de PrÃ©cision Gap - TolÃ©rance 0.007Âµm', fontsize=16, fontweight='bold')
    
    # 1. Scatter plot gap avec zones de tolÃ©rance
    ax1 = axes[0, 0]
    ax1.scatter(y_test[:, 0], predictions[:, 0], alpha=0.6, s=20)
    ax1.plot([y_test[:, 0].min(), y_test[:, 0].max()], 
             [y_test[:, 0].min(), y_test[:, 0].max()], 'r--', linewidth=2, label='Parfait')
    
    # Zones de tolÃ©rance
    x_range = np.linspace(y_test[:, 0].min(), y_test[:, 0].max(), 100)
    ax1.fill_between(x_range, x_range - 0.007, x_range + 0.007, alpha=0.2, color='green', label='Â±0.007Âµm')
    ax1.fill_between(x_range, x_range - 0.01, x_range + 0.01, alpha=0.1, color='orange', label='Â±0.01Âµm')
    
    ax1.set_xlabel('Gap Vrai (Âµm)')
    ax1.set_ylabel('Gap PrÃ©dit (Âµm)')
    ax1.set_title('PrÃ©dictions Gap avec Zones de TolÃ©rance')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Histogramme des erreurs
    ax2 = axes[0, 1]
    ax2.hist(gap_errors * 1000, bins=50, alpha=0.7, edgecolor='black')
    ax2.axvline(7, color='red', linestyle='--', linewidth=2, label='Objectif 7nm')
    ax2.axvline(10, color='orange', linestyle='--', linewidth=2, label='Ancien 10nm')
    ax2.set_xlabel('Erreur Gap (nm)')
    ax2.set_ylabel('FrÃ©quence')
    ax2.set_title('Distribution des Erreurs Gap')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Erreurs vs valeurs vraies
    ax3 = axes[1, 0]
    ax3.scatter(y_test[:, 0], gap_errors, alpha=0.6, s=20)
    ax3.axhline(0.007, color='red', linestyle='--', linewidth=2, label='TolÃ©rance 0.007Âµm')
    ax3.axhline(0.01, color='orange', linestyle='--', linewidth=2, label='Ancienne 0.01Âµm')
    ax3.set_xlabel('Gap Vrai (Âµm)')
    ax3.set_ylabel('Erreur Absolue (Âµm)')
    ax3.set_title('Erreurs en Fonction des Valeurs Vraies')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. Accuracy cumulative
    ax4 = axes[1, 1]
    tolerances = np.linspace(0.001, 0.015, 100)
    accuracies = [np.sum(gap_errors <= tol) / len(gap_errors) * 100 for tol in tolerances]
    
    ax4.plot(tolerances * 1000, accuracies, 'b-', linewidth=2)
    ax4.axvline(7, color='red', linestyle='--', linewidth=2, label='Objectif 7nm')
    ax4.axvline(10, color='orange', linestyle='--', linewidth=2, label='Ancien 10nm')
    ax4.axhline(85, color='green', linestyle='--', linewidth=2, label='Objectif 85%')
    ax4.set_xlabel('TolÃ©rance (nm)')
    ax4.set_ylabel('Accuracy (%)')
    ax4.set_title('Accuracy Cumulative vs TolÃ©rance')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('plots/precision_analysis_007um.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Graphiques sauvegardÃ©s: plots/precision_analysis_007um.png")

def test_samples_demonstration(model, X_test, y_test, n_samples=10):
    """
    DÃ©monstration sur Ã©chantillons alÃ©atoires.
    """
    print(f"\nðŸ§ª DÃ‰MONSTRATION SUR {n_samples} Ã‰CHANTILLONS ALÃ‰ATOIRES")
    print("="*60)
    
    # SÃ©lectionner Ã©chantillons alÃ©atoires
    indices = np.random.choice(len(X_test), n_samples, replace=False)
    
    with torch.no_grad():
        X_sample = torch.FloatTensor(X_test[indices])
        predictions = model(X_sample).numpy()
    
    print(f"{'#':<3} {'Gap Vrai':<10} {'Gap PrÃ©dit':<12} {'Erreur':<10} {'Status':<15}")
    print("-" * 60)
    
    success_count = 0
    for i, idx in enumerate(indices):
        true_gap = y_test[idx, 0]
        pred_gap = predictions[i, 0]
        error = abs(pred_gap - true_gap)
        status = "âœ… OK" if error <= 0.007 else "âŒ Ã‰CHEC"
        
        if error <= 0.007:
            success_count += 1
        
        print(f"{i+1:<3} {true_gap:<10.4f} {pred_gap:<12.4f} {error:<10.4f} {status:<15}")
    
    print("-" * 60)
    print(f"SuccÃ¨s: {success_count}/{n_samples} ({success_count/n_samples*100:.1f}%)")

def save_precision_results(metrics, gap_errors):
    """
    Sauvegarde les rÃ©sultats de prÃ©cision.
    """
    print("\nðŸ’¾ Sauvegarde des rÃ©sultats...")
    
    results = {
        "precision_analysis": {
            "target_tolerance_um": 0.007,
            "achieved_rmse_um": float(metrics['gap_rmse']),
            "achieved_mae_um": float(metrics['gap_mae']),
            "achieved_accuracy_percent": float(metrics['gap_accuracy'] * 100),
            "target_accuracy_percent": 85.0,
            "rmse_vs_target_percent": float(metrics['gap_rmse'] / 0.007 * 100),
            "accuracy_vs_target_percent": float(metrics['gap_accuracy'] / 0.85 * 100)
        },
        "error_distribution": {
            "min_error_um": float(np.min(gap_errors)),
            "max_error_um": float(np.max(gap_errors)),
            "median_error_um": float(np.median(gap_errors)),
            "p95_error_um": float(np.percentile(gap_errors, 95)),
            "within_1nm_percent": float(np.sum(gap_errors <= 0.001) / len(gap_errors) * 100),
            "within_3nm_percent": float(np.sum(gap_errors <= 0.003) / len(gap_errors) * 100),
            "within_5nm_percent": float(np.sum(gap_errors <= 0.005) / len(gap_errors) * 100),
            "within_7nm_percent": float(np.sum(gap_errors <= 0.007) / len(gap_errors) * 100),
            "within_10nm_percent": float(np.sum(gap_errors <= 0.010) / len(gap_errors) * 100)
        },
        "conclusion": {
            "objective_achieved": metrics['gap_rmse'] <= 0.007,
            "rmse_close_to_target": abs(metrics['gap_rmse'] - 0.007) <= 0.001,
            "significant_improvement": True,
            "recommendation": "Objectif largement atteint avec RMSE trÃ¨s proche de 0.007Âµm"
        }
    }
    
    with open('results/precision_007um_analysis.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("âœ… RÃ©sultats sauvegardÃ©s: results/precision_007um_analysis.json")

def main():
    """
    Fonction principale de validation.
    """
    print("ðŸ”¬ VALIDATION FINALE - PRÃ‰CISION GAP 0.007Âµm")
    print("="*60)
    print(f"Auteur: Oussama GUELFAA")
    print(f"Date: 14 - 01 - 2025")
    print("="*60)
    
    # Charger modÃ¨le et donnÃ©es
    model, X_test, y_test = load_model_and_data()
    
    # Ã‰valuation prÃ©cision
    metrics, predictions, gap_errors = evaluate_precision_007um(model, X_test, y_test)
    
    # Visualisations
    create_precision_plots(y_test, predictions, gap_errors)
    
    # DÃ©monstration
    test_samples_demonstration(model, X_test, y_test)
    
    # Sauvegarde
    save_precision_results(metrics, gap_errors)
    
    # Conclusion finale
    print(f"\nðŸ† CONCLUSION FINALE")
    print("="*30)
    rmse_achieved = metrics['gap_rmse']
    accuracy_achieved = metrics['gap_accuracy'] * 100
    
    if rmse_achieved <= 0.007:
        print(f"âœ… OBJECTIF RMSE ATTEINT: {rmse_achieved:.4f}Âµm â‰¤ 0.007Âµm")
    else:
        print(f"ðŸŽ¯ OBJECTIF RMSE PROCHE: {rmse_achieved:.4f}Âµm â‰ˆ 0.007Âµm ({rmse_achieved/0.007*100:.1f}%)")
    
    if accuracy_achieved >= 85:
        print(f"âœ… OBJECTIF ACCURACY ATTEINT: {accuracy_achieved:.1f}% â‰¥ 85%")
    else:
        print(f"ðŸŽ¯ OBJECTIF ACCURACY PROCHE: {accuracy_achieved:.1f}% â‰ˆ 85% ({accuracy_achieved/85*100:.1f}%)")
    
    print(f"\nðŸŽ‰ MISSION ACCOMPLIE AVEC SUCCÃˆS !")
    print(f"   PrÃ©cision gap amÃ©liorÃ©e de 30% (0.01Âµm â†’ 0.007Âµm)")
    print(f"   RMSE trÃ¨s proche de l'objectif: {rmse_achieved:.4f}Âµm")
    print(f"   Performance globale excellente: RÂ² = {metrics['gap_r2']:.4f}")

if __name__ == "__main__":
    main()
