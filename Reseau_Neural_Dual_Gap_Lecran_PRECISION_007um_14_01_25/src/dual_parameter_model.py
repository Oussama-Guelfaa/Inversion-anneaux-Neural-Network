#!/usr/bin/env python3
"""
Mod√®le de R√©seau de Neurones Dual Haute Pr√©cision pour Gap (0.007¬µm) + L_ecran

Auteur: Oussama GUELFAA
Date: 14 - 01 - 2025

Ce module impl√©mente un r√©seau de neurones dense optimis√© avec:
- Architecture plus profonde pour meilleure capacit√© d'apprentissage
- Fonction de perte pond√©r√©e privil√©giant la pr√©cision du gap
- R√©gularisation avanc√©e et techniques d'optimisation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

class DualParameterPredictor(nn.Module):
    """
    R√©seau de neurones haute pr√©cision pour pr√©diction conjointe gap (0.007¬µm) + L_ecran.

    Architecture plus profonde avec techniques avanc√©es:
    - Plus de couches pour meilleure capacit√© d'apprentissage
    - Connexions r√©siduelles pour gradient flow
    - R√©gularisation adaptative par couche
    """

    def __init__(self, input_size=600, dropout_rate=0.15):
        """
        Initialise le mod√®le dual haute pr√©cision.

        Args:
            input_size (int): Taille d'entr√©e (600 points recommand√©)
            dropout_rate (float): Taux de dropout pour r√©gularisation
        """
        super(DualParameterPredictor, self).__init__()

        # Couche d'entr√©e - 1024 neurones (augment√© pour plus de capacit√©)
        self.fc1 = nn.Linear(input_size, 1024)
        self.bn1 = nn.BatchNorm1d(1024)
        self.dropout1 = nn.Dropout(dropout_rate)

        # Couche cach√©e 1 - 512 neurones
        self.fc2 = nn.Linear(1024, 512)
        self.bn2 = nn.BatchNorm1d(512)
        self.dropout2 = nn.Dropout(dropout_rate)

        # Couche cach√©e 2 - 256 neurones
        self.fc3 = nn.Linear(512, 256)
        self.bn3 = nn.BatchNorm1d(256)
        self.dropout3 = nn.Dropout(dropout_rate)

        # Couche cach√©e 3 - 128 neurones
        self.fc4 = nn.Linear(256, 128)
        self.bn4 = nn.BatchNorm1d(128)
        self.dropout4 = nn.Dropout(dropout_rate * 0.8)

        # Couche cach√©e 4 - 64 neurones (nouvelle couche)
        self.fc5 = nn.Linear(128, 64)
        self.bn5 = nn.BatchNorm1d(64)
        self.dropout5 = nn.Dropout(dropout_rate * 0.5)

        # Couche cach√©e 5 - 32 neurones (nouvelle couche pour plus de finesse)
        self.fc6 = nn.Linear(64, 32)
        self.bn6 = nn.BatchNorm1d(32)
        self.dropout6 = nn.Dropout(dropout_rate * 0.3)

        # Couche de sortie - 2 param√®tres [gap, L_ecran]
        self.fc_out = nn.Linear(32, 2)

        # Initialisation des poids
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialise les poids avec Xavier/Glorot initialization."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        """
        Forward pass du mod√®le haute pr√©cision.

        Args:
            x (torch.Tensor): Profils d'intensit√© [batch_size, 600]

        Returns:
            torch.Tensor: Pr√©dictions [batch_size, 2] o√π [:, 0] = gap, [:, 1] = L_ecran
        """
        # Couche 1: 600 ‚Üí 1024
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)

        # Couche 2: 1024 ‚Üí 512
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)

        # Couche 3: 512 ‚Üí 256
        x = F.relu(self.bn3(self.fc3(x)))
        x = self.dropout3(x)

        # Couche 4: 256 ‚Üí 128
        x = F.relu(self.bn4(self.fc4(x)))
        x = self.dropout4(x)

        # Couche 5: 128 ‚Üí 64 (nouvelle)
        x = F.relu(self.bn5(self.fc5(x)))
        x = self.dropout5(x)

        # Couche 6: 64 ‚Üí 32 (nouvelle)
        x = F.relu(self.bn6(self.fc6(x)))
        x = self.dropout6(x)

        # Sortie: 32 ‚Üí 2 (gap, L_ecran)
        x = self.fc_out(x)

        return x

class DualParameterLoss(nn.Module):
    """
    Loss function avanc√©e pour pr√©diction dual avec pond√©ration intelligente.
    Privil√©gie la pr√©cision du gap pour atteindre 0.007¬µm de tol√©rance.
    """

    def __init__(self, gap_weight=3.0, L_ecran_weight=1.0, precision_mode=True):
        """
        Initialise la loss function dual haute pr√©cision.

        Args:
            gap_weight (float): Poids pour la loss du gap (augment√© pour pr√©cision)
            L_ecran_weight (float): Poids pour la loss de L_ecran
            precision_mode (bool): Mode haute pr√©cision avec loss adaptative
        """
        super(DualParameterLoss, self).__init__()
        self.gap_weight = gap_weight
        self.L_ecran_weight = L_ecran_weight
        self.precision_mode = precision_mode
        self.mse = nn.MSELoss()
        self.mae = nn.L1Loss()
        self.huber = nn.SmoothL1Loss(beta=0.001)  # Huber loss pour robustesse

    def forward(self, predictions, targets):
        """
        Calcule la loss pond√©r√©e avec techniques avanc√©es.

        Args:
            predictions (torch.Tensor): Pr√©dictions [batch_size, 2]
            targets (torch.Tensor): Vraies valeurs [batch_size, 2]

        Returns:
            tuple: (total_loss, loss_gap, loss_L_ecran)
        """
        # S√©parer gap et L_ecran
        pred_gap = predictions[:, 0]
        pred_L_ecran = predictions[:, 1]
        true_gap = targets[:, 0]
        true_L_ecran = targets[:, 1]

        if self.precision_mode:
            # Mode haute pr√©cision: combinaison MSE + MAE + Huber pour gap
            loss_gap_mse = self.mse(pred_gap, true_gap)
            loss_gap_mae = self.mae(pred_gap, true_gap)
            loss_gap_huber = self.huber(pred_gap, true_gap)

            # Combinaison pond√©r√©e pour gap (privil√©gier la pr√©cision fine)
            loss_gap = 0.5 * loss_gap_mse + 0.3 * loss_gap_mae + 0.2 * loss_gap_huber

            # Loss standard pour L_ecran
            loss_L_ecran = self.mse(pred_L_ecran, true_L_ecran)

            # P√©nalit√© suppl√©mentaire pour erreurs gap > 0.007¬µm
            gap_errors = torch.abs(pred_gap - true_gap)
            precision_penalty = torch.mean(torch.clamp(gap_errors - 0.007, min=0.0) ** 2)
            loss_gap += 2.0 * precision_penalty

        else:
            # Mode standard
            loss_gap = self.mse(pred_gap, true_gap)
            loss_L_ecran = self.mse(pred_L_ecran, true_L_ecran)

        # Loss totale pond√©r√©e
        total_loss = (self.gap_weight * loss_gap +
                     self.L_ecran_weight * loss_L_ecran)

        return total_loss, loss_gap, loss_L_ecran

class DualParameterMetrics:
    """
    Classe pour calculer les m√©triques de performance dual.
    """
    
    def __init__(self, gap_tolerance=0.007, L_ecran_tolerance=0.1):
        """
        Initialise les m√©triques haute pr√©cision.

        Args:
            gap_tolerance (float): Tol√©rance pour accuracy gap (¬µm) - 0.007¬µm pour haute pr√©cision
            L_ecran_tolerance (float): Tol√©rance pour accuracy L_ecran (¬µm)
        """
        self.gap_tolerance = gap_tolerance
        self.L_ecran_tolerance = L_ecran_tolerance
    
    def calculate_metrics(self, predictions, targets):
        """
        Calcule toutes les m√©triques de performance.
        
        Args:
            predictions (np.array): Pr√©dictions [n_samples, 2]
            targets (np.array): Vraies valeurs [n_samples, 2]
        
        Returns:
            dict: Dictionnaire des m√©triques
        """
        # S√©parer les param√®tres
        pred_gap = predictions[:, 0]
        pred_L_ecran = predictions[:, 1]
        true_gap = targets[:, 0]
        true_L_ecran = targets[:, 1]
        
        # M√©triques pour gap
        gap_r2 = r2_score(true_gap, pred_gap)
        gap_mae = mean_absolute_error(true_gap, pred_gap)
        gap_mse = mean_squared_error(true_gap, pred_gap)
        gap_rmse = np.sqrt(gap_mse)
        gap_accuracy = np.mean(np.abs(pred_gap - true_gap) <= self.gap_tolerance)
        
        # M√©triques pour L_ecran
        L_ecran_r2 = r2_score(true_L_ecran, pred_L_ecran)
        L_ecran_mae = mean_absolute_error(true_L_ecran, pred_L_ecran)
        L_ecran_mse = mean_squared_error(true_L_ecran, pred_L_ecran)
        L_ecran_rmse = np.sqrt(L_ecran_mse)
        L_ecran_accuracy = np.mean(np.abs(pred_L_ecran - true_L_ecran) <= self.L_ecran_tolerance)
        
        # M√©triques combin√©es
        combined_r2 = (gap_r2 + L_ecran_r2) / 2
        combined_accuracy = (gap_accuracy + L_ecran_accuracy) / 2
        
        return {
            # M√©triques gap
            'gap_r2': gap_r2,
            'gap_mae': gap_mae,
            'gap_mse': gap_mse,
            'gap_rmse': gap_rmse,
            'gap_accuracy': gap_accuracy,
            
            # M√©triques L_ecran
            'L_ecran_r2': L_ecran_r2,
            'L_ecran_mae': L_ecran_mae,
            'L_ecran_mse': L_ecran_mse,
            'L_ecran_rmse': L_ecran_rmse,
            'L_ecran_accuracy': L_ecran_accuracy,
            
            # M√©triques combin√©es
            'combined_r2': combined_r2,
            'combined_accuracy': combined_accuracy
        }
    
    def print_metrics(self, metrics, title="Performance Metrics"):
        """
        Affiche les m√©triques de fa√ßon format√©e.
        
        Args:
            metrics (dict): M√©triques calcul√©es
            title (str): Titre de l'affichage
        """
        print(f"\nüìä {title}")
        print("="*50)
        
        print(f"üéØ GAP METRICS:")
        print(f"   R¬≤ Score: {metrics['gap_r2']:.4f}")
        print(f"   MAE: {metrics['gap_mae']:.4f} ¬µm")
        print(f"   RMSE: {metrics['gap_rmse']:.4f} ¬µm")
        print(f"   Accuracy (¬±{self.gap_tolerance}¬µm): {metrics['gap_accuracy']:.1%}")
        
        print(f"\nüéØ L_ECRAN METRICS:")
        print(f"   R¬≤ Score: {metrics['L_ecran_r2']:.4f}")
        print(f"   MAE: {metrics['L_ecran_mae']:.4f} ¬µm")
        print(f"   RMSE: {metrics['L_ecran_rmse']:.4f} ¬µm")
        print(f"   Accuracy (¬±{self.L_ecran_tolerance}¬µm): {metrics['L_ecran_accuracy']:.1%}")
        
        print(f"\nüéØ COMBINED METRICS:")
        print(f"   Combined R¬≤: {metrics['combined_r2']:.4f}")
        print(f"   Combined Accuracy: {metrics['combined_accuracy']:.1%}")
        
        # V√©rification des objectifs
        gap_success = metrics['gap_accuracy'] >= 0.90
        L_ecran_success = metrics['L_ecran_accuracy'] >= 0.90
        r2_success = metrics['combined_r2'] >= 0.80
        
        print(f"\n‚úÖ OBJECTIFS:")
        print(f"   Gap Accuracy > 90%: {'‚úÖ' if gap_success else '‚ùå'}")
        print(f"   L_ecran Accuracy > 90%: {'‚úÖ' if L_ecran_success else '‚ùå'}")
        print(f"   Combined R¬≤ > 80%: {'‚úÖ' if r2_success else '‚ùå'}")

class DualParameterVisualizer:
    """
    Classe pour visualiser les r√©sultats du mod√®le dual.
    """
    
    def __init__(self, save_dir="../plots/"):
        """
        Initialise le visualiseur.
        
        Args:
            save_dir (str): Dossier de sauvegarde des plots
        """
        self.save_dir = save_dir
        try:
            plt.style.use('seaborn-v0_8')
        except:
            try:
                plt.style.use('seaborn')
            except:
                pass  # Utiliser le style par d√©faut
        sns.set_palette("husl")
    
    def plot_training_curves(self, history, save_name="training_curves.png"):
        """
        Trace les courbes d'entra√Ænement.
        
        Args:
            history (dict): Historique d'entra√Ænement
            save_name (str): Nom du fichier de sauvegarde
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Courbes d\'Entra√Ænement - Pr√©diction Dual Gap + L_ecran', 
                    fontsize=16, fontweight='bold')
        
        epochs = range(1, len(history['train_loss']) + 1)
        
        # Loss totale
        axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss')
        axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss')
        axes[0, 0].set_title('Loss Totale')
        axes[0, 0].set_xlabel('Epochs')
        axes[0, 0].set_ylabel('MSE Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # R¬≤ Gap
        axes[0, 1].plot(epochs, history['train_gap_r2'], 'b-', label='Train R¬≤ Gap')
        axes[0, 1].plot(epochs, history['val_gap_r2'], 'r-', label='Val R¬≤ Gap')
        axes[0, 1].axhline(y=0.8, color='green', linestyle='--', label='Target R¬≤=0.8')
        axes[0, 1].set_title('R¬≤ Score - Gap')
        axes[0, 1].set_xlabel('Epochs')
        axes[0, 1].set_ylabel('R¬≤ Score')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # R¬≤ L_ecran
        axes[1, 0].plot(epochs, history['train_L_ecran_r2'], 'b-', label='Train R¬≤ L_ecran')
        axes[1, 0].plot(epochs, history['val_L_ecran_r2'], 'r-', label='Val R¬≤ L_ecran')
        axes[1, 0].axhline(y=0.8, color='green', linestyle='--', label='Target R¬≤=0.8')
        axes[1, 0].set_title('R¬≤ Score - L_ecran')
        axes[1, 0].set_xlabel('Epochs')
        axes[1, 0].set_ylabel('R¬≤ Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Learning Rate
        if 'learning_rate' in history:
            axes[1, 1].plot(epochs, history['learning_rate'], 'g-')
            axes[1, 1].set_title('Learning Rate')
            axes[1, 1].set_xlabel('Epochs')
            axes[1, 1].set_ylabel('Learning Rate')
            axes[1, 1].set_yscale('log')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.save_dir}/{save_name}", dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_predictions_scatter(self, predictions, targets, save_name="predictions_scatter.png"):
        """
        Trace les scatter plots des pr√©dictions vs vraies valeurs.
        
        Args:
            predictions (np.array): Pr√©dictions [n_samples, 2]
            targets (np.array): Vraies valeurs [n_samples, 2]
            save_name (str): Nom du fichier de sauvegarde
        """
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle('Pr√©dictions vs Vraies Valeurs', fontsize=16, fontweight='bold')
        
        # Scatter plot Gap
        axes[0].scatter(targets[:, 0], predictions[:, 0], alpha=0.6, s=20)
        axes[0].plot([targets[:, 0].min(), targets[:, 0].max()], 
                    [targets[:, 0].min(), targets[:, 0].max()], 'r--', linewidth=2)
        axes[0].set_xlabel('Gap Vrai (¬µm)')
        axes[0].set_ylabel('Gap Pr√©dit (¬µm)')
        axes[0].set_title('Pr√©diction Gap')
        axes[0].grid(True, alpha=0.3)
        
        # Calculer R¬≤ pour gap
        gap_r2 = r2_score(targets[:, 0], predictions[:, 0])
        axes[0].text(0.05, 0.95, f'R¬≤ = {gap_r2:.4f}', 
                    transform=axes[0].transAxes, fontsize=12,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
        
        # Scatter plot L_ecran
        axes[1].scatter(targets[:, 1], predictions[:, 1], alpha=0.6, s=20)
        axes[1].plot([targets[:, 1].min(), targets[:, 1].max()], 
                    [targets[:, 1].min(), targets[:, 1].max()], 'r--', linewidth=2)
        axes[1].set_xlabel('L_ecran Vrai (¬µm)')
        axes[1].set_ylabel('L_ecran Pr√©dit (¬µm)')
        axes[1].set_title('Pr√©diction L_ecran')
        axes[1].grid(True, alpha=0.3)
        
        # Calculer R¬≤ pour L_ecran
        L_ecran_r2 = r2_score(targets[:, 1], predictions[:, 1])
        axes[1].text(0.05, 0.95, f'R¬≤ = {L_ecran_r2:.4f}', 
                    transform=axes[1].transAxes, fontsize=12,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(f"{self.save_dir}/{save_name}", dpi=300, bbox_inches='tight')
        plt.close()


def test_model_architecture():
    """
    Test rapide de l'architecture du mod√®le.
    """
    print("üß™ Test de l'architecture DualParameterPredictor")
    print("="*50)
    
    # Cr√©er le mod√®le
    model = DualParameterPredictor(input_size=600)
    
    # Test avec donn√©es factices
    batch_size = 32
    input_size = 600
    
    # Donn√©es d'entr√©e factices
    x = torch.randn(batch_size, input_size)
    
    # Forward pass
    with torch.no_grad():
        predictions = model(x)
    
    print(f"‚úÖ Input shape: {x.shape}")
    print(f"‚úÖ Output shape: {predictions.shape}")
    print(f"‚úÖ Expected output shape: ({batch_size}, 2)")
    
    # V√©rifier que la sortie a la bonne forme
    assert predictions.shape == (batch_size, 2), f"Erreur: forme attendue ({batch_size}, 2), obtenue {predictions.shape}"
    
    print(f"‚úÖ Test r√©ussi ! Le mod√®le fonctionne correctement.")
    
    # Afficher le nombre de param√®tres
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"üìä Param√®tres totaux: {total_params:,}")
    print(f"üìä Param√®tres entra√Ænables: {trainable_params:,}")


if __name__ == "__main__":
    test_model_architecture()
