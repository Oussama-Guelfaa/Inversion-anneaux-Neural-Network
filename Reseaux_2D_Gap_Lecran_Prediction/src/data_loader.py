#!/usr/bin/env python3
"""
Chargeur de DonnÃ©es pour PrÃ©diction Dual Gap + L_ecran

Auteur: Oussama GUELFAA
Date: 06 - 01 - 2025

Ce module gÃ¨re le chargement, l'augmentation et la prÃ©paration
des donnÃ©es pour l'entraÃ®nement du rÃ©seau dual.
"""

import numpy as np
import pandas as pd
from scipy.io import loadmat
from pathlib import Path
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import sys
import os

# Ajouter le chemin vers le module d'augmentation
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
from data_augmentation_2D import DataAugmentation2D

class IntensityDataset(Dataset):
    """
    Dataset PyTorch pour les profils d'intensitÃ© et paramÃ¨tres dual.
    """
    
    def __init__(self, X, y):
        """
        Initialise le dataset.
        
        Args:
            X (np.array): Profils d'intensitÃ© [n_samples, 600]
            y (np.array): ParamÃ¨tres [n_samples, 2] oÃ¹ [:, 0] = gap, [:, 1] = L_ecran
        """
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class DualDataLoader:
    """
    Classe principale pour charger et prÃ©parer les donnÃ©es dual.
    """
    
    def __init__(self, dataset_path="../data_generation/dataset_2D", 
                 cache_path="data/augmented_dataset.npz"):
        """
        Initialise le chargeur de donnÃ©es.
        
        Args:
            dataset_path (str): Chemin vers le dataset 2D
            cache_path (str): Chemin pour cache des donnÃ©es augmentÃ©es
        """
        self.dataset_path = Path(dataset_path)
        self.cache_path = Path(cache_path)
        self.cache_path.parent.mkdir(exist_ok=True)
        
        # Scalers pour normalisation
        self.input_scaler = StandardScaler()
        self.gap_scaler = StandardScaler()
        self.L_ecran_scaler = StandardScaler()
        
        print(f"ğŸ”§ DualDataLoader initialisÃ©")
        print(f"ğŸ“ Dataset: {self.dataset_path}")
        print(f"ğŸ’¾ Cache: {self.cache_path}")
    
    def load_and_augment_data(self, use_cache=True, augmentation_config=None):
        """
        Charge et augmente les donnÃ©es avec cache intelligent.
        
        Args:
            use_cache (bool): Utiliser le cache si disponible
            augmentation_config (dict): Configuration d'augmentation
        
        Returns:
            tuple: (X_augmented, y_augmented) donnÃ©es augmentÃ©es
        """
        print(f"\nğŸ“Š Chargement et augmentation des donnÃ©es...")
        
        # VÃ©rifier le cache
        if use_cache and self.cache_path.exists():
            print(f"ğŸ’¾ Cache trouvÃ©, chargement depuis {self.cache_path}")
            cached_data = np.load(self.cache_path)
            X_augmented = cached_data['X']
            y_augmented = cached_data['y']
            print(f"âœ… DonnÃ©es chargÃ©es depuis cache: X{X_augmented.shape}, y{y_augmented.shape}")
            return X_augmented, y_augmented
        
        # Configuration par dÃ©faut
        if augmentation_config is None:
            augmentation_config = {
                'gap_density': 2,
                'L_ecran_density': 2,
                'method': 'linear',
                'include_original': True
            }

        # Filtrer les paramÃ¨tres non supportÃ©s
        valid_params = ['gap_density', 'L_ecran_density', 'method', 'include_original']
        filtered_config = {k: v for k, v in augmentation_config.items() if k in valid_params}
        
        # CrÃ©er l'augmentateur et charger les donnÃ©es
        augmenter = DataAugmentation2D(self.dataset_path)
        X_augmented, y_augmented = augmenter.augment_dataset(**filtered_config)
        
        # Sauvegarder en cache
        print(f"ğŸ’¾ Sauvegarde en cache: {self.cache_path}")
        np.savez_compressed(self.cache_path, X=X_augmented, y=y_augmented)
        
        return X_augmented, y_augmented
    
    def prepare_data_splits(self, X, y, train_size=0.64, val_size=0.16, test_size=0.20,
                           random_state=42):
        """
        Divise les donnÃ©es en train/validation/test.
        
        Args:
            X (np.array): DonnÃ©es d'entrÃ©e
            y (np.array): DonnÃ©es de sortie
            train_size (float): Proportion train
            val_size (float): Proportion validation
            test_size (float): Proportion test
            random_state (int): Seed pour reproductibilitÃ©
        
        Returns:
            tuple: (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        print(f"\nğŸ”„ Division des donnÃ©es...")
        print(f"   Train: {train_size:.0%}, Val: {val_size:.0%}, Test: {test_size:.0%}")
        
        # VÃ©rifier que les proportions sont correctes
        assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \
            "Les proportions doivent sommer Ã  1.0"
        
        # PremiÃ¨re division: train+val / test
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=None
        )
        
        # DeuxiÃ¨me division: train / val
        val_size_adjusted = val_size / (train_size + val_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size_adjusted, 
            random_state=random_state, stratify=None
        )
        
        print(f"âœ… Division terminÃ©e:")
        print(f"   Train: {X_train.shape[0]} Ã©chantillons")
        print(f"   Val: {X_val.shape[0]} Ã©chantillons")
        print(f"   Test: {X_test.shape[0]} Ã©chantillons")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def normalize_data(self, X_train, X_val, X_test, y_train, y_val, y_test, 
                      separate_target_scaling=True):
        """
        Normalise les donnÃ©es avec StandardScaler.
        
        Args:
            X_train, X_val, X_test: DonnÃ©es d'entrÃ©e
            y_train, y_val, y_test: DonnÃ©es de sortie
            separate_target_scaling (bool): Scaling sÃ©parÃ© pour gap et L_ecran
        
        Returns:
            tuple: DonnÃ©es normalisÃ©es
        """
        print(f"\nğŸ”§ Normalisation des donnÃ©es...")
        
        # Normalisation des entrÃ©es (profils d'intensitÃ©)
        print(f"   Normalisation des profils d'intensitÃ©...")
        X_train_scaled = self.input_scaler.fit_transform(X_train)
        X_val_scaled = self.input_scaler.transform(X_val)
        X_test_scaled = self.input_scaler.transform(X_test)
        
        if separate_target_scaling:
            # Scaling sÃ©parÃ© pour gap et L_ecran
            print(f"   Scaling sÃ©parÃ© pour gap et L_ecran...")
            
            # Gap (colonne 0)
            y_train_gap_scaled = self.gap_scaler.fit_transform(y_train[:, 0:1])
            y_val_gap_scaled = self.gap_scaler.transform(y_val[:, 0:1])
            y_test_gap_scaled = self.gap_scaler.transform(y_test[:, 0:1])
            
            # L_ecran (colonne 1)
            y_train_L_scaled = self.L_ecran_scaler.fit_transform(y_train[:, 1:2])
            y_val_L_scaled = self.L_ecran_scaler.transform(y_val[:, 1:2])
            y_test_L_scaled = self.L_ecran_scaler.transform(y_test[:, 1:2])
            
            # Recombiner
            y_train_scaled = np.hstack([y_train_gap_scaled, y_train_L_scaled])
            y_val_scaled = np.hstack([y_val_gap_scaled, y_val_L_scaled])
            y_test_scaled = np.hstack([y_test_gap_scaled, y_test_L_scaled])
            
        else:
            # Scaling global pour les deux paramÃ¨tres
            print(f"   Scaling global pour les paramÃ¨tres...")
            y_train_scaled = self.input_scaler.fit_transform(y_train)
            y_val_scaled = self.input_scaler.transform(y_val)
            y_test_scaled = self.input_scaler.transform(y_test)
        
        print(f"âœ… Normalisation terminÃ©e")
        
        return (X_train_scaled, X_val_scaled, X_test_scaled,
                y_train_scaled, y_val_scaled, y_test_scaled)
    
    def create_data_loaders(self, X_train, X_val, X_test, y_train, y_val, y_test,
                           batch_size=32, shuffle_train=True):
        """
        CrÃ©e les DataLoaders PyTorch.
        
        Args:
            X_train, X_val, X_test: DonnÃ©es d'entrÃ©e normalisÃ©es
            y_train, y_val, y_test: DonnÃ©es de sortie normalisÃ©es
            batch_size (int): Taille des batches
            shuffle_train (bool): MÃ©langer les donnÃ©es d'entraÃ®nement
        
        Returns:
            tuple: (train_loader, val_loader, test_loader)
        """
        print(f"\nğŸ”„ CrÃ©ation des DataLoaders (batch_size={batch_size})...")
        
        # CrÃ©er les datasets
        train_dataset = IntensityDataset(X_train, y_train)
        val_dataset = IntensityDataset(X_val, y_val)
        test_dataset = IntensityDataset(X_test, y_test)
        
        # CrÃ©er les loaders
        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, 
            shuffle=shuffle_train, num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, batch_size=batch_size, 
            shuffle=False, num_workers=0
        )
        test_loader = DataLoader(
            test_dataset, batch_size=batch_size, 
            shuffle=False, num_workers=0
        )
        
        print(f"âœ… DataLoaders crÃ©Ã©s:")
        print(f"   Train: {len(train_loader)} batches")
        print(f"   Val: {len(val_loader)} batches")
        print(f"   Test: {len(test_loader)} batches")
        
        return train_loader, val_loader, test_loader
    
    def inverse_transform_predictions(self, predictions, separate_scaling=True):
        """
        Inverse la normalisation des prÃ©dictions.
        
        Args:
            predictions (np.array): PrÃ©dictions normalisÃ©es [n_samples, 2]
            separate_scaling (bool): Utiliser scaling sÃ©parÃ©
        
        Returns:
            np.array: PrÃ©dictions dans l'espace original
        """
        if separate_scaling:
            # Inverse transform sÃ©parÃ©
            pred_gap = self.gap_scaler.inverse_transform(predictions[:, 0:1])
            pred_L_ecran = self.L_ecran_scaler.inverse_transform(predictions[:, 1:2])
            return np.hstack([pred_gap, pred_L_ecran])
        else:
            # Inverse transform global
            return self.input_scaler.inverse_transform(predictions)
    
    def get_complete_pipeline(self, config):
        """
        Pipeline complet de prÃ©paration des donnÃ©es.
        
        Args:
            config (dict): Configuration complÃ¨te
        
        Returns:
            tuple: Tous les Ã©lÃ©ments nÃ©cessaires pour l'entraÃ®nement
        """
        print(f"\nğŸš€ PIPELINE COMPLET DE PRÃ‰PARATION DES DONNÃ‰ES")
        print("="*60)
        
        # 1. Charger et augmenter les donnÃ©es
        augmentation_config = config.get('data_processing', {}).get('augmentation', {})
        X_augmented, y_augmented = self.load_and_augment_data(
            use_cache=True, 
            augmentation_config=augmentation_config
        )
        
        # 2. Diviser les donnÃ©es
        splits_config = config.get('data_processing', {}).get('data_splits', {})
        X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_data_splits(
            X_augmented, y_augmented,
            train_size=splits_config.get('train', 0.64),
            val_size=splits_config.get('validation', 0.16),
            test_size=splits_config.get('test', 0.20)
        )
        
        # 3. Normaliser les donnÃ©es
        normalization_config = config.get('data_processing', {}).get('normalization', {})
        separate_scaling = normalization_config.get('target_scaling', {}).get('separate_scaling', True)
        
        (X_train_scaled, X_val_scaled, X_test_scaled,
         y_train_scaled, y_val_scaled, y_test_scaled) = self.normalize_data(
            X_train, X_val, X_test, y_train, y_val, y_test,
            separate_target_scaling=separate_scaling
        )
        
        # 4. CrÃ©er les DataLoaders
        batch_size = config.get('training', {}).get('batch_size', 32)
        train_loader, val_loader, test_loader = self.create_data_loaders(
            X_train_scaled, X_val_scaled, X_test_scaled,
            y_train_scaled, y_val_scaled, y_test_scaled,
            batch_size=batch_size
        )
        
        print(f"\nâœ… Pipeline terminÃ© avec succÃ¨s !")
        
        return {
            'loaders': (train_loader, val_loader, test_loader),
            'raw_data': (X_train, X_val, X_test, y_train, y_val, y_test),
            'scaled_data': (X_train_scaled, X_val_scaled, X_test_scaled,
                           y_train_scaled, y_val_scaled, y_test_scaled),
            'scalers': (self.input_scaler, self.gap_scaler, self.L_ecran_scaler)
        }


def test_data_loader():
    """
    Test du chargeur de donnÃ©es.
    """
    print("ğŸ§ª Test du DualDataLoader")
    print("="*40)
    
    # Configuration de test
    config = {
        'data_processing': {
            'augmentation': {
                'gap_density': 1,  # Pas d'augmentation pour le test
                'L_ecran_density': 1,
                'method': 'linear',
                'include_original': True
            },
            'data_splits': {
                'train': 0.70,
                'validation': 0.15,
                'test': 0.15
            },
            'normalization': {
                'target_scaling': {
                    'separate_scaling': True
                }
            }
        },
        'training': {
            'batch_size': 16
        }
    }
    
    # CrÃ©er le loader
    data_loader = DualDataLoader()
    
    # Tester le pipeline complet
    try:
        pipeline_result = data_loader.get_complete_pipeline(config)
        print(f"âœ… Test rÃ©ussi ! Pipeline fonctionnel.")
        
        # VÃ©rifier un batch
        train_loader = pipeline_result['loaders'][0]
        for batch_X, batch_y in train_loader:
            print(f"âœ… Batch test: X{batch_X.shape}, y{batch_y.shape}")
            break
            
    except Exception as e:
        print(f"âŒ Erreur dans le test: {e}")


if __name__ == "__main__":
    test_data_loader()
