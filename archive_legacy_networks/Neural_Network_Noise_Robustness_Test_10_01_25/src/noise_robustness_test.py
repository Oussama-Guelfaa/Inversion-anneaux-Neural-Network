#!/usr/bin/env python3
"""
Test de robustesse au bruit pour le mod√®le de pr√©diction du gap

Ce script √©value la performance du mod√®le face √† diff√©rents niveaux de bruit gaussien
pour d√©terminer les conditions optimales de fonctionnement en environnement r√©el.

Auteur: Oussama GUELFAA
Date: 10 - 01 - 2025
"""

import os
import numpy as np
import pandas as pd
import scipy.io as sio
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau

import json
import time
from collections import defaultdict

# Configuration pour reproductibilit√©
torch.manual_seed(42)
np.random.seed(42)

class EarlyStopping:
    """Early stopping pour √©viter l'overfitting."""
    
    def __init__(self, patience=20, min_delta=1e-6, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights and self.best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False

class IntensityDataset(Dataset):
    """Dataset PyTorch pour les profils d'intensit√© avec support du bruit."""
    
    def __init__(self, intensity_profiles, gap_values):
        self.intensity_profiles = torch.FloatTensor(intensity_profiles)
        self.gap_values = torch.FloatTensor(gap_values)
    
    def __len__(self):
        return len(self.intensity_profiles)
    
    def __getitem__(self, idx):
        return self.intensity_profiles[idx], self.gap_values[idx]

class RobustGapPredictor(nn.Module):
    """
    Mod√®le robuste pour pr√©diction du gap avec r√©gularisation.
    
    Architecture optimis√©e pour la robustesse au bruit avec dropout et batch normalization.
    """
    
    def __init__(self, input_size=1000, dropout_rate=0.2):
        super(RobustGapPredictor, self).__init__()
        
        self.fc1 = nn.Linear(input_size, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.dropout1 = nn.Dropout(dropout_rate)
        
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout2 = nn.Dropout(dropout_rate)
        
        self.fc3 = nn.Linear(256, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.dropout3 = nn.Dropout(dropout_rate)
        
        self.fc4 = nn.Linear(128, 1)
        
    def forward(self, x):
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)
        
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)
        
        x = F.relu(self.bn3(self.fc3(x)))
        x = self.dropout3(x)
        
        x = self.fc4(x)
        return x

def load_dataset():
    """
    Charge le dataset complet depuis dataset_small_particle.
    
    Returns:
        tuple: (X, y) o√π X sont les profils d'intensit√© et y les valeurs de gap
    """
    print("=== CHARGEMENT DU DATASET ===")
    
    dataset_dir = "../../data_generation/dataset_small_particle"
    
    if not os.path.exists(dataset_dir):
        raise FileNotFoundError(f"Le dossier {dataset_dir} n'existe pas")
    
    # Lister tous les fichiers .mat
    mat_files = [f for f in os.listdir(dataset_dir) if f.endswith('.mat') and f.startswith('gap_')]
    mat_files.sort()
    
    print(f"Nombre de fichiers trouv√©s: {len(mat_files)}")
    
    X = []  # Profils d'intensit√©
    y = []  # Valeurs de gap
    
    for i, filename in enumerate(mat_files):
        mat_path = os.path.join(dataset_dir, filename)
        
        try:
            data = sio.loadmat(mat_path)
            
            # Extraire le profil d'intensit√© (ratio)
            ratio = data['ratio'].flatten()  # (1000,)
            
            # Extraire la valeur du gap
            gap_value = float(data['gap'][0, 0])
            
            X.append(ratio)
            y.append(gap_value)
            
        except Exception as e:
            print(f"Erreur avec {filename}: {e}")
    
    X = np.array(X)  # (400, 1000)
    y = np.array(y)  # (400,)
    
    print(f"Donn√©es charg√©es: X{X.shape}, y{y.shape}")
    print(f"Gap range: {y.min():.4f} - {y.max():.4f} ¬µm")
    
    return X, y

def add_gaussian_noise(X, noise_level_percent):
    """
    Ajoute du bruit gaussien proportionnel au signal.
    
    Args:
        X (np.ndarray): Donn√©es originales
        noise_level_percent (float): Niveau de bruit en pourcentage (0-100)
        
    Returns:
        np.ndarray: Donn√©es avec bruit ajout√©
    """
    if noise_level_percent == 0:
        return X.copy()
    
    # Calculer l'√©cart-type du signal pour chaque √©chantillon
    signal_std = np.std(X, axis=1, keepdims=True)
    
    # G√©n√©rer le bruit proportionnel
    noise_std = (noise_level_percent / 100.0) * signal_std
    noise = np.random.normal(0, noise_std, X.shape)
    
    X_noisy = X + noise
    
    print(f"Bruit {noise_level_percent}% ajout√© - SNR moyen: {1/(noise_level_percent/100):.1f}")
    
    return X_noisy

def prepare_data_splits(X, y, test_size=0.2, val_size=0.2):
    """
    Divise les donn√©es en train/validation/test.
    
    Args:
        X, y: Donn√©es compl√®tes
        test_size: Proportion pour le test
        val_size: Proportion pour la validation (du reste apr√®s test)
        
    Returns:
        tuple: (X_train, X_val, X_test, y_train, y_val, y_test)
    """
    # Premi√®re division: train+val / test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=None
    )
    
    # Deuxi√®me division: train / val
    val_size_adjusted = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size_adjusted, random_state=42
    )
    
    print(f"Division des donn√©es:")
    print(f"  Train: {X_train.shape[0]} √©chantillons ({X_train.shape[0]/len(X)*100:.1f}%)")
    print(f"  Validation: {X_val.shape[0]} √©chantillons ({X_val.shape[0]/len(X)*100:.1f}%)")
    print(f"  Test: {X_test.shape[0]} √©chantillons ({X_test.shape[0]/len(X)*100:.1f}%)")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def train_model_with_noise(X_train, y_train, X_val, y_val, noise_level, 
                          max_epochs=200, batch_size=16, learning_rate=0.001):
    """
    Entra√Æne un mod√®le avec un niveau de bruit sp√©cifique.
    
    Args:
        X_train, y_train: Donn√©es d'entra√Ænement
        X_val, y_val: Donn√©es de validation
        noise_level: Niveau de bruit en pourcentage
        
    Returns:
        tuple: (model, history, training_time)
    """
    print(f"\n=== ENTRA√éNEMENT AVEC {noise_level}% DE BRUIT ===")
    
    start_time = time.time()
    
    # Ajouter du bruit aux donn√©es d'entra√Ænement UNIQUEMENT
    X_train_noisy = add_gaussian_noise(X_train, noise_level)
    
    # Normalisation
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_noisy)
    X_val_scaled = scaler.transform(X_val)  # Validation SANS bruit
    
    # Datasets et DataLoaders
    train_dataset = IntensityDataset(X_train_scaled, y_train)
    val_dataset = IntensityDataset(X_val_scaled, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # Mod√®le et optimisation
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = RobustGapPredictor(input_size=X_train.shape[1]).to(device)
    
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    criterion = nn.MSELoss()
    early_stopping = EarlyStopping(patience=20)
    
    # Historique d'entra√Ænement
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_r2': [],
        'val_r2': []
    }
    
    print(f"Entra√Ænement sur {device}")
    
    for epoch in range(max_epochs):
        # Phase d'entra√Ænement
        model.train()
        train_loss = 0.0
        train_predictions = []
        train_targets = []
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X).squeeze()
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_predictions.extend(outputs.detach().cpu().numpy())
            train_targets.extend(batch_y.detach().cpu().numpy())
        
        train_loss /= len(train_loader)
        train_r2 = r2_score(train_targets, train_predictions)
        
        # Phase de validation
        model.eval()
        val_loss = 0.0
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X).squeeze()
                loss = criterion(outputs, batch_y)
                
                val_loss += loss.item()
                val_predictions.extend(outputs.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_r2 = r2_score(val_targets, val_predictions)
        
        # Mise √† jour historique
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_r2'].append(train_r2)
        history['val_r2'].append(val_r2)
        
        # Scheduler et early stopping
        scheduler.step(val_loss)
        
        if (epoch + 1) % 20 == 0:
            print(f"  Epoch {epoch+1:3d}: Train R¬≤={train_r2:.4f}, Val R¬≤={val_r2:.4f}, "
                  f"Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}")
        
        if early_stopping(val_loss, model):
            print(f"  Early stopping √† l'√©poque {epoch+1}")
            break
    
    training_time = time.time() - start_time
    
    print(f"Entra√Ænement termin√© en {training_time:.1f}s")
    print(f"Performance finale: R¬≤ = {val_r2:.4f}")
    
    return model, scaler, history, training_time

def evaluate_model(model, scaler, X_test, y_test):
    """
    √âvalue un mod√®le sur l'ensemble de test.
    
    Args:
        model: Mod√®le entra√Æn√©
        scaler: Scaler utilis√©
        X_test, y_test: Donn√©es de test
        
    Returns:
        dict: M√©triques de performance
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()
    
    # Normalisation et pr√©diction
    X_test_scaled = scaler.transform(X_test)
    
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_test_scaled).to(device)
        y_pred = model(X_tensor).squeeze().cpu().numpy()
    
    # Calcul des m√©triques
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    
    metrics = {
        'r2': r2,
        'mse': mse,
        'mae': mae,
        'rmse': rmse,
        'predictions': y_pred
    }
    
    return metrics

def save_complete_results(results, noise_levels, y_test):
    """Sauvegarde tous les r√©sultats du test de robustesse."""

    print("\n=== SAUVEGARDE DES R√âSULTATS ===")

    # R√©sum√© des performances
    summary = {
        'test_type': 'noise_robustness',
        'noise_levels': noise_levels,
        'n_test_samples': len(y_test),
        'results_by_noise': {}
    }

    # Tableau de performance
    performance_data = []

    for noise_level in noise_levels:
        result = results[noise_level]
        metrics = result['metrics']

        summary['results_by_noise'][noise_level] = {
            'r2': metrics['r2'],
            'rmse': metrics['rmse'],
            'mae': metrics['mae'],
            'training_time': result['training_time'],
            'epochs': result['final_epoch']
        }

        performance_data.append({
            'noise_level': noise_level,
            'r2_score': metrics['r2'],
            'rmse_um': metrics['rmse'],
            'mae_um': metrics['mae'],
            'mse': metrics['mse'],
            'training_time_s': result['training_time'],
            'epochs_to_convergence': result['final_epoch']
        })

    # Sauvegarder le r√©sum√© JSON
    with open('../results/noise_robustness_summary.json', 'w') as f:
        json.dump(summary, f, indent=2)

    # Sauvegarder le tableau de performance
    performance_df = pd.DataFrame(performance_data)
    performance_df.to_csv('../results/performance_by_noise_level.csv', index=False)

    # Sauvegarder les pr√©dictions d√©taill√©es
    for noise_level in noise_levels:
        predictions = results[noise_level]['metrics']['predictions']
        pred_df = pd.DataFrame({
            'gap_true': y_test,
            'gap_predicted': predictions,
            'error': predictions - y_test,
            'absolute_error': np.abs(predictions - y_test)
        })
        pred_df.to_csv(f'../results/predictions_noise_{noise_level}percent.csv', index=False)

    print(f"R√©sultats sauvegard√©s:")
    print(f"  - R√©sum√©: ../results/noise_robustness_summary.json")
    print(f"  - Performance: ../results/performance_by_noise_level.csv")
    print(f"  - Pr√©dictions: ../results/predictions_noise_X%.csv")

def create_robustness_plots(results, noise_levels, y_test):
    """Cr√©e les visualisations de robustesse au bruit."""

    print("\n=== G√âN√âRATION DES GRAPHIQUES ===")

    # 1. Courbe principale: R¬≤ vs Niveau de bruit
    plt.figure(figsize=(15, 12))

    # Subplot 1: R¬≤ vs Noise Level
    plt.subplot(2, 3, 1)
    r2_scores = [results[noise]['metrics']['r2'] for noise in noise_levels]
    plt.plot(noise_levels, r2_scores, 'bo-', linewidth=2, markersize=8)
    plt.axhline(y=0.8, color='red', linestyle='--', linewidth=2, label='Objectif R¬≤ = 0.8')
    plt.xlabel('Niveau de bruit (%)')
    plt.ylabel('R¬≤ Score')
    plt.title('Performance vs Niveau de Bruit')
    plt.grid(True, alpha=0.3)
    plt.legend()

    # Subplot 2: RMSE vs Noise Level
    plt.subplot(2, 3, 2)
    rmse_scores = [results[noise]['metrics']['rmse'] for noise in noise_levels]
    plt.plot(noise_levels, rmse_scores, 'ro-', linewidth=2, markersize=8)
    plt.xlabel('Niveau de bruit (%)')
    plt.ylabel('RMSE (¬µm)')
    plt.title('Erreur RMSE vs Niveau de Bruit')
    plt.grid(True, alpha=0.3)

    # Subplot 3: Temps d'entra√Ænement vs Noise Level
    plt.subplot(2, 3, 3)
    training_times = [results[noise]['training_time'] for noise in noise_levels]
    plt.plot(noise_levels, training_times, 'go-', linewidth=2, markersize=8)
    plt.xlabel('Niveau de bruit (%)')
    plt.ylabel('Temps d\'entra√Ænement (s)')
    plt.title('Temps de Convergence vs Bruit')
    plt.grid(True, alpha=0.3)

    # Subplot 4: √âpoques de convergence
    plt.subplot(2, 3, 4)
    epochs = [results[noise]['final_epoch'] for noise in noise_levels]
    plt.plot(noise_levels, epochs, 'mo-', linewidth=2, markersize=8)
    plt.xlabel('Niveau de bruit (%)')
    plt.ylabel('√âpoques de convergence')
    plt.title('Convergence vs Niveau de Bruit')
    plt.grid(True, alpha=0.3)

    # Subplot 5: Comparaison R¬≤ train vs val (pour 0%, 5%, 20%)
    plt.subplot(2, 3, 5)
    selected_noise = [0, 5, 20]
    for noise in selected_noise:
        if noise in results:
            history = results[noise]['history']
            epochs_range = range(1, len(history['val_r2']) + 1)
            plt.plot(epochs_range, history['val_r2'],
                    label=f'{noise}% bruit', linewidth=2)
    plt.xlabel('√âpoque')
    plt.ylabel('R¬≤ Validation')
    plt.title('Convergence R¬≤ par Niveau de Bruit')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 6: Distribution des erreurs pour diff√©rents niveaux
    plt.subplot(2, 3, 6)
    selected_noise = [0, 5, 10, 20]
    for noise in selected_noise:
        if noise in results:
            predictions = results[noise]['metrics']['predictions']
            errors = predictions - y_test
            plt.hist(errors, bins=20, alpha=0.6, label=f'{noise}% bruit')
    plt.xlabel('Erreur de pr√©diction (¬µm)')
    plt.ylabel('Fr√©quence')
    plt.title('Distribution des Erreurs')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('../plots/noise_robustness_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

    # 2. Graphique d√©taill√© des pr√©dictions pour chaque niveau
    create_predictions_comparison(results, noise_levels, y_test)

    print("Graphiques sauvegard√©s:")
    print("  - Vue d'ensemble: ../plots/noise_robustness_analysis.png")
    print("  - Pr√©dictions d√©taill√©es: ../plots/predictions_by_noise.png")

def create_predictions_comparison(results, noise_levels, y_test):
    """Cr√©e un graphique comparatif des pr√©dictions pour chaque niveau de bruit."""

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()

    for i, noise_level in enumerate(noise_levels):
        if i < len(axes):
            ax = axes[i]

            predictions = results[noise_level]['metrics']['predictions']

            # Scatter plot
            ax.scatter(y_test, predictions, alpha=0.6, s=20)

            # Ligne parfaite
            min_val = min(y_test.min(), predictions.min())
            max_val = max(y_test.max(), predictions.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)

            # M√©triques
            r2 = results[noise_level]['metrics']['r2']
            rmse = results[noise_level]['metrics']['rmse']

            ax.set_xlabel('Gap r√©el (¬µm)')
            ax.set_ylabel('Gap pr√©dit (¬µm)')
            ax.set_title(f'Bruit {noise_level}%\nR¬≤ = {r2:.3f}, RMSE = {rmse:.4f}')
            ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('../plots/predictions_by_noise.png', dpi=300, bbox_inches='tight')
    plt.show()

def analyze_robustness_results(results, noise_levels):
    """Analyse et r√©sume les r√©sultats de robustesse."""

    print("\n" + "="*60)
    print("ANALYSE DE LA ROBUSTESSE AU BRUIT")
    print("="*60)

    # Tableau de performance
    print(f"\nüìä PERFORMANCE PAR NIVEAU DE BRUIT:")
    print(f"{'Bruit':<8} {'R¬≤':<8} {'RMSE':<10} {'MAE':<10} {'Temps':<8} {'√âpoques':<8}")
    print("-" * 60)

    for noise_level in noise_levels:
        result = results[noise_level]
        metrics = result['metrics']

        print(f"{noise_level:>5}%   "
              f"{metrics['r2']:>6.3f}   "
              f"{metrics['rmse']:>8.4f}   "
              f"{metrics['mae']:>8.4f}   "
              f"{result['training_time']:>6.1f}s  "
              f"{result['final_epoch']:>6d}")

    # Analyse des seuils
    print(f"\nüéØ ANALYSE DES SEUILS:")

    # Trouver le seuil pour R¬≤ > 0.8
    threshold_80 = None
    for noise_level in noise_levels:
        if results[noise_level]['metrics']['r2'] >= 0.8:
            threshold_80 = noise_level
        else:
            break

    if threshold_80 is not None:
        print(f"  ‚úÖ R¬≤ > 0.8 maintenu jusqu'√† {threshold_80}% de bruit")
    else:
        print(f"  ‚ùå R¬≤ < 0.8 d√®s le premier niveau de bruit test√©")

    # D√©gradation relative
    r2_clean = results[0]['metrics']['r2']
    print(f"\nüìâ D√âGRADATION RELATIVE (vs 0% bruit):")

    for noise_level in noise_levels[1:]:  # Skip 0%
        r2_noisy = results[noise_level]['metrics']['r2']
        degradation = (r2_clean - r2_noisy) / r2_clean * 100
        print(f"  {noise_level:>2}% bruit: -{degradation:>5.1f}% de performance")

    # Recommandations
    print(f"\nüí° RECOMMANDATIONS:")

    if threshold_80 and threshold_80 >= 5:
        print(f"  ‚úÖ Mod√®le robuste - Tol√©rance jusqu'√† {threshold_80}% de bruit")
        print(f"  ‚úÖ Acquisition r√©elle: SNR > {100/threshold_80:.1f} recommand√©")
    elif threshold_80 and threshold_80 >= 2:
        print(f"  ‚ö†Ô∏è  Robustesse mod√©r√©e - Attention aux conditions d'acquisition")
        print(f"  ‚ö†Ô∏è  SNR > {100/threshold_80:.1f} requis pour performance optimale")
    else:
        print(f"  ‚ùå Robustesse insuffisante - Am√©lioration du mod√®le n√©cessaire")
        print(f"  ‚ùå Conditions d'acquisition tr√®s strictes requises")

    print(f"\nüî¨ CONCLUSIONS:")
    print(f"  ‚Ä¢ Mod√®le test√© sur {len(noise_levels)} niveaux de bruit")
    print(f"  ‚Ä¢ Performance de r√©f√©rence: R¬≤ = {r2_clean:.4f}")
    print(f"  ‚Ä¢ Seuil de tol√©rance identifi√©: {threshold_80}% de bruit")
    print(f"  ‚Ä¢ Recommandations pratiques √©tablies")

if __name__ == "__main__":
    print("=== TEST DE ROBUSTESSE AU BRUIT ===")
    print("√âvaluation de la performance du mod√®le face √† diff√©rents niveaux de bruit\n")
    
    # Cr√©er les dossiers de sortie
    os.makedirs("../models", exist_ok=True)
    os.makedirs("../plots", exist_ok=True)
    os.makedirs("../results", exist_ok=True)
    
    # Niveaux de bruit √† tester
    noise_levels = [0, 1, 2, 5, 10, 20]
    
    try:
        # 1. Charger les donn√©es
        X, y = load_dataset()
        
        # 2. Diviser les donn√©es
        X_train, X_val, X_test, y_train, y_val, y_test = prepare_data_splits(X, y)
        
        # 3. Tester chaque niveau de bruit
        results = {}
        
        for noise_level in noise_levels:
            print(f"\n{'='*60}")
            print(f"TEST AVEC {noise_level}% DE BRUIT")
            print(f"{'='*60}")
            
            # Entra√Æner le mod√®le
            model, scaler, history, training_time = train_model_with_noise(
                X_train, y_train, X_val, y_val, noise_level
            )
            
            # √âvaluer sur le test
            metrics = evaluate_model(model, scaler, X_test, y_test)
            
            # Sauvegarder les r√©sultats
            results[noise_level] = {
                'metrics': metrics,
                'history': history,
                'training_time': training_time,
                'final_epoch': len(history['train_loss'])
            }
            
            # Sauvegarder le mod√®le
            torch.save({
                'model_state_dict': model.state_dict(),
                'scaler': scaler,
                'noise_level': noise_level,
                'metrics': metrics
            }, f'../models/model_noise_{noise_level}percent.pth')
            
            print(f"R√©sultats {noise_level}% bruit: R¬≤ = {metrics['r2']:.4f}, "
                  f"RMSE = {metrics['rmse']:.4f} ¬µm")
        
        # 4. Sauvegarder les r√©sultats complets
        save_complete_results(results, noise_levels, y_test)

        # 5. G√©n√©rer les visualisations
        create_robustness_plots(results, noise_levels, y_test)

        # 6. Analyser et r√©sumer
        analyze_robustness_results(results, noise_levels)

        print(f"\n=== TEST DE ROBUSTESSE TERMIN√â ===")
        print(f"üìä Mod√®les sauvegard√©s: ../models/")
        print(f"üìà Graphiques g√©n√©r√©s: ../plots/")
        print(f"üìã R√©sultats d√©taill√©s: ../results/")

    except Exception as e:
        print(f"Erreur durant le test: {e}")
        import traceback
        traceback.print_exc()


