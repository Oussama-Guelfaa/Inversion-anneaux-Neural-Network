#!/usr/bin/env python3
"""
Analyse de la gÃ©nÃ©ralisation locale dans la plage 0.5-1.0 Âµm

Ce script calcule les erreurs relatives entre les prÃ©dictions et la droite idÃ©ale
pour Ã©valuer la finesse de gÃ©nÃ©ralisation du rÃ©seau dans une plage rÃ©aliste d'usage.

Auteur: Oussama GUELFAA
Date: 11 - 01 - 2025
"""

import os
import numpy as np
import pandas as pd
import scipy.io as sio
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

import torch
import torch.nn as nn
import torch.nn.functional as F

import json

# Configuration pour reproductibilitÃ©
torch.manual_seed(42)
np.random.seed(42)

class RobustGapPredictor(nn.Module):
    """ModÃ¨le robuste pour prÃ©diction du gap - Architecture identique."""
    
    def __init__(self, input_size=1000, dropout_rate=0.2):
        super(RobustGapPredictor, self).__init__()
        
        self.fc1 = nn.Linear(input_size, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.dropout1 = nn.Dropout(dropout_rate)
        
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout2 = nn.Dropout(dropout_rate)
        
        self.fc3 = nn.Linear(256, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.dropout3 = nn.Dropout(dropout_rate)
        
        self.fc4 = nn.Linear(128, 1)
        
    def forward(self, x):
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)
        
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)
        
        x = F.relu(self.bn3(self.fc3(x)))
        x = self.dropout3(x)
        
        x = self.fc4(x)
        return x

def load_dataset():
    """Charge le dataset complet depuis dataset_small_particle."""
    
    print("=== CHARGEMENT DU DATASET ===")
    
    dataset_dir = "../../data_generation/dataset_small_particle"
    
    if not os.path.exists(dataset_dir):
        raise FileNotFoundError(f"Le dossier {dataset_dir} n'existe pas")
    
    # Lister tous les fichiers .mat
    mat_files = [f for f in os.listdir(dataset_dir) if f.endswith('.mat') and f.startswith('gap_')]
    mat_files.sort()
    
    print(f"Nombre de fichiers trouvÃ©s: {len(mat_files)}")
    
    X = []  # Profils d'intensitÃ©
    y = []  # Valeurs de gap
    
    for i, filename in enumerate(mat_files):
        mat_path = os.path.join(dataset_dir, filename)
        
        try:
            data = sio.loadmat(mat_path)
            
            # Extraire le profil d'intensitÃ© (ratio)
            ratio = data['ratio'].flatten()  # (1000,)
            
            # Extraire la valeur du gap
            gap_value = float(data['gap'][0, 0])
            
            X.append(ratio)
            y.append(gap_value)
            
        except Exception as e:
            print(f"Erreur avec {filename}: {e}")
    
    X = np.array(X)  # (400, 1000)
    y = np.array(y)  # (400,)
    
    print(f"DonnÃ©es chargÃ©es: X{X.shape}, y{y.shape}")
    print(f"Gap range: {y.min():.4f} - {y.max():.4f} Âµm")
    
    return X, y

def add_gaussian_noise(X, noise_level_percent):
    """Ajoute du bruit gaussien proportionnel au signal."""
    
    if noise_level_percent == 0:
        return X.copy()
    
    # Calculer l'Ã©cart-type du signal pour chaque Ã©chantillon
    signal_std = np.std(X, axis=1, keepdims=True)
    
    # GÃ©nÃ©rer le bruit proportionnel
    noise_std = (noise_level_percent / 100.0) * signal_std
    noise = np.random.normal(0, noise_std, X.shape)
    
    X_noisy = X + noise
    
    return X_noisy

def recreate_experiment_data():
    """RecrÃ©e exactement les mÃªmes donnÃ©es que l'expÃ©rience originale."""
    
    print("=== RECRÃ‰ATION DES DONNÃ‰ES D'EXPÃ‰RIENCE ===")
    
    # Charger le dataset complet
    X, y = load_dataset()
    
    # Reproduire exactement la mÃªme division (mÃªme seed)
    total_needed = 400  # 300 train + 100 test
    indices = np.random.choice(len(X), size=total_needed, replace=False)
    
    # Diviser en train/test
    train_indices = indices[:300]
    test_indices = indices[300:]
    
    X_train = X[train_indices]
    y_train = y[train_indices]
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    print(f"DonnÃ©es recrÃ©Ã©es:")
    print(f"  Train: X{X_train.shape}, y{y_train.shape}")
    print(f"  Test: X{X_test.shape}, y{y_test.shape}")
    
    return X_train, X_test, y_train, y_test

def train_and_predict():
    """EntraÃ®ne le modÃ¨le et gÃ©nÃ¨re les prÃ©dictions."""
    
    print("=== ENTRAÃNEMENT ET PRÃ‰DICTION ===")
    
    # RecrÃ©er les donnÃ©es
    X_train, X_test, y_train, y_test = recreate_experiment_data()
    
    # Ajouter du bruit aux donnÃ©es d'entraÃ®nement (5%)
    X_train_noisy = add_gaussian_noise(X_train, 5)
    
    # Normalisation
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_noisy)
    X_test_scaled = scaler.transform(X_test)  # Test sans bruit
    
    # ModÃ¨le
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = RobustGapPredictor(input_size=X_train.shape[1]).to(device)
    
    # EntraÃ®nement rapide (version simplifiÃ©e pour l'analyse)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    criterion = nn.MSELoss()
    
    # Convertir en tensors
    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)
    y_train_tensor = torch.FloatTensor(y_train).to(device)
    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
    
    print("EntraÃ®nement en cours...")
    model.train()
    
    # EntraÃ®nement simplifiÃ© (50 Ã©poques pour l'analyse)
    for epoch in range(50):
        optimizer.zero_grad()
        outputs = model(X_train_tensor).squeeze()
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 10 == 0:
            print(f"  Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    # PrÃ©dictions sur le test
    model.eval()
    with torch.no_grad():
        y_pred = model(X_test_tensor).squeeze().cpu().numpy()
    
    print(f"PrÃ©dictions gÃ©nÃ©rÃ©es pour {len(y_test)} Ã©chantillons de test")
    
    return y_test, y_pred

def analyze_local_generalization(y_true, y_pred, range_min=0.5, range_max=1.0):
    """
    Analyse la gÃ©nÃ©ralisation locale dans la plage spÃ©cifiÃ©e.
    
    Args:
        y_true: Valeurs rÃ©elles de gap
        y_pred: Valeurs prÃ©dites de gap
        range_min: Limite infÃ©rieure de la plage (Âµm)
        range_max: Limite supÃ©rieure de la plage (Âµm)
    """
    
    print(f"\n=== ANALYSE GÃ‰NÃ‰RALISATION LOCALE [{range_min}-{range_max} Âµm] ===")
    
    # Filtrer les points dans la plage spÃ©cifiÃ©e
    mask = (y_true >= range_min) & (y_true <= range_max)
    y_true_range = y_true[mask]
    y_pred_range = y_pred[mask]
    
    print(f"Nombre de points dans la plage [{range_min}-{range_max} Âµm]: {len(y_true_range)}")
    
    if len(y_true_range) == 0:
        print("âŒ Aucun point dans la plage spÃ©cifiÃ©e")
        return
    
    # Calculer les erreurs relatives
    relative_errors = np.abs((y_pred_range - y_true_range) / y_true_range) * 100
    
    # Statistiques des erreurs relatives
    max_error = np.max(relative_errors)
    mean_error = np.mean(relative_errors)
    median_error = np.median(relative_errors)
    std_error = np.std(relative_errors)
    
    print(f"\nğŸ“Š STATISTIQUES DES ERREURS RELATIVES:")
    print(f"  Erreur maximale:  {max_error:.3f}%")
    print(f"  Erreur moyenne:   {mean_error:.3f}%")
    print(f"  Erreur mÃ©diane:   {median_error:.3f}%")
    print(f"  Ã‰cart-type:       {std_error:.3f}%")
    
    # VÃ©rification du seuil de 5%
    points_under_5_percent = np.sum(relative_errors < 5.0)
    percentage_under_5 = (points_under_5_percent / len(relative_errors)) * 100
    
    print(f"\nğŸ¯ Ã‰VALUATION DU SEUIL 5%:")
    print(f"  Points < 5% d'erreur: {points_under_5_percent}/{len(relative_errors)} ({percentage_under_5:.1f}%)")
    
    if max_error < 5.0:
        print(f"  âœ… EXCELLENT: Toutes les erreurs < 5%")
        print(f"  âœ… GÃ©nÃ©ralisation locale exceptionnelle")
    elif mean_error < 5.0:
        print(f"  âœ… TRÃˆS BON: Erreur moyenne < 5%")
        print(f"  âš ï¸  Quelques points dÃ©passent 5% (max: {max_error:.3f}%)")
    else:
        print(f"  âŒ PROBLÃ‰MATIQUE: Erreur moyenne > 5%")
        print(f"  âŒ GÃ©nÃ©ralisation locale insuffisante")
    
    # Analyse dÃ©taillÃ©e des points
    print(f"\nğŸ“‹ ANALYSE DÃ‰TAILLÃ‰E DES POINTS:")
    print(f"{'Gap RÃ©el':<10} {'Gap PrÃ©dit':<12} {'Erreur Abs':<12} {'Erreur Rel':<12}")
    print("-" * 50)
    
    # Trier par erreur relative dÃ©croissante
    sorted_indices = np.argsort(relative_errors)[::-1]
    
    # Afficher les 10 premiers (pires erreurs)
    for i in sorted_indices[:min(10, len(sorted_indices))]:
        gap_real = y_true_range[i]
        gap_pred = y_pred_range[i]
        abs_error = abs(gap_pred - gap_real)
        rel_error = relative_errors[i]
        
        print(f"{gap_real:<10.4f} {gap_pred:<12.4f} {abs_error:<12.4f} {rel_error:<12.3f}%")
    
    # CrÃ©er un graphique spÃ©cifique pour cette plage
    create_local_analysis_plot(y_true_range, y_pred_range, relative_errors, range_min, range_max)
    
    return {
        'n_points': len(y_true_range),
        'max_error': max_error,
        'mean_error': mean_error,
        'median_error': median_error,
        'std_error': std_error,
        'points_under_5_percent': points_under_5_percent,
        'percentage_under_5': percentage_under_5,
        'all_under_5': max_error < 5.0
    }

def create_local_analysis_plot(y_true, y_pred, relative_errors, range_min, range_max):
    """CrÃ©e un graphique spÃ©cifique pour l'analyse locale."""
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # 1. Scatter plot pour la plage spÃ©cifique
    axes[0].scatter(y_true, y_pred, alpha=0.7, s=50, c='blue', edgecolors='black', linewidth=0.5)
    
    # Ligne parfaite
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='PrÃ©diction parfaite')
    
    axes[0].set_xlabel('Gap rÃ©el (Âµm)')
    axes[0].set_ylabel('Gap prÃ©dit (Âµm)')
    axes[0].set_title(f'GÃ©nÃ©ralisation Locale [{range_min}-{range_max} Âµm]\n{len(y_true)} points')
    axes[0].grid(True, alpha=0.3)
    axes[0].legend()
    axes[0].axis('equal')
    
    # 2. Distribution des erreurs relatives
    axes[1].hist(relative_errors, bins=15, alpha=0.7, color='orange', edgecolor='black')
    axes[1].axvline(x=5.0, color='red', linestyle='--', linewidth=2, label='Seuil 5%')
    axes[1].axvline(x=np.mean(relative_errors), color='green', linestyle='-', linewidth=2, label=f'Moyenne: {np.mean(relative_errors):.2f}%')
    axes[1].set_xlabel('Erreur relative (%)')
    axes[1].set_ylabel('FrÃ©quence')
    axes[1].set_title('Distribution des Erreurs Relatives')
    axes[1].grid(True, alpha=0.3)
    axes[1].legend()
    
    # 3. Erreur relative vs Gap rÃ©el
    axes[2].scatter(y_true, relative_errors, alpha=0.7, s=50, c='red', edgecolors='black', linewidth=0.5)
    axes[2].axhline(y=5.0, color='red', linestyle='--', linewidth=2, label='Seuil 5%')
    axes[2].axhline(y=np.mean(relative_errors), color='green', linestyle='-', linewidth=2, label=f'Moyenne: {np.mean(relative_errors):.2f}%')
    axes[2].set_xlabel('Gap rÃ©el (Âµm)')
    axes[2].set_ylabel('Erreur relative (%)')
    axes[2].set_title('Erreur Relative vs Gap RÃ©el')
    axes[2].grid(True, alpha=0.3)
    axes[2].legend()
    
    plt.tight_layout()
    plt.savefig(f'../plots/local_generalization_analysis_{range_min}_{range_max}um.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"Graphique d'analyse locale sauvegardÃ©: ../plots/local_generalization_analysis_{range_min}_{range_max}um.png")

def main():
    """Fonction principale d'analyse."""
    
    print("="*60)
    print("ANALYSE DE LA GÃ‰NÃ‰RALISATION LOCALE [0.5-1.0 Âµm]")
    print("="*60)
    print("Calcul des erreurs relatives par rapport Ã  la droite idÃ©ale")
    print("="*60)
    
    try:
        # EntraÃ®ner le modÃ¨le et obtenir les prÃ©dictions
        y_true, y_pred = train_and_predict()
        
        # Analyser la gÃ©nÃ©ralisation locale dans la plage 0.5-1.0 Âµm
        results = analyze_local_generalization(y_true, y_pred, range_min=0.5, range_max=1.0)
        
        # Sauvegarder les rÃ©sultats
        if results:
            with open('../results/local_generalization_analysis.json', 'w') as f:
                json.dump(results, f, indent=2)
            
            print(f"\n{'='*60}")
            print("RÃ‰SUMÃ‰ FINAL")
            print(f"{'='*60}")
            
            print(f"ğŸ¯ PLAGE ANALYSÃ‰E: 0.5-1.0 Âµm")
            print(f"ğŸ“Š POINTS ANALYSÃ‰S: {results['n_points']}")
            print(f"ğŸ“ˆ ERREUR MAXIMALE: {results['max_error']:.3f}%")
            print(f"ğŸ“ˆ ERREUR MOYENNE: {results['mean_error']:.3f}%")
            print(f"ğŸ“ˆ ERREUR MÃ‰DIANE: {results['median_error']:.3f}%")
            
            if results['all_under_5']:
                print(f"âœ… CONFIRMATION: Toutes les erreurs < 5%")
                print(f"âœ… GÃ‰NÃ‰RALISATION LOCALE EXCEPTIONNELLE")
            else:
                print(f"âš ï¸  ATTENTION: {results['points_under_5_percent']}/{results['n_points']} points < 5%")
                print(f"âš ï¸  POURCENTAGE SOUS SEUIL: {results['percentage_under_5']:.1f}%")
            
            print(f"\nğŸ“‹ RÃ©sultats sauvegardÃ©s: ../results/local_generalization_analysis.json")
        
    except Exception as e:
        print(f"âŒ Erreur durant l'analyse: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
