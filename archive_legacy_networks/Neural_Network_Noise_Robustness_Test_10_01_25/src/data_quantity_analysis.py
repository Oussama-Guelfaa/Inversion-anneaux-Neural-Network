#!/usr/bin/env python3
"""
Analyse de l'impact de la quantit√© de donn√©es sur les performances

Ce script compare les performances avec diff√©rentes quantit√©s de donn√©es
pour √©tablir la relation entre taille du dataset et capacit√© de g√©n√©ralisation.

Auteur: Oussama GUELFAA
Date: 11 - 01 - 2025
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from pathlib import Path

def load_experiment_results():
    """Charge les r√©sultats des diff√©rentes exp√©riences."""
    
    print("=== CHARGEMENT DES R√âSULTATS D'EXP√âRIENCES ===")
    
    results = {}
    
    # R√©sultats de l'exp√©rience originale (240 train, 80 test)
    original_path = "../results/noise_robustness_summary.json"
    if os.path.exists(original_path):
        with open(original_path, 'r') as f:
            original_data = json.load(f)
            if '5' in original_data.get('results_by_noise', {}):
                results['original'] = {
                    'n_train': 240,  # 60% de 400
                    'n_test': 80,    # 20% de 400
                    'r2': original_data['results_by_noise']['5']['r2'],
                    'rmse': original_data['results_by_noise']['5']['rmse'],
                    'mae': original_data['results_by_noise']['5']['mae']
                }
                print(f"‚úÖ Exp√©rience originale charg√©e: R¬≤ = {results['original']['r2']:.4f}")
    
    # R√©sultats de l'exp√©rience avec donn√©es r√©duites (300 train, 100 test)
    reduced_path = "../results/reduced_data_experiment.json"
    if os.path.exists(reduced_path):
        with open(reduced_path, 'r') as f:
            reduced_data = json.load(f)
            results['reduced'] = {
                'n_train': reduced_data['experiment_config']['n_train'],
                'n_test': reduced_data['experiment_config']['n_test'],
                'r2': reduced_data['performance']['r2'],
                'rmse': reduced_data['performance']['rmse'],
                'mae': reduced_data['performance']['mae']
            }
            print(f"‚úÖ Exp√©rience donn√©es r√©duites charg√©e: R¬≤ = {results['reduced']['r2']:.4f}")
    
    return results

def create_comprehensive_analysis(results):
    """Cr√©e une analyse compl√®te de l'impact de la quantit√© de donn√©es."""
    
    print(f"\n=== ANALYSE COMPARATIVE ===")
    
    if len(results) < 2:
        print("‚ùå Pas assez de r√©sultats pour la comparaison")
        return
    
    # Cr√©er le graphique comparatif
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # Extraire les donn√©es pour les graphiques
    experiments = list(results.keys())
    n_trains = [results[exp]['n_train'] for exp in experiments]
    r2_scores = [results[exp]['r2'] for exp in experiments]
    rmse_values = [results[exp]['rmse'] for exp in experiments]
    mae_values = [results[exp]['mae'] for exp in experiments]
    
    colors = ['blue', 'red', 'green', 'orange']
    
    # 1. R¬≤ vs Nombre d'√©chantillons d'entra√Ænement
    axes[0, 0].scatter(n_trains, r2_scores, s=100, c=colors[:len(experiments)], alpha=0.7)
    for i, exp in enumerate(experiments):
        axes[0, 0].annotate(f'{exp}\n({n_trains[i]} √©chantillons)', 
                           (n_trains[i], r2_scores[i]), 
                           xytext=(10, 10), textcoords='offset points',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[i], alpha=0.3))
    
    axes[0, 0].set_xlabel('Nombre d\'√©chantillons d\'entra√Ænement')
    axes[0, 0].set_ylabel('R¬≤ Score')
    axes[0, 0].set_title('Performance vs Quantit√© de Donn√©es d\'Entra√Ænement')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Seuil acceptable (0.8)')
    axes[0, 0].legend()
    
    # 2. RMSE vs Nombre d'√©chantillons
    axes[0, 1].scatter(n_trains, rmse_values, s=100, c=colors[:len(experiments)], alpha=0.7)
    for i, exp in enumerate(experiments):
        axes[0, 1].annotate(f'{exp}\n({rmse_values[i]:.3f} ¬µm)', 
                           (n_trains[i], rmse_values[i]), 
                           xytext=(10, 10), textcoords='offset points',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[i], alpha=0.3))
    
    axes[0, 1].set_xlabel('Nombre d\'√©chantillons d\'entra√Ænement')
    axes[0, 1].set_ylabel('RMSE (¬µm)')
    axes[0, 1].set_title('Erreur RMSE vs Quantit√© de Donn√©es')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Comparaison des m√©triques
    metrics = ['R¬≤', 'RMSE (¬µm)', 'MAE (¬µm)']
    x_pos = np.arange(len(metrics))
    width = 0.35
    
    if 'original' in results and 'reduced' in results:
        original_values = [results['original']['r2'], results['original']['rmse'], results['original']['mae']]
        reduced_values = [results['reduced']['r2'], results['reduced']['rmse'], results['reduced']['mae']]
        
        axes[1, 0].bar(x_pos - width/2, original_values, width, label=f'Original ({results["original"]["n_train"]} train)', alpha=0.7)
        axes[1, 0].bar(x_pos + width/2, reduced_values, width, label=f'R√©duit ({results["reduced"]["n_train"]} train)', alpha=0.7)
        
        axes[1, 0].set_xlabel('M√©triques')
        axes[1, 0].set_ylabel('Valeur')
        axes[1, 0].set_title('Comparaison des M√©triques de Performance')
        axes[1, 0].set_xticks(x_pos)
        axes[1, 0].set_xticklabels(metrics)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        # Ajouter les valeurs sur les barres
        for i, (orig, red) in enumerate(zip(original_values, reduced_values)):
            axes[1, 0].text(i - width/2, orig + orig*0.01, f'{orig:.3f}', ha='center', va='bottom', fontweight='bold')
            axes[1, 0].text(i + width/2, red + red*0.01, f'{red:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 4. Efficacit√© des donn√©es (Performance par √©chantillon)
    efficiency = [r2 / n_train for r2, n_train in zip(r2_scores, n_trains)]
    
    axes[1, 1].bar(experiments, efficiency, color=colors[:len(experiments)], alpha=0.7)
    axes[1, 1].set_xlabel('Exp√©rience')
    axes[1, 1].set_ylabel('R¬≤ / Nombre d\'√©chantillons')
    axes[1, 1].set_title('Efficacit√© des Donn√©es\n(Performance par √©chantillon)')
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    # Ajouter les valeurs sur les barres
    for i, (exp, eff) in enumerate(zip(experiments, efficiency)):
        axes[1, 1].text(i, eff + eff*0.01, f'{eff:.6f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('../plots/data_quantity_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("Analyse comparative sauvegard√©e: ../plots/data_quantity_analysis.png")

def generate_insights(results):
    """G√©n√®re des insights sur l'impact de la quantit√© de donn√©es."""
    
    print(f"\n" + "="*60)
    print("INSIGHTS SUR L'IMPACT DE LA QUANTIT√â DE DONN√âES")
    print("="*60)
    
    if 'original' in results and 'reduced' in results:
        orig = results['original']
        red = results['reduced']
        
        print(f"\nüìä COMPARAISON D√âTAILL√âE:")
        print(f"{'M√©trique':<15} {'Original':<15} {'R√©duit':<15} {'Diff√©rence':<15}")
        print("-" * 65)
        print(f"{'N¬∞ train':<15} {orig['n_train']:<15} {red['n_train']:<15} {red['n_train'] - orig['n_train']:+<15}")
        print(f"{'R¬≤ Score':<15} {orig['r2']:<15.4f} {red['r2']:<15.4f} {red['r2'] - orig['r2']:+<15.4f}")
        print(f"{'RMSE (¬µm)':<15} {orig['rmse']:<15.4f} {red['rmse']:<15.4f} {red['rmse'] - orig['rmse']:+<15.4f}")
        print(f"{'MAE (¬µm)':<15} {orig['mae']:<15.4f} {red['mae']:<15.4f} {red['mae'] - orig['mae']:+<15.4f}")
        
        # Calcul des am√©liorations relatives
        r2_improvement = ((red['r2'] - orig['r2']) / orig['r2']) * 100
        rmse_improvement = ((orig['rmse'] - red['rmse']) / orig['rmse']) * 100
        data_increase = ((red['n_train'] - orig['n_train']) / orig['n_train']) * 100
        
        print(f"\nüìà AM√âLIORATIONS RELATIVES:")
        print(f"  Augmentation donn√©es: +{data_increase:.1f}%")
        print(f"  Am√©lioration R¬≤: {r2_improvement:+.2f}%")
        print(f"  Am√©lioration RMSE: {rmse_improvement:+.2f}%")
        
        print(f"\nüéØ CONCLUSIONS:")
        
        if abs(r2_improvement) < 1:
            print(f"  ‚úÖ R√âSULTAT SURPRENANT: Performance quasi-identique")
            print(f"     ‚Ä¢ +{data_increase:.0f}% de donn√©es ‚Üí {r2_improvement:+.1f}% de performance")
            print(f"     ‚Ä¢ Le mod√®le semble d√©j√† bien optimis√© avec 240 √©chantillons")
            print(f"     ‚Ä¢ Rendements d√©croissants observ√©s")
        elif r2_improvement > 2:
            print(f"  üìà AM√âLIORATION SIGNIFICATIVE avec plus de donn√©es")
            print(f"     ‚Ä¢ +{data_increase:.0f}% de donn√©es ‚Üí +{r2_improvement:.1f}% de performance")
            print(f"     ‚Ä¢ B√©n√©fice clair de l'augmentation des donn√©es")
        else:
            print(f"  ‚öñÔ∏è  AM√âLIORATION MOD√âR√âE")
            print(f"     ‚Ä¢ +{data_increase:.0f}% de donn√©es ‚Üí +{r2_improvement:.1f}% de performance")
        
        # Efficacit√© des donn√©es
        orig_efficiency = orig['r2'] / orig['n_train']
        red_efficiency = red['r2'] / red['n_train']
        
        print(f"\n‚ö° EFFICACIT√â DES DONN√âES:")
        print(f"  Original: {orig_efficiency:.6f} R¬≤/√©chantillon")
        print(f"  R√©duit: {red_efficiency:.6f} R¬≤/√©chantillon")
        
        if red_efficiency > orig_efficiency:
            print(f"  ‚úÖ Plus de donn√©es = Meilleure efficacit√©")
        else:
            print(f"  ‚ö†Ô∏è  Rendements d√©croissants observ√©s")
    
    print(f"\nüí° RECOMMANDATIONS G√âN√âRALES:")
    
    # Analyser les performances absolues
    best_r2 = max([results[exp]['r2'] for exp in results])
    
    if best_r2 > 0.99:
        print(f"  üéâ PERFORMANCE EXCEPTIONNELLE (R¬≤ > 0.99)")
        print(f"     ‚Ä¢ Le mod√®le atteint d√©j√† une pr√©cision quasi-parfaite")
        print(f"     ‚Ä¢ Augmenter les donn√©es n'apportera que des gains marginaux")
        print(f"     ‚Ä¢ Focus recommand√©: Validation sur donn√©es r√©elles")
    elif best_r2 > 0.95:
        print(f"  ‚úÖ PERFORMANCE TR√àS BONNE (R¬≤ > 0.95)")
        print(f"     ‚Ä¢ Qualit√© suffisante pour la plupart des applications")
        print(f"     ‚Ä¢ Gains potentiels limit√©s avec plus de donn√©es")
    elif best_r2 > 0.8:
        print(f"  üëç PERFORMANCE ACCEPTABLE (R¬≤ > 0.8)")
        print(f"     ‚Ä¢ Plus de donn√©es pourraient am√©liorer significativement")
        print(f"     ‚Ä¢ Recommandation: Tester avec 500+ √©chantillons")
    else:
        print(f"  ‚ùå PERFORMANCE INSUFFISANTE")
        print(f"     ‚Ä¢ Augmentation substantielle des donn√©es n√©cessaire")
        print(f"     ‚Ä¢ Ou r√©vision de l'architecture du mod√®le")
    
    print(f"\nüî¨ POUR CETTE T√ÇCHE SP√âCIFIQUE:")
    print(f"  ‚Ä¢ 240-300 √©chantillons semblent suffisants")
    print(f"  ‚Ä¢ Performance plateau atteint autour de R¬≤ = 0.99")
    print(f"  ‚Ä¢ Priorit√©: Validation sur donn√©es exp√©rimentales r√©elles")

def main():
    """Fonction principale d'analyse."""
    
    print("="*60)
    print("ANALYSE DE L'IMPACT DE LA QUANTIT√â DE DONN√âES")
    print("="*60)
    
    try:
        # Charger les r√©sultats des exp√©riences
        results = load_experiment_results()
        
        if len(results) == 0:
            print("‚ùå Aucun r√©sultat d'exp√©rience trouv√©")
            return
        
        # Cr√©er l'analyse comparative
        create_comprehensive_analysis(results)
        
        # G√©n√©rer les insights
        generate_insights(results)
        
        print(f"\n{'='*60}")
        print("ANALYSE TERMIN√âE")
        print(f"{'='*60}")
        print(f"üìä Graphiques g√©n√©r√©s: ../plots/data_quantity_analysis.png")
        
    except Exception as e:
        print(f"‚ùå Erreur durant l'analyse: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
