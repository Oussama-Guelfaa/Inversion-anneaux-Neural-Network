#!/usr/bin/env python3
"""
Script principal pour le test complet de robustesse au bruit

Ce script orchestre tous les tests de robustesse au bruit:
1. Test de base sans augmentation
2. Test avec augmentation de donnÃ©es
3. Comparaison et analyse complÃ¨te

Auteur: Oussama GUELFAA
Date: 10 - 01 - 2025
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path

def run_basic_robustness_test():
    """ExÃ©cute le test de robustesse de base."""
    
    print("="*60)
    print("Ã‰TAPE 1: TEST DE ROBUSTESSE DE BASE")
    print("="*60)
    
    try:
        from noise_robustness_test import main as run_noise_test
        
        # Simuler l'exÃ©cution du test principal
        print("Lancement du test de robustesse au bruit...")
        
        # Importer et exÃ©cuter le code principal
        exec(open('noise_robustness_test.py').read())
        
        print("âœ… Test de robustesse de base terminÃ©")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur dans le test de base: {e}")
        return False

def run_augmentation_test():
    """ExÃ©cute le test avec augmentation de donnÃ©es."""
    
    print("\n" + "="*60)
    print("Ã‰TAPE 2: TEST AVEC AUGMENTATION DE DONNÃ‰ES")
    print("="*60)
    
    try:
        from test_augmentation_robustness import main as run_aug_test
        
        print("Lancement du test avec augmentation...")
        
        # Importer et exÃ©cuter le test d'augmentation
        exec(open('test_augmentation_robustness.py').read())
        
        print("âœ… Test avec augmentation terminÃ©")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur dans le test d'augmentation: {e}")
        return False

def generate_comprehensive_report():
    """GÃ©nÃ¨re un rapport complet de tous les tests."""
    
    print("\n" + "="*60)
    print("Ã‰TAPE 3: GÃ‰NÃ‰RATION DU RAPPORT COMPLET")
    print("="*60)
    
    try:
        # Charger les rÃ©sultats des diffÃ©rents tests
        results = {}
        
        # RÃ©sultats du test de base
        if os.path.exists('../results/noise_robustness_summary.json'):
            with open('../results/noise_robustness_summary.json', 'r') as f:
                results['basic_robustness'] = json.load(f)
        
        # RÃ©sultats du test d'augmentation
        if os.path.exists('../results/augmentation_comparison.json'):
            with open('../results/augmentation_comparison.json', 'r') as f:
                results['augmentation_comparison'] = json.load(f)
        
        # GÃ©nÃ©rer le rapport consolidÃ©
        report = generate_consolidated_report(results)
        
        # Sauvegarder le rapport
        with open('../results/comprehensive_robustness_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # GÃ©nÃ©rer le rapport markdown
        generate_markdown_report(report)
        
        print("âœ… Rapport complet gÃ©nÃ©rÃ©")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur dans la gÃ©nÃ©ration du rapport: {e}")
        return False

def generate_consolidated_report(results):
    """GÃ©nÃ¨re un rapport consolidÃ© de tous les tests."""
    
    report = {
        'test_summary': {
            'test_date': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_type': 'comprehensive_noise_robustness',
            'tests_completed': list(results.keys())
        },
        'key_findings': {},
        'recommendations': {},
        'detailed_results': results
    }
    
    # Analyser les rÃ©sultats du test de base
    if 'basic_robustness' in results:
        basic_results = results['basic_robustness']
        
        # Trouver le seuil de tolÃ©rance
        threshold_80 = None
        for noise_level, data in basic_results.get('results_by_noise', {}).items():
            if data['r2'] >= 0.8:
                threshold_80 = int(noise_level)
            else:
                break
        
        report['key_findings']['noise_tolerance_threshold'] = threshold_80
        report['key_findings']['max_tested_noise'] = max([int(k) for k in basic_results.get('results_by_noise', {}).keys()])
        
        # Performance de rÃ©fÃ©rence (0% bruit)
        if '0' in basic_results.get('results_by_noise', {}):
            ref_performance = basic_results['results_by_noise']['0']
            report['key_findings']['reference_performance'] = {
                'r2': ref_performance['r2'],
                'rmse': ref_performance['rmse']
            }
    
    # Analyser les bÃ©nÃ©fices de l'augmentation
    if 'augmentation_comparison' in results:
        aug_results = results['augmentation_comparison']
        
        if 'comparison' in aug_results:
            improvements = []
            for noise_level, data in aug_results['comparison'].items():
                improvements.append(data['r2_improvement_percent'])
            
            avg_improvement = sum(improvements) / len(improvements) if improvements else 0
            report['key_findings']['augmentation_benefit'] = {
                'average_r2_improvement_percent': avg_improvement,
                'beneficial': avg_improvement > 1.0  # Seuil de 1% d'amÃ©lioration
            }
    
    # GÃ©nÃ©rer les recommandations
    report['recommendations'] = generate_recommendations(report['key_findings'])
    
    return report

def generate_recommendations(findings):
    """GÃ©nÃ¨re des recommandations basÃ©es sur les rÃ©sultats."""
    
    recommendations = {
        'acquisition_specifications': {},
        'model_deployment': {},
        'future_improvements': {}
    }
    
    # Recommandations d'acquisition
    threshold = findings.get('noise_tolerance_threshold')
    if threshold:
        if threshold >= 5:
            recommendations['acquisition_specifications'] = {
                'snr_requirement': f"SNR > {100/threshold:.1f}",
                'noise_level_max': f"{threshold}%",
                'acquisition_quality': "Standard - ModÃ¨le robuste"
            }
        elif threshold >= 2:
            recommendations['acquisition_specifications'] = {
                'snr_requirement': f"SNR > {100/threshold:.1f}",
                'noise_level_max': f"{threshold}%",
                'acquisition_quality': "Ã‰levÃ©e - Attention aux conditions"
            }
        else:
            recommendations['acquisition_specifications'] = {
                'snr_requirement': "SNR > 50",
                'noise_level_max': "< 2%",
                'acquisition_quality': "TrÃ¨s Ã©levÃ©e - Conditions strictes"
            }
    
    # Recommandations de dÃ©ploiement
    ref_perf = findings.get('reference_performance', {})
    if ref_perf.get('r2', 0) > 0.95:
        recommendations['model_deployment'] = {
            'readiness': "PrÃªt pour dÃ©ploiement",
            'confidence_level': "Ã‰levÃ©e",
            'monitoring': "Standard"
        }
    else:
        recommendations['model_deployment'] = {
            'readiness': "NÃ©cessite optimisation",
            'confidence_level': "ModÃ©rÃ©e",
            'monitoring': "RenforcÃ©"
        }
    
    # Recommandations d'amÃ©lioration
    aug_benefit = findings.get('augmentation_benefit', {})
    if aug_benefit.get('beneficial', False):
        recommendations['future_improvements'] = {
            'data_augmentation': "RecommandÃ©e - BÃ©nÃ©fice dÃ©montrÃ©",
            'priority': "Haute",
            'next_steps': ["Optimiser paramÃ¨tres d'augmentation", "Tester autres techniques"]
        }
    else:
        recommendations['future_improvements'] = {
            'data_augmentation': "Peu bÃ©nÃ©fique",
            'priority': "Basse",
            'next_steps': ["Explorer autres architectures", "AmÃ©liorer qualitÃ© des donnÃ©es"]
        }
    
    return recommendations

def generate_markdown_report(report):
    """GÃ©nÃ¨re un rapport au format Markdown."""
    
    markdown_content = f"""# Rapport Complet - Test de Robustesse au Bruit

**Date:** {report['test_summary']['test_date']}  
**Auteur:** Oussama GUELFAA  
**Type de test:** {report['test_summary']['test_type']}

## ğŸ¯ RÃ©sumÃ© ExÃ©cutif

### Tests RÃ©alisÃ©s
{', '.join(report['test_summary']['tests_completed'])}

### RÃ©sultats ClÃ©s

#### Seuil de TolÃ©rance au Bruit
- **Niveau maximum tolÃ©rÃ© (RÂ² > 0.8):** {report['key_findings'].get('noise_tolerance_threshold', 'Non dÃ©terminÃ©')}%
- **Niveau maximum testÃ©:** {report['key_findings'].get('max_tested_noise', 'N/A')}%

#### Performance de RÃ©fÃ©rence (0% bruit)
- **RÂ² Score:** {report['key_findings'].get('reference_performance', {}).get('r2', 'N/A'):.4f}
- **RMSE:** {report['key_findings'].get('reference_performance', {}).get('rmse', 'N/A'):.4f} Âµm

#### BÃ©nÃ©fice de l'Augmentation de DonnÃ©es
- **AmÃ©lioration moyenne RÂ²:** {report['key_findings'].get('augmentation_benefit', {}).get('average_r2_improvement_percent', 'N/A'):.1f}%
- **RecommandÃ©e:** {'Oui' if report['key_findings'].get('augmentation_benefit', {}).get('beneficial', False) else 'Non'}

## ğŸ“‹ Recommandations

### SpÃ©cifications d'Acquisition
- **SNR requis:** {report['recommendations']['acquisition_specifications'].get('snr_requirement', 'N/A')}
- **Niveau de bruit max:** {report['recommendations']['acquisition_specifications'].get('noise_level_max', 'N/A')}
- **QualitÃ© d'acquisition:** {report['recommendations']['acquisition_specifications'].get('acquisition_quality', 'N/A')}

### DÃ©ploiement du ModÃ¨le
- **Ã‰tat de prÃ©paration:** {report['recommendations']['model_deployment'].get('readiness', 'N/A')}
- **Niveau de confiance:** {report['recommendations']['model_deployment'].get('confidence_level', 'N/A')}
- **Monitoring:** {report['recommendations']['model_deployment'].get('monitoring', 'N/A')}

### AmÃ©liorations Futures
- **Augmentation de donnÃ©es:** {report['recommendations']['future_improvements'].get('data_augmentation', 'N/A')}
- **PrioritÃ©:** {report['recommendations']['future_improvements'].get('priority', 'N/A')}

## ğŸ“Š Fichiers GÃ©nÃ©rÃ©s

### RÃ©sultats NumÃ©riques
- `noise_robustness_summary.json` - RÃ©sultats du test de base
- `augmentation_comparison.json` - Comparaison avec/sans augmentation
- `comprehensive_robustness_report.json` - Rapport consolidÃ©

### Visualisations
- `noise_robustness_analysis.png` - Analyse de robustesse
- `predictions_by_noise.png` - PrÃ©dictions par niveau de bruit
- `augmentation_comparison.png` - Comparaison augmentation

### Documentation
- `README.md` - Guide du projet
- `comprehensive_robustness_report.md` - Ce rapport

---

**Note:** Ce rapport synthÃ©tise tous les tests de robustesse au bruit effectuÃ©s sur le modÃ¨le de prÃ©diction du gap.
"""
    
    with open('../results/comprehensive_robustness_report.md', 'w') as f:
        f.write(markdown_content)

def main():
    """Fonction principale pour exÃ©cuter tous les tests."""
    
    print("ğŸš€ LANCEMENT DU TEST COMPLET DE ROBUSTESSE AU BRUIT")
    print("="*60)
    
    start_time = time.time()
    
    # CrÃ©er les dossiers nÃ©cessaires
    os.makedirs("../models", exist_ok=True)
    os.makedirs("../plots", exist_ok=True)
    os.makedirs("../results", exist_ok=True)
    
    success_count = 0
    total_tests = 3
    
    # Ã‰tape 1: Test de robustesse de base
    if run_basic_robustness_test():
        success_count += 1
    
    # Ã‰tape 2: Test avec augmentation
    if run_augmentation_test():
        success_count += 1
    
    # Ã‰tape 3: Rapport complet
    if generate_comprehensive_report():
        success_count += 1
    
    # RÃ©sumÃ© final
    total_time = time.time() - start_time
    
    print("\n" + "="*60)
    print("ğŸ RÃ‰SUMÃ‰ FINAL DU TEST COMPLET")
    print("="*60)
    
    print(f"âœ… Tests rÃ©ussis: {success_count}/{total_tests}")
    print(f"â±ï¸  Temps total: {total_time/60:.1f} minutes")
    
    if success_count == total_tests:
        print("ğŸ‰ TOUS LES TESTS TERMINÃ‰S AVEC SUCCÃˆS!")
        print("\nğŸ“ Fichiers gÃ©nÃ©rÃ©s:")
        print("   ğŸ“Š ../results/ - RÃ©sultats numÃ©riques")
        print("   ğŸ“ˆ ../plots/ - Graphiques et visualisations")
        print("   ğŸ¤– ../models/ - ModÃ¨les entraÃ®nÃ©s")
        print("   ğŸ“‹ ../results/comprehensive_robustness_report.md - Rapport final")
    else:
        print("âš ï¸  Certains tests ont Ã©chouÃ©. VÃ©rifiez les logs ci-dessus.")
    
    return success_count == total_tests

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Test complet de robustesse au bruit')
    parser.add_argument('--quick', action='store_true', 
                       help='Mode rapide avec moins de niveaux de bruit')
    parser.add_argument('--no-augmentation', action='store_true',
                       help='Ignorer les tests d\'augmentation')
    
    args = parser.parse_args()
    
    if args.quick:
        print("ğŸƒ Mode rapide activÃ©")
    
    if args.no_augmentation:
        print("â­ï¸  Tests d'augmentation ignorÃ©s")
    
    success = main()
    sys.exit(0 if success else 1)
