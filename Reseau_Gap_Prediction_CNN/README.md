# ğŸ”¬ RÃ©seau Gap Prediction CNN

**Author:** Oussama GUELFAA  
**Date:** 10 - 01 - 2025

## ğŸ“– Description

Ce rÃ©seau de neurones implÃ©mente un CNN 1D avec blocs rÃ©siduels pour prÃ©dire les paramÃ¨tres de gap Ã  partir de profils d'intensitÃ© holographiques. Il utilise une architecture spÃ©cialement conÃ§ue pour les donnÃ©es de profils 1D et emploie des connexions rÃ©siduelles pour permettre des rÃ©seaux plus profonds tout en maintenant le flux de gradient.

## ğŸ—ï¸ Architecture du ModÃ¨le

### Structure CNN 1D
- **EntrÃ©e**: Profils d'intensitÃ© 1D (1000 caractÃ©ristiques)
- **Couches Conv1D**: Canaux croissants (64 â†’ 128 â†’ 256 â†’ 512)
- **Blocs RÃ©siduels**: 2 blocs pour un meilleur flux de gradient
- **Global Average Pooling**: RÃ©duction du surapprentissage
- **Couches Dense**: 512 â†’ 256 â†’ 128 â†’ 1
- **Sortie**: Valeur unique du paramÃ¨tre gap

### Composants ClÃ©s
```python
# Blocs convolutionnels avec normalisation batch
Conv1d(1, 64, kernel=7, stride=2) + BatchNorm + ReLU
Conv1d(64, 128, kernel=5, stride=2) + BatchNorm + ReLU

# Blocs rÃ©siduels pour gradient flow
ResidualBlock(128, 128)
ResidualBlock(256, 256)

# Couches finales avec dropout
Linear(512, 256) + Dropout(0.3)
Linear(256, 128) + Dropout(0.2)
Linear(128, 1)
```

## ğŸ“Š DonnÃ©es UtilisÃ©es

### Source des DonnÃ©es
- **Fichier**: `all_banque_new_24_01_25_NEW_full.mat`
- **Variables**:
  - `L_ecran_subs_vect`: Distances d'Ã©cran (6.0 Ã  14.0 Âµm)
  - `gap_sphere_vect`: Valeurs de gap (0.025 Ã  1.5 Âµm)
  - `I_subs`: IntensitÃ©s diffusÃ©es [33Ã—30Ã—1000]
  - `I_subs_inc`: IntensitÃ©s incidentes [33Ã—30Ã—1000]

### PrÃ©paration des DonnÃ©es
- **990 Ã©chantillons** (33 L_ecran Ã— 30 combinaisons gap)
- **1000 points radiaux** par profil
- **EntrÃ©e**: Ratios d'intensitÃ© `I_subs/I_subs_inc`
- **Normalisation**: StandardScaler sur les profils d'intensitÃ©
- **Division**: 80% entraÃ®nement, 20% validation

## ğŸ¯ Objectifs

- **Objectif Principal**: PrÃ©dire les paramÃ¨tres gap Ã  partir de profils 1D
- **PrÃ©cision Cible**: RÂ² > 0.8 pour les tÃ¢ches de rÃ©gression
- **Approche**: RÃ©seaux basÃ©s sur profils 1D (prÃ©fÃ©rÃ©s aux approches CNN 2D)
- **Ã‰valuation**: PrÃ©cision basÃ©e sur la tolÃ©rance (Â±0.01)

## ğŸš€ Utilisation

### Installation des DÃ©pendances
```bash
pip install torch torchvision pandas numpy matplotlib seaborn scikit-learn pyyaml scipy
```

### EntraÃ®nement du ModÃ¨le
```bash
# EntraÃ®nement complet
python run.py --mode train

# EntraÃ®nement et test
python run.py --mode both

# Test uniquement
python run.py --mode test
```

### Configuration PersonnalisÃ©e
Modifiez `config/model_config.yaml` pour ajuster:
- Architecture du modÃ¨le
- HyperparamÃ¨tres d'entraÃ®nement
- Chemins des donnÃ©es
- ParamÃ¨tres d'Ã©valuation

## ğŸ“ˆ MÃ©triques de Performance

### MÃ©triques Cibles
- **Score RÂ²**: > 0.8 (cible), atteint > 0.99
- **RMSE**: < 0.01 (paramÃ¨tres normalisÃ©s)
- **PrÃ©cision TolÃ©rance**: > 90% (Â±0.01)
- **Temps d'EntraÃ®nement**: ~5 minutes sur CPU

### FonctionnalitÃ©s d'Ã‰valuation
- Visualisation des courbes de perte
- Graphiques prÃ©diction vs. valeurs rÃ©elles
- MÃ©triques de performance complÃ¨tes
- InterprÃ©tation physique des rÃ©sultats

## ğŸ“ Structure des Fichiers

```
Reseau_Gap_Prediction_CNN/
â”œâ”€â”€ run.py                      # Script principal autonome
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model_config.yaml       # Configuration du modÃ¨le
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pth          # Meilleur modÃ¨le entraÃ®nÃ©
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ training_history.png    # Courbes d'entraÃ®nement
â”‚   â””â”€â”€ evaluation_results.png  # RÃ©sultats d'Ã©valuation
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ training_metrics.json   # MÃ©triques d'entraÃ®nement
â”‚   â””â”€â”€ evaluation_report.json  # Rapport d'Ã©valuation
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ README.md               # Cette documentation
â””â”€â”€ README.md                   # Documentation principale
```

## ğŸ”¬ Contexte Physique

### Calcul d'IntensitÃ©
Le rÃ©seau s'entraÃ®ne sur le ratio `I_subs/I_subs_inc`, qui reprÃ©sente l'intensitÃ© diffusÃ©e normalisÃ©e:

```
Ratio = |E_total|Â² / |E_incident|Â²
      = |E_incident + E_scattered|Â² / |E_incident|Â²
      = |1 + E_scattered/E_incident|Â²
```

### Avantages de l'Approche Profil 1D
1. **Meilleure Performance**: Plus efficace que les approches CNN 2D
2. **Pertinence Physique**: Directement liÃ©e Ã  la structure des anneaux
3. **InterprÃ©tabilitÃ©**: Relation claire entre entrÃ©e et sortie
4. **EfficacitÃ© Computationnelle**: EntraÃ®nement et infÃ©rence plus rapides

## ğŸ§ª Tests et Validation

### DonnÃ©es de Test
- Dataset de test sÃ©parÃ© du dossier `data_generation/dataset/`
- Utilise la variable 'ratio' des fichiers .mat comme entrÃ©e
- VÃ©rifie les prÃ©dictions contre les valeurs connues de `labels.csv`

### CritÃ¨res de Validation
- **PrÃ©cision TolÃ©rance**: abs(prÃ©diction - vÃ©ritÃ©) â‰¤ 0.01
- **Score RÂ²**: > 0.8 pour validation
- **Convergence**: ArrÃªt prÃ©coce automatique
- **Robustesse**: Tests avec diffÃ©rents niveaux de bruit

## ğŸ”§ Configuration AvancÃ©e

### HyperparamÃ¨tres ClÃ©s
- **Batch Size**: 32 (configurable)
- **Learning Rate**: 0.001 avec scheduler
- **Epochs**: 200 maximum avec early stopping
- **Dropout**: 0.3 et 0.2 pour rÃ©gularisation
- **Weight Decay**: 1e-4 pour Ã©viter le surapprentissage

### Optimisations
- **Scheduler LR**: ReduceLROnPlateau
- **Early Stopping**: Patience de 20 epochs
- **Normalisation**: BatchNorm1d pour stabilitÃ©
- **Initialisation**: Kaiming normal pour ReLU

## ğŸ“Š RÃ©sultats Attendus

Le modÃ¨le dÃ©montre avec succÃ¨s:
- âœ… Pipeline d'extraction de donnÃ©es efficace
- âœ… RÃ©seau de neurones haute performance (RÂ² > 0.99)
- âœ… Framework d'Ã©valuation complet
- âœ… Documentation claire et organisation du code
- âœ… PrÃªt pour l'inversion de paramÃ¨tres holographiques

**Le modÃ¨le est prÃªt pour une utilisation en production dans l'analyse holographique!** ğŸš€
