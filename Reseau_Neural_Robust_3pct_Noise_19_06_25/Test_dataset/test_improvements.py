#!/usr/bin/env python3
"""
Test des AmÃ©liorations pour PrÃ©cision Gap 0.007Âµm

Auteur: Oussama GUELFAA
Date: 14 - 01 - 2025

Ce script teste les amÃ©liorations apportÃ©es au rÃ©seau de neurones
pour atteindre une prÃ©cision de 0.007Âµm sur le paramÃ¨tre gap.
"""

import torch
import numpy as np
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from src.dual_parameter_model import DualParameterPredictor, DualParameterLoss, DualParameterMetrics
import time

def test_improved_architecture():
    """
    Test de l'architecture amÃ©liorÃ©e.
    """
    print("ðŸ§ª TEST DE L'ARCHITECTURE AMÃ‰LIORÃ‰E")
    print("="*50)
    
    # CrÃ©er le modÃ¨le amÃ©liorÃ©
    model = DualParameterPredictor(input_size=600, dropout_rate=0.15)
    
    # Test avec donnÃ©es factices
    batch_size = 24  # Nouveau batch size
    input_size = 600
    
    # DonnÃ©es d'entrÃ©e factices
    x = torch.randn(batch_size, input_size)
    
    # Forward pass
    start_time = time.time()
    with torch.no_grad():
        predictions = model(x)
    forward_time = time.time() - start_time
    
    print(f"âœ… Input shape: {x.shape}")
    print(f"âœ… Output shape: {predictions.shape}")
    print(f"âœ… Forward pass time: {forward_time*1000:.2f}ms")
    
    # VÃ©rifier que la sortie a la bonne forme
    assert predictions.shape == (batch_size, 2), f"Erreur: forme attendue ({batch_size}, 2), obtenue {predictions.shape}"
    
    # Afficher le nombre de paramÃ¨tres
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ðŸ“Š ParamÃ¨tres totaux: {total_params:,}")
    print(f"ðŸ“Š ParamÃ¨tres entraÃ®nables: {trainable_params:,}")
    print(f"ðŸ“Š Augmentation vs modÃ¨le original: {(total_params - 482242) / 482242 * 100:.1f}%")
    
    return model

def test_improved_loss_function():
    """
    Test de la fonction de perte amÃ©liorÃ©e.
    """
    print(f"\nðŸ§ª TEST DE LA FONCTION DE PERTE AMÃ‰LIORÃ‰E")
    print("="*50)
    
    # CrÃ©er la loss function amÃ©liorÃ©e
    loss_fn = DualParameterLoss(gap_weight=3.0, L_ecran_weight=1.0, precision_mode=True)
    
    # DonnÃ©es factices
    batch_size = 24
    predictions = torch.randn(batch_size, 2)
    targets = torch.randn(batch_size, 2)
    
    # Calculer la loss
    total_loss, gap_loss, L_ecran_loss = loss_fn(predictions, targets)
    
    print(f"âœ… Total Loss: {total_loss.item():.6f}")
    print(f"âœ… Gap Loss: {gap_loss.item():.6f}")
    print(f"âœ… L_ecran Loss: {L_ecran_loss.item():.6f}")
    print(f"âœ… Ratio Gap/L_ecran: {gap_loss.item() / L_ecran_loss.item():.2f}")
    
    # Test avec erreurs gap importantes
    print(f"\nðŸŽ¯ Test avec erreurs gap importantes...")
    predictions_bad = targets.clone()
    predictions_bad[:, 0] += 0.01  # Erreur gap de 0.01Âµm
    
    total_loss_bad, gap_loss_bad, L_ecran_loss_bad = loss_fn(predictions_bad, targets)
    
    print(f"âœ… Total Loss (erreur gap): {total_loss_bad.item():.6f}")
    print(f"âœ… Gap Loss (erreur gap): {gap_loss_bad.item():.6f}")
    print(f"âœ… PÃ©nalitÃ© prÃ©cision activÃ©e: {gap_loss_bad.item() > gap_loss.item()}")
    
    return loss_fn

def test_improved_metrics():
    """
    Test des mÃ©triques avec nouvelle tolÃ©rance 0.007Âµm.
    """
    print(f"\nðŸ§ª TEST DES MÃ‰TRIQUES HAUTE PRÃ‰CISION")
    print("="*50)
    
    # CrÃ©er les mÃ©triques avec nouvelle tolÃ©rance
    metrics_calc = DualParameterMetrics(gap_tolerance=0.007, L_ecran_tolerance=0.1)
    
    # DonnÃ©es factices avec diffÃ©rents niveaux de prÃ©cision
    n_samples = 1000
    
    # Cas 1: PrÃ©cision excellente
    targets = np.random.rand(n_samples, 2)
    targets[:, 0] *= 0.1  # Gap entre 0 et 0.1Âµm
    targets[:, 1] *= 100  # L_ecran entre 0 et 100Âµm
    
    # PrÃ©dictions avec erreur contrÃ´lÃ©e
    predictions = targets.copy()
    predictions[:, 0] += np.random.normal(0, 0.003, n_samples)  # Erreur gap Â±3nm
    predictions[:, 1] += np.random.normal(0, 0.05, n_samples)   # Erreur L_ecran Â±50nm
    
    metrics = metrics_calc.calculate_metrics(predictions, targets)
    
    print(f"âœ… Test avec erreur gap Â±3nm:")
    print(f"   Gap Accuracy (0.007Âµm): {metrics['gap_accuracy']:.1%}")
    print(f"   Gap MAE: {metrics['gap_mae']:.4f}Âµm")
    print(f"   Gap RÂ²: {metrics['gap_r2']:.4f}")
    
    # Cas 2: PrÃ©cision limite
    predictions_limit = targets.copy()
    predictions_limit[:, 0] += np.random.normal(0, 0.006, n_samples)  # Erreur gap Â±6nm
    predictions_limit[:, 1] += np.random.normal(0, 0.05, n_samples)
    
    metrics_limit = metrics_calc.calculate_metrics(predictions_limit, targets)
    
    print(f"\nâœ… Test avec erreur gap Â±6nm:")
    print(f"   Gap Accuracy (0.007Âµm): {metrics_limit['gap_accuracy']:.1%}")
    print(f"   Gap MAE: {metrics_limit['gap_mae']:.4f}Âµm")
    print(f"   Gap RÂ²: {metrics_limit['gap_r2']:.4f}")
    
    # Cas 3: PrÃ©cision insuffisante
    predictions_bad = targets.copy()
    predictions_bad[:, 0] += np.random.normal(0, 0.01, n_samples)  # Erreur gap Â±10nm
    predictions_bad[:, 1] += np.random.normal(0, 0.05, n_samples)
    
    metrics_bad = metrics_calc.calculate_metrics(predictions_bad, targets)
    
    print(f"\nâœ… Test avec erreur gap Â±10nm:")
    print(f"   Gap Accuracy (0.007Âµm): {metrics_bad['gap_accuracy']:.1%}")
    print(f"   Gap MAE: {metrics_bad['gap_mae']:.4f}Âµm")
    print(f"   Gap RÂ²: {metrics_bad['gap_r2']:.4f}")
    
    return metrics_calc

def test_data_loading():
    """
    Test du chargement des donnÃ©es existantes.
    """
    print(f"\nðŸ§ª TEST DU CHARGEMENT DES DONNÃ‰ES")
    print("="*50)
    
    try:
        # Charger le dataset augmentÃ© existant
        data = np.load('data/augmented_dataset.npz')
        X = data['X']
        y = data['y']
        
        print(f"âœ… Dataset chargÃ© avec succÃ¨s:")
        print(f"   X shape: {X.shape}")
        print(f"   y shape: {y.shape}")
        print(f"   Gap range: {np.min(y[:, 0]):.3f} - {np.max(y[:, 0]):.3f} Âµm")
        print(f"   L_ecran range: {np.min(y[:, 1]):.1f} - {np.max(y[:, 1]):.1f} Âµm")
        
        # Analyser la distribution des gaps
        gap_std = np.std(y[:, 0])
        gap_mean = np.mean(y[:, 0])
        
        print(f"   Gap mean: {gap_mean:.3f}Âµm")
        print(f"   Gap std: {gap_std:.3f}Âµm")
        
        # Identifier les Ã©chantillons dans la zone critique (Â±0.007Âµm autour de la moyenne)
        critical_mask = np.abs(y[:, 0] - gap_mean) <= 0.007
        critical_samples = np.sum(critical_mask)
        
        print(f"   Ã‰chantillons dans zone critique Â±0.007Âµm: {critical_samples} ({critical_samples/len(y)*100:.1f}%)")
        
        return X, y
        
    except Exception as e:
        print(f"âŒ Erreur chargement dataset: {e}")
        return None, None

def main():
    """
    Fonction principale de test.
    """
    print("ðŸš€ TEST COMPLET DES AMÃ‰LIORATIONS - PRÃ‰CISION GAP 0.007Âµm")
    print("="*70)
    print(f"Auteur: Oussama GUELFAA")
    print(f"Date: 14 - 01 - 2025")
    print("="*70)
    
    # Test 1: Architecture amÃ©liorÃ©e
    model = test_improved_architecture()
    
    # Test 2: Fonction de perte amÃ©liorÃ©e
    loss_fn = test_improved_loss_function()
    
    # Test 3: MÃ©triques haute prÃ©cision
    metrics_calc = test_improved_metrics()
    
    # Test 4: Chargement des donnÃ©es
    X, y = test_data_loading()
    
    # RÃ©sumÃ© des amÃ©liorations
    print(f"\nðŸŽ¯ RÃ‰SUMÃ‰ DES AMÃ‰LIORATIONS")
    print("="*40)
    print(f"âœ… Architecture: 6 couches (vs 4), 1024â†’512â†’256â†’128â†’64â†’32â†’2")
    print(f"âœ… ParamÃ¨tres: ~1.2M (vs 482K), +150% capacitÃ©")
    print(f"âœ… Loss function: PondÃ©rÃ©e 3:1 + pÃ©nalitÃ© prÃ©cision")
    print(f"âœ… TolÃ©rance gap: 0.007Âµm (vs 0.01Âµm), -30% tolÃ©rance")
    print(f"âœ… Dropout adaptatif: 0.15â†’0.05 par couche")
    print(f"âœ… Batch size: 24 (vs 32) pour stabilitÃ©")
    print(f"âœ… Learning rate: 0.0008 (vs 0.001) pour prÃ©cision")
    
    if X is not None and y is not None:
        print(f"âœ… Dataset: {len(X)} Ã©chantillons prÃªts pour entraÃ®nement")
        
        # Test rapide avec donnÃ©es rÃ©elles
        print(f"\nðŸ§ª Test rapide avec donnÃ©es rÃ©elles...")
        
        # SÃ©lectionner un petit Ã©chantillon
        indices = np.random.choice(len(X), 100, replace=False)
        X_sample = torch.FloatTensor(X[indices])
        y_sample = y[indices]
        
        # Forward pass
        with torch.no_grad():
            predictions = model(X_sample).numpy()
        
        # Calculer mÃ©triques
        metrics = metrics_calc.calculate_metrics(predictions, y_sample)
        
        print(f"   Gap RÂ² (modÃ¨le non-entraÃ®nÃ©): {metrics['gap_r2']:.4f}")
        print(f"   Gap MAE (modÃ¨le non-entraÃ®nÃ©): {metrics['gap_mae']:.4f}Âµm")
        print(f"   Gap Accuracy 0.007Âµm (modÃ¨le non-entraÃ®nÃ©): {metrics['gap_accuracy']:.1%}")
    
    print(f"\nðŸ† PRÃŠT POUR ENTRAÃŽNEMENT HAUTE PRÃ‰CISION !")
    print(f"   Objectif: Gap Accuracy > 85% avec tolÃ©rance 0.007Âµm")
    print(f"   Commande: python run.py")

if __name__ == "__main__":
    main()
