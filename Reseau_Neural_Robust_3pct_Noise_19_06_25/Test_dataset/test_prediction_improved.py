#!/usr/bin/env python3
"""
Test de PrÃ©diction AmÃ©liorÃ© - ModÃ¨le Dual Gap + L_ecran

Auteur: Oussama GUELFAA
Date: 19 - 06 - 2025

Script pour tester les prÃ©dictions du modÃ¨le entraÃ®nÃ©
utilisant la mÃªme approche que demo.py pour des rÃ©sultats optimaux.
"""

import sys
import os
sys.path.append('../src')

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import pandas as pd

from dual_parameter_model import DualParameterPredictor
from data_loader import DualDataLoader

def predict_sample(model, profile, data_loader):
    """
    PrÃ©dit les paramÃ¨tres pour un Ã©chantillon (identique Ã  demo.py).
    
    Args:
        model: ModÃ¨le entraÃ®nÃ©
        profile (np.array): Profil d'intensitÃ© [600]
        data_loader: DataLoader pour normalisation
    
    Returns:
        tuple: (gap_pred, L_ecran_pred)
    """
    # Normaliser l'entrÃ©e
    profile_scaled = data_loader.input_scaler.transform(profile.reshape(1, -1))
    
    # PrÃ©diction
    with torch.no_grad():
        input_tensor = torch.FloatTensor(profile_scaled)
        prediction_scaled = model(input_tensor).numpy()
    
    # DÃ©normaliser la sortie
    prediction_original = data_loader.inverse_transform_predictions(prediction_scaled)
    
    gap_pred = prediction_original[0, 0]
    L_ecran_pred = prediction_original[0, 1]
    
    return gap_pred, L_ecran_pred

def load_model_and_data():
    """
    Charge le modÃ¨le entraÃ®nÃ© et les donnÃ©es de test (approche demo.py).
    
    Returns:
        tuple: (model, X_test, y_test, data_loader)
    """
    print("ðŸ”„ Chargement du modÃ¨le et des donnÃ©es...")
    
    # Charger le modÃ¨le
    model_path = "../models/dual_parameter_model.pth"
    if not Path(model_path).exists():
        raise FileNotFoundError(f"ModÃ¨le non trouvÃ©: {model_path}")
    
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    
    model = DualParameterPredictor(input_size=600)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ… ModÃ¨le chargÃ©: {sum(p.numel() for p in model.parameters()):,} paramÃ¨tres")
    
    # Charger les donnÃ©es avec la mÃªme approche que demo.py
    data_loader = DualDataLoader()
    
    # Configuration modifiÃ©e pour avoir ~2400 Ã©chantillons de test
    # Avec 17,080 Ã©chantillons total: 70/16/14 â‰ˆ 11,956/2,733/2,391 Ã©chantillons
    config = {
        'data_processing': {
            'augmentation': {'enable': False},  # Pas d'augmentation pour le test
            'data_splits': {'train': 0.70, 'validation': 0.16, 'test': 0.14},  # Split modifiÃ© pour ~2400 test
            'normalization': {'target_scaling': {'separate_scaling': True}}
        },
        'training': {'batch_size': 32}
    }
    
    pipeline_result = data_loader.get_complete_pipeline(config)
    
    # RÃ©cupÃ©rer les donnÃ©es de test non normalisÃ©es (comme dans demo.py)
    X_test = pipeline_result['raw_data'][2]  # DonnÃ©es de test non normalisÃ©es
    y_test = pipeline_result['raw_data'][5]  # Labels de test
    
    print(f"âœ… DonnÃ©es chargÃ©es: {len(X_test)} Ã©chantillons de test")
    
    return model, X_test, y_test, data_loader

def evaluate_model_comprehensive(model, X_test, y_test, data_loader, n_samples=None):
    """
    Ã‰valuation complÃ¨te du modÃ¨le sur les donnÃ©es de test.
    
    Args:
        model: ModÃ¨le entraÃ®nÃ©
        X_test: DonnÃ©es de test
        y_test: Labels de test
        data_loader: DataLoader configurÃ©
        n_samples: Nombre d'Ã©chantillons Ã  tester (None = tous)
    
    Returns:
        dict: RÃ©sultats dÃ©taillÃ©s
    """
    print(f"\nðŸŽ¯ Ã‰VALUATION COMPLÃˆTE DU MODÃˆLE")
    print("="*50)
    
    # SÃ©lectionner les Ã©chantillons
    if n_samples is None or n_samples > len(X_test):
        n_samples = len(X_test)
        indices = np.arange(len(X_test))
    else:
        indices = np.random.choice(len(X_test), n_samples, replace=False)
    
    print(f"ðŸ“Š Test sur {n_samples} Ã©chantillons")
    
    # PrÃ©dictions
    predictions_gap = []
    predictions_L_ecran = []
    true_gap = []
    true_L_ecran = []
    
    print("ðŸ”„ Calcul des prÃ©dictions...")
    for i, idx in enumerate(indices):
        if i % 100 == 0:
            print(f"   Progression: {i}/{n_samples}")
        
        profile = X_test[idx]
        gap_pred, L_ecran_pred = predict_sample(model, profile, data_loader)
        
        predictions_gap.append(gap_pred)
        predictions_L_ecran.append(L_ecran_pred)
        true_gap.append(y_test[idx, 0])
        true_L_ecran.append(y_test[idx, 1])
    
    # Conversion en arrays
    predictions_gap = np.array(predictions_gap)
    predictions_L_ecran = np.array(predictions_L_ecran)
    true_gap = np.array(true_gap)
    true_L_ecran = np.array(true_L_ecran)
    
    # Calcul des mÃ©triques
    gap_r2 = r2_score(true_gap, predictions_gap)
    L_ecran_r2 = r2_score(true_L_ecran, predictions_L_ecran)
    combined_r2 = (gap_r2 + L_ecran_r2) / 2
    
    gap_mae = mean_absolute_error(true_gap, predictions_gap)
    L_ecran_mae = mean_absolute_error(true_L_ecran, predictions_L_ecran)
    
    gap_rmse = np.sqrt(mean_squared_error(true_gap, predictions_gap))
    L_ecran_rmse = np.sqrt(mean_squared_error(true_L_ecran, predictions_L_ecran))
    
    # Analyse de tolÃ©rance
    gap_tolerance = 0.01  # Âµm
    L_ecran_tolerance = 0.1  # Âµm
    
    gap_within_tolerance = np.sum(np.abs(predictions_gap - true_gap) <= gap_tolerance)
    L_ecran_within_tolerance = np.sum(np.abs(predictions_L_ecran - true_L_ecran) <= L_ecran_tolerance)
    
    gap_accuracy = gap_within_tolerance / n_samples
    L_ecran_accuracy = L_ecran_within_tolerance / n_samples
    
    # Affichage des rÃ©sultats
    print(f"\nðŸ“Š RÃ‰SULTATS DE L'Ã‰VALUATION")
    print("="*40)
    print(f"ðŸŽ¯ MÃ©triques RÂ²:")
    print(f"   Gap RÂ²: {gap_r2:.4f}")
    print(f"   L_ecran RÂ²: {L_ecran_r2:.4f}")
    print(f"   Combined RÂ²: {combined_r2:.4f}")
    
    print(f"\nðŸ“ Erreurs moyennes:")
    print(f"   Gap MAE: {gap_mae:.4f} Âµm")
    print(f"   Gap RMSE: {gap_rmse:.4f} Âµm")
    print(f"   L_ecran MAE: {L_ecran_mae:.4f} Âµm")
    print(f"   L_ecran RMSE: {L_ecran_rmse:.4f} Âµm")
    
    print(f"\nðŸŽ¯ PrÃ©cision dans tolÃ©rance:")
    print(f"   Gap (Â±{gap_tolerance} Âµm): {gap_within_tolerance}/{n_samples} ({gap_accuracy:.1%})")
    print(f"   L_ecran (Â±{L_ecran_tolerance} Âµm): {L_ecran_within_tolerance}/{n_samples} ({L_ecran_accuracy:.1%})")
    
    # Retourner les rÃ©sultats
    results = {
        'n_samples': n_samples,
        'metrics': {
            'gap_r2': gap_r2,
            'L_ecran_r2': L_ecran_r2,
            'combined_r2': combined_r2,
            'gap_mae': gap_mae,
            'L_ecran_mae': L_ecran_mae,
            'gap_rmse': gap_rmse,
            'L_ecran_rmse': L_ecran_rmse
        },
        'tolerance_analysis': {
            'gap_tolerance': gap_tolerance,
            'L_ecran_tolerance': L_ecran_tolerance,
            'gap_accuracy': gap_accuracy,
            'L_ecran_accuracy': L_ecran_accuracy,
            'gap_within_tolerance': int(gap_within_tolerance),
            'L_ecran_within_tolerance': int(L_ecran_within_tolerance)
        },
        'predictions': {
            'gap_true': true_gap.tolist(),
            'gap_pred': predictions_gap.tolist(),
            'L_ecran_true': true_L_ecran.tolist(),
            'L_ecran_pred': predictions_L_ecran.tolist()
        }
    }
    
    return results

def create_visualizations(results, save_dir="plots"):
    """
    CrÃ©e des visualisations des rÃ©sultats.
    
    Args:
        results: RÃ©sultats de l'Ã©valuation
        save_dir: RÃ©pertoire de sauvegarde
    """
    print(f"\nðŸ“ˆ CrÃ©ation des visualisations...")
    
    Path(save_dir).mkdir(exist_ok=True)
    
    # DonnÃ©es
    gap_true = np.array(results['predictions']['gap_true'])
    gap_pred = np.array(results['predictions']['gap_pred'])
    L_ecran_true = np.array(results['predictions']['L_ecran_true'])
    L_ecran_pred = np.array(results['predictions']['L_ecran_pred'])
    
    # Figure avec 2 sous-graphiques
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Gap scatter plot
    ax1.scatter(gap_true, gap_pred, alpha=0.6, s=20)
    ax1.plot([gap_true.min(), gap_true.max()], [gap_true.min(), gap_true.max()], 'r--', lw=2)
    ax1.set_xlabel('Gap Vrai (Âµm)')
    ax1.set_ylabel('Gap PrÃ©dit (Âµm)')
    ax1.set_title(f'PrÃ©diction Gap (RÂ² = {results["metrics"]["gap_r2"]:.4f})')
    ax1.grid(True, alpha=0.3)
    
    # L_ecran scatter plot
    ax2.scatter(L_ecran_true, L_ecran_pred, alpha=0.6, s=20)
    ax2.plot([L_ecran_true.min(), L_ecran_true.max()], [L_ecran_true.min(), L_ecran_true.max()], 'r--', lw=2)
    ax2.set_xlabel('L_ecran Vrai (Âµm)')
    ax2.set_ylabel('L_ecran PrÃ©dit (Âµm)')
    ax2.set_title(f'PrÃ©diction L_ecran (RÂ² = {results["metrics"]["L_ecran_r2"]:.4f})')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"../{save_dir}/test_predictions_improved.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… Visualisations sauvegardÃ©es dans {save_dir}/")

def main():
    """
    Fonction principale de test.
    """
    print("ðŸŽ¯ TEST DE PRÃ‰DICTION AMÃ‰LIORÃ‰ - MODÃˆLE DUAL GAP + L_ECRAN")
    print("="*60)
    print(f"Auteur: Oussama GUELFAA")
    print(f"Date: 19-06-2025")
    print("="*60)
    
    try:
        # 1. Charger le modÃ¨le et les donnÃ©es
        model, X_test, y_test, data_loader = load_model_and_data()
        
        # 2. Ã‰valuation complÃ¨te
        results = evaluate_model_comprehensive(model, X_test, y_test, data_loader)
        
        # 3. CrÃ©er les visualisations
        create_visualizations(results)
        
        # 4. Sauvegarder les rÃ©sultats
        results_path = "../results/test_predictions_improved.json"
        Path("../results").mkdir(exist_ok=True)
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nðŸŽ‰ Test terminÃ© avec succÃ¨s !")
        print(f"ðŸ“Š RÃ©sultats sauvegardÃ©s: {results_path}")
        print(f"ðŸ“ˆ Visualisations: plots/test_predictions_improved.png")
        
    except Exception as e:
        print(f"âŒ Erreur dans le test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
