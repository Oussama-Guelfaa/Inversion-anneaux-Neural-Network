# ðŸ§  RÃ©seau Advanced Regressor

**Author:** Oussama GUELFAA  
**Date:** 10 - 01 - 2025

## ðŸ“– Description

Ce rÃ©seau de neurones implÃ©mente un rÃ©gresseur avancÃ© avec mÃ©canisme d'attention pour prÃ©dire simultanÃ©ment les paramÃ¨tres gap et L_ecran. Il rÃ©sout systÃ©matiquement 5 problÃ¨mes identifiÃ©s dans les versions prÃ©cÃ©dentes en utilisant une architecture spÃ©cialisÃ©e avec tÃªtes dÃ©diÃ©es et attention pour le signal gap plus faible.

## ðŸŽ¯ Objectifs

- **PrÃ©diction SimultanÃ©e**: Gap et L_ecran en une seule passe
- **RÃ©solution de ProblÃ¨mes**: 5 problÃ¨mes systÃ©matiquement adressÃ©s
- **Architecture SpÃ©cialisÃ©e**: TÃªtes dÃ©diÃ©es avec mÃ©canisme d'attention
- **Performance Cible**: RÂ² > 0.8 pour gap, RÂ² > 0.95 pour L_ecran

## ðŸ—ï¸ Architecture du ModÃ¨le

### Structure Multi-TÃªtes avec Attention
- **EntrÃ©e**: Profils d'intensitÃ© tronquÃ©s (600 caractÃ©ristiques)
- **Feature Extractor**: Commun aux deux paramÃ¨tres
- **TÃªte L_ecran**: Simple (signal fort)
- **TÃªte Gap**: SpÃ©cialisÃ©e avec attention (signal faible)
- **Sortie**: [L_ecran, gap] simultanÃ©ment

### Composants Architecturaux
```python
# Feature extractor commun
Linear(600, 512) + BatchNorm + ReLU + Dropout(0.2)
Linear(512, 256) + BatchNorm + ReLU + Dropout(0.15)
Linear(256, 128) + BatchNorm + ReLU + Dropout(0.1)

# TÃªte L_ecran (signal fort)
Linear(128, 64) + ReLU + Dropout(0.05)
Linear(64, 32) + ReLU
Linear(32, 1)

# TÃªte gap avec attention (signal faible)
Attention: Linear(128, 64) + Tanh + Linear(64, 128) + Sigmoid
Gap Head: Linear(128, 128) + BatchNorm + ReLU + Dropout(0.01)
          Linear(128, 64) + BatchNorm + ReLU + Dropout(0.01)
          Linear(64, 32) + ReLU + Linear(32, 1)
```

## ðŸ”§ RÃ©solution des 5 ProblÃ¨mes

### ProblÃ¨me 1: PrÃ©cision Excessive des Labels
- **Solution**: Arrondissement Ã  3 dÃ©cimales
- **Impact**: RÃ©duction du bruit dans les labels
- **ImplÃ©mentation**: `np.round(labels, 3)`

### ProblÃ¨me 2: Ã‰chelles DÃ©sÃ©quilibrÃ©es
- **Solution**: Normalisation sÃ©parÃ©e par paramÃ¨tre
- **Impact**: Ã‰quilibrage des gradients
- **ImplÃ©mentation**: StandardScaler individuel pour L_ecran et gap

### ProblÃ¨me 3: Distribution DÃ©sÃ©quilibrÃ©e
- **Solution**: Focus sur plage expÃ©rimentale [0.025, 0.517] Âµm
- **Impact**: Concentration sur donnÃ©es pertinentes
- **ImplÃ©mentation**: Filtrage par masque boolÃ©en

### ProblÃ¨me 4: Loss Function InadaptÃ©e
- **Solution**: Loss pondÃ©rÃ©e (gap Ã— 30, L_ecran Ã— 1)
- **Impact**: Attention accrue sur le paramÃ¨tre gap
- **ImplÃ©mentation**: WeightedMSELoss personnalisÃ©e

### ProblÃ¨me 5: Signal Gap Faible
- **Solution**: Architecture spÃ©cialisÃ©e avec attention
- **Impact**: Amplification du signal gap
- **ImplÃ©mentation**: MÃ©canisme d'attention + tÃªte dÃ©diÃ©e

## ðŸ“Š DonnÃ©es et PrÃ©traitement

### Source des DonnÃ©es
- **Fichier**: `all_banque_new_24_01_25_NEW_full.mat`
- **Ã‰chantillons**: 990 profils d'intensitÃ©
- **CaractÃ©ristiques**: 600 points radiaux (tronquÃ©s)
- **ParamÃ¨tres**: L_ecran [6.0-14.0 Âµm], gap [0.025-1.5 Âµm]

### Pipeline de PrÃ©traitement
1. **Extraction**: Ratios I_subs/I_subs_inc
2. **Troncature**: 1000 â†’ 600 points
3. **Arrondissement**: Labels Ã  3 dÃ©cimales
4. **Filtrage**: Focus plage expÃ©rimentale
5. **Normalisation**: StandardScaler sÃ©parÃ©

## ðŸš€ Utilisation

### Installation des DÃ©pendances
```bash
pip install torch pandas numpy matplotlib seaborn scikit-learn pyyaml scipy joblib
```

### EntraÃ®nement du ModÃ¨le
```bash
# EntraÃ®nement complet avec rÃ©solution des problÃ¨mes
python run.py

# Avec configuration personnalisÃ©e
python run.py --config config/advanced_config.yaml
```

### Configuration PersonnalisÃ©e
Modifiez `config/advanced_config.yaml` pour ajuster:
- Poids de la loss pondÃ©rÃ©e
- Architecture du modÃ¨le
- ParamÃ¨tres d'entraÃ®nement
- CritÃ¨res d'Ã©valuation

## ðŸ“ˆ MÃ©triques de Performance

### MÃ©triques Principales
- **RÂ² Global**: Performance combinÃ©e
- **RÂ² L_ecran**: Performance sur L_ecran (cible > 0.95)
- **RÂ² Gap**: Performance sur gap (cible > 0.8)
- **RMSE**: Erreur quadratique moyenne
- **MAE**: Erreur absolue moyenne
- **TolÃ©rance**: PrÃ©cision dans seuils dÃ©finis

### Ã‰valuation avec TolÃ©rance
- **TolÃ©rance L_ecran**: Â±0.5 Âµm
- **TolÃ©rance Gap**: Â±0.01 Âµm (2 dÃ©cimales)
- **CritÃ¨re de SuccÃ¨s**: 90% des prÃ©dictions dans tolÃ©rance

## ðŸ“ Structure des Fichiers

```
Reseau_Advanced_Regressor/
â”œâ”€â”€ run.py                              # Script principal autonome
â”œâ”€â”€ config/
â”‚   â””â”€â”€ advanced_config.yaml            # Configuration complÃ¨te
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ advanced_regressor_best.pth     # Meilleur modÃ¨le
â”‚   â”œâ”€â”€ advanced_regressor_scaler_X.pkl # Scaler profils
â”‚   â”œâ”€â”€ advanced_regressor_scaler_L.pkl # Scaler L_ecran
â”‚   â””â”€â”€ advanced_regressor_scaler_gap.pkl # Scaler gap
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ training_curves.png             # Courbes d'entraÃ®nement
â”‚   â”œâ”€â”€ predictions_scatter.png         # PrÃ©dictions vs rÃ©el
â”‚   â”œâ”€â”€ tolerance_analysis.png          # Analyse tolÃ©rance
â”‚   â””â”€â”€ attention_weights.png           # Visualisation attention
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ training_history.csv            # Historique entraÃ®nement
â”‚   â”œâ”€â”€ evaluation_metrics.json         # MÃ©triques dÃ©taillÃ©es
â”‚   â””â”€â”€ config_used.yaml                # Configuration utilisÃ©e
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ EXECUTIVE_SUMMARY.md             # RÃ©sumÃ© exÃ©cutif
â”‚   â”œâ”€â”€ README_neural_network_06_06_25.md # Documentation dÃ©taillÃ©e
â”‚   â””â”€â”€ README_tolerance_evaluation.md   # Ã‰valuation tolÃ©rance
â””â”€â”€ README.md                           # Cette documentation
```

## ðŸ”¬ FonctionnalitÃ©s AvancÃ©es

### MÃ©canisme d'Attention
- **Type**: Self-attention pour gap
- **Fonction**: Amplification des features pertinentes
- **Architecture**: Linear + Tanh + Linear + Sigmoid
- **Application**: Multiplication Ã©lÃ©ment par Ã©lÃ©ment

### Loss PondÃ©rÃ©e
```python
class WeightedMSELoss(nn.Module):
    def forward(self, pred, target):
        mse = (pred - target) ** 2
        weighted_mse = mse * weights  # [1.0, 30.0]
        return weighted_mse.mean()
```

### Gradient Clipping
- **Activation**: Configurable
- **Norme Max**: 1.0 par dÃ©faut
- **Objectif**: StabilitÃ© d'entraÃ®nement

### Early Stopping
- **Patience**: 25 epochs
- **CritÃ¨re**: Validation loss
- **Restauration**: Meilleur modÃ¨le automatique

## ðŸ§ª Tests et Validation

### Validation CroisÃ©e
- **Division**: 80% train, 20% validation
- **StratÃ©gie**: AlÃ©atoire avec seed fixe
- **MÃ©triques**: RÂ², RMSE, MAE, tolÃ©rance

### Tests de Robustesse
- **Initialisation**: Kaiming normal
- **ReproductibilitÃ©**: Seed fixe (42)
- **StabilitÃ©**: Batch normalization

## ðŸŽ¯ RÃ©sultats Attendus

### Performance L_ecran
- **RÂ²**: > 0.95 (signal fort)
- **RMSE**: < 0.1 Âµm
- **TolÃ©rance**: > 95% dans Â±0.5 Âµm

### Performance Gap
- **RÂ²**: > 0.8 (signal faible mais amÃ©liorÃ©)
- **RMSE**: < 0.05 Âµm
- **TolÃ©rance**: > 90% dans Â±0.01 Âµm

### Performance Globale
- **Convergence**: < 100 epochs
- **StabilitÃ©**: EntraÃ®nement robuste
- **GÃ©nÃ©ralisation**: Bonne performance test

## ðŸ”§ Optimisations ImplÃ©mentÃ©es

### Architecture
- **TÃªtes SpÃ©cialisÃ©es**: Adaptation au signal
- **Attention**: Focus sur features importantes
- **Dropout Adaptatif**: RÃ©gularisation progressive
- **Batch Normalization**: Stabilisation

### EntraÃ®nement
- **Loss PondÃ©rÃ©e**: Ã‰quilibrage des objectifs
- **Learning Rate**: Adaptatif avec scheduler
- **Weight Decay**: RÃ©gularisation L2
- **Gradient Clipping**: PrÃ©vention explosion

### DonnÃ©es
- **PrÃ©traitement SystÃ©matique**: RÃ©solution des 5 problÃ¨mes
- **Normalisation SÃ©parÃ©e**: Ã‰quilibrage des Ã©chelles
- **Focus ExpÃ©rimental**: DonnÃ©es pertinentes
- **Troncature Optimale**: 600 points

## ðŸ“Š Monitoring et Logging

### MÃ©triques Suivies
- **Loss**: Train et validation
- **RÂ²**: Global, L_ecran, gap
- **TolÃ©rance**: PrÃ©cision pratique
- **Learning Rate**: Adaptation automatique

### FrÃ©quence de Log
- **EntraÃ®nement**: Toutes les 10 epochs
- **Sauvegarde**: Meilleur modÃ¨le automatique
- **Visualisation**: Graphiques automatiques

**Ce modÃ¨le rÃ©sout systÃ©matiquement les problÃ¨mes identifiÃ©s pour une performance optimale!** ðŸš€
